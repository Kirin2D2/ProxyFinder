{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uWUQYLoYUZb"
      },
      "source": [
        "# Monte Carlo Testing for Proxy Finder Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAwEOp8GYdEY"
      },
      "outputs": [],
      "source": [
        "from proxy_finder import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(df, target, target_correlation):\n",
        "  # split dataset in half\n",
        "  df_train = df.sample(frac=0.5, random_state=42)\n",
        "  df_test = df.drop(df_train.index)\n",
        "\n",
        "  # add synthetic proxies to test set\n",
        "  target_column = df_test[target]\n",
        "  synthetic_proxies = generate_synthetic_proxies(target_column, target_correlation)\n",
        "  for name, proxy in synthetic_proxies.items():\n",
        "    df_test[name] = proxy\n",
        "\n",
        "  # drop target from test set\n",
        "  df_test = df_test.drop(columns=[target])\n",
        "\n",
        "  return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_proxies(target_column, target_correlation, noise_level=0.1):\n",
        "   # Convert target_column to numpy array and standardize\n",
        "    target = np.array(target_column)\n",
        "    target = (target - np.mean(target)) / np.std(target)\n",
        "\n",
        "    synthetic_proxies = {}\n",
        "\n",
        "    # Generate independent standard normal variable\n",
        "    z = np.random.standard_normal(len(target))\n",
        "\n",
        "    # Create correlated variable using the correlation formula\n",
        "    proxy = target_correlation * target + np.sqrt(1 - target_correlation**2) * z\n",
        "\n",
        "    # Add controlled noise\n",
        "    proxy = proxy + np.random.normal(0, noise_level, len(target))\n",
        "\n",
        "    # Standardize final proxy\n",
        "    proxy = (proxy - np.mean(proxy)) / np.std(proxy)\n",
        "\n",
        "    synthetic_proxies[f'proxy_{target_correlation:.2f}'] = proxy\n",
        "\n",
        "    return synthetic_proxies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnUMZlYeqfW"
      },
      "source": [
        "# Stage 1: Testing Mean Penalty Approach with Several Target Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wIImp49Se3XQ"
      },
      "outputs": [],
      "source": [
        "def run_and_visualize_monte_carlo(df, weights, num_iterations, target, target_correlations, predictors):\n",
        "    selection_trackers = []\n",
        "    proxy_names = []\n",
        "    \n",
        "    # Run Monte Carlo for each target correlation\n",
        "    for target_correlation in target_correlations:\n",
        "        # Prepare dataset\n",
        "        df_train, df_test = prepare_dataset(df, target, target_correlation)\n",
        "        selection_tracker = {orth_weight: {} for orth_weight in weights}\n",
        "        \n",
        "        # Run iterations for each weight\n",
        "        for orth_weight in weights:\n",
        "            print(f\"Testing with orthogonality weight: {orth_weight}\")\n",
        "            print(f\"Testing with target correlation: {target_correlation}\")\n",
        "            \n",
        "            for i in range(num_iterations):\n",
        "                print(f\"Running iteration {i+1}/{num_iterations}\")\n",
        "                top_proxies = proxy_finder(df_train=df_train, \n",
        "                                         df_test=df_test, \n",
        "                                         target=target, \n",
        "                                         predictors=predictors, \n",
        "                                         num_proxies=5, \n",
        "                                         orth_weight=orth_weight, \n",
        "                                         orthogonal_vars=predictors)\n",
        "                \n",
        "                # Update selection tracker for top pick\n",
        "                for rank, proxy in enumerate(top_proxies, 1):\n",
        "                    if rank == 1:\n",
        "                        selection_tracker[orth_weight][proxy] = selection_tracker[orth_weight].get(proxy, 0) + 1\n",
        "        \n",
        "        selection_trackers.append(selection_tracker)\n",
        "        proxy_names.append(f'proxy_{target_correlation:.2f}')\n",
        "    \n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot results for each target correlation\n",
        "    for index, tracker in enumerate(selection_trackers):\n",
        "        results = []\n",
        "        for orth_weight, proxies in tracker.items():\n",
        "            for proxy, frequency in proxies.items():\n",
        "                results.append({\n",
        "                    'orth_weight': orth_weight,\n",
        "                    'proxy': proxy,\n",
        "                    'frequency': (frequency / num_iterations) * 100\n",
        "                })\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        pivot_data = results_df.pivot(index='orth_weight', columns='proxy', values='frequency')\n",
        "        pivot_data.fillna(0, inplace=True)\n",
        "        print(pivot_data)\n",
        "        \n",
        "        # Plot each proxy as a separate line\n",
        "        name = proxy_names[index]\n",
        "        plt.plot(pivot_data.index, pivot_data[name], marker='o', label=name, linewidth=2)\n",
        "    \n",
        "    # Create the line plot\n",
        "    plt.xlabel('Orthogonality Weight')\n",
        "    plt.ylabel('Selection Frequency')\n",
        "    plt.title('Selection Frequency vs Orthogonality Weight')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# change parameters as needed\n",
        "df = pd.read_stata(\"/content/temp_yougov.dta\")\n",
        "weights = np.arange(0.4, 0.85, 0.05)\n",
        "target_correlations = [0.95, 0.90, 0.80]\n",
        "num_iterations = 30\n",
        "target = 'christian_nationalism'\n",
        "predictors = [\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new']\n",
        "run_and_visualize_monte_carlo(df, weights, num_iterations, target, target_correlations, predictors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Testing Threshold-Based Penalty Approach with Several Tau Values \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from proxy_finder_threshold_orthogonalization import proxy_finder_threshold_ortho as pf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_and_visualize_monte_carlo2(df, weights, num_iterations, target, target_correlation, predictors, taus):\n",
        "    selection_trackers = []\n",
        "    proxy_name = (f'proxy_{target_correlation:.2f}')\n",
        "    tau_names = [f'tau_{tau:.2f}' for tau in taus]\n",
        "\n",
        "    # Run Monte Carlo for each target correlation\n",
        "    for tau in taus:\n",
        "        # Prepare dataset\n",
        "        df_train, df_test = prepare_dataset(df, target, target_correlation)\n",
        "        selection_tracker = {orth_weight: {} for orth_weight in weights}\n",
        "        \n",
        "        # Run iterations for each weight\n",
        "        for orth_weight in weights:\n",
        "            print(f\"Testing with orthogonality weight: {orth_weight}\")\n",
        "            print(f\"Testing with target correlation: {target_correlation}\")\n",
        "            \n",
        "            for i in range(num_iterations):\n",
        "                print(f\"Running iteration {i+1}/{num_iterations}\")\n",
        "                top_proxies = pf(df_train=df_train, \n",
        "                                         df_test=df_test, \n",
        "                                         target=target, \n",
        "                                         predictors=predictors, \n",
        "                                         num_proxies=5, \n",
        "                                         orth_weight=orth_weight, \n",
        "                                         orthogonal_vars=predictors)\n",
        "                \n",
        "                # Update selection tracker for top pick\n",
        "                for rank, proxy in enumerate(top_proxies, 1):\n",
        "                    if rank == 1:\n",
        "                        selection_tracker[orth_weight][proxy] = selection_tracker[orth_weight].get(proxy, 0) + 1\n",
        "        \n",
        "        selection_trackers.append(selection_tracker)\n",
        "    \n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot results for each target correlation\n",
        "    for index, tracker in enumerate(selection_trackers):\n",
        "        results = []\n",
        "        for orth_weight, proxies in tracker.items():\n",
        "            for proxy, frequency in proxies.items():\n",
        "                results.append({\n",
        "                    'orth_weight': orth_weight,\n",
        "                    'proxy': proxy,\n",
        "                    'frequency': (frequency / num_iterations) * 100\n",
        "                })\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        pivot_data = results_df.pivot(index='orth_weight', columns='proxy', values='frequency')\n",
        "        pivot_data.fillna(0, inplace=True)\n",
        "        print(pivot_data)\n",
        "        \n",
        "        # Plot each proxy as a separate line\n",
        "        name = tau_names[index]\n",
        "        plt.plot(pivot_data.index, pivot_data[proxy_name], marker='o', label=name, linewidth=2)\n",
        "    \n",
        "    # Create the line plot\n",
        "    plt.xlabel('Orthogonality Weight')\n",
        "    plt.ylabel('Selection Frequency')\n",
        "    plt.title('Selection Frequency vs Orthogonality Weight')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_stata(\"/content/temp_yougov.dta\")\n",
        "weights=[0.35, 0.65]\n",
        "taus = [0.00, 0.05]\n",
        "num_iterations = 2\n",
        "target_correlation = 0.90\n",
        "target = 'christian_nationalism'\n",
        "predictors = [\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new']\n",
        "\n",
        "run_and_visualize_monte_carlo2(df, weights, num_iterations, target, target_correlation, predictors, taus)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
