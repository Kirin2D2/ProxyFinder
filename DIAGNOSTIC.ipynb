{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uWUQYLoYUZb"
      },
      "source": [
        "# Monte Carlo Testing for Proxy Finder Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sys\n",
        "from gain import GAIN\n",
        "from usage_example import *\n",
        "import utils\n",
        "import models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nC-Hn0_GjtJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
        "    # validate proxies and st item\n",
        "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
        "\n",
        "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
        "\n",
        "    for c in predictors:\n",
        "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
        "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset\n",
        "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'\n",
        "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'\n",
        "\n",
        "    for c in candidates:\n",
        "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
        "\n",
        "    if (orthogonal_vars != None):\n",
        "        for c in orthogonal_vars:\n",
        "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'"
      ],
      "metadata": {
        "id": "nd3YKbPqka9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(df_train, df_test, predictors, target, epochs=250, learning_rate=0.001, l2_lambda=0.001):\n",
        "  # CODE IMPLEMENTATION ASSISTED BY GENERATIVE AI\n",
        "\n",
        "  # Set parameters\n",
        "\n",
        "  DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  TRAIN_SIZE = 1.0  # Using all of df1 for training\n",
        "\n",
        "\n",
        "  df1 = df_train.copy()\n",
        "  df2 = df_test.copy()\n",
        "\n",
        "  # drop everything but predictors and target from df1\n",
        "  target_col_df1 = df1[target]\n",
        "  df1 = df1[predictors]\n",
        "  df1[target] = target_col_df1\n",
        "\n",
        "  # drop everything but predictors from df2\n",
        "  df2 = df2[predictors]\n",
        "  # add missing target\n",
        "  df2[target] = np.nan\n",
        "\n",
        "  combined_df = pd.concat([df1, df2])\n",
        "\n",
        "  # # Step 3: Normalize the data\n",
        "  # scaler = MinMaxScaler()\n",
        "  # combined_data_std = scaler.fit_transform(combined_df)\n",
        "\n",
        "  # Split back into df1 (training) and df2 (prediction)\n",
        "  df1_std = combined_df[:len(df1)]\n",
        "  df2_std = combined_df[len(df1):]\n",
        "\n",
        "  # Create tensors and masks\n",
        "  X_train_tensor = torch.tensor(df1_std.values).float()\n",
        "  M_train_tensor = get_mask(X_train_tensor)  # This creates mask with 0s for observed values, 1s for missing values\n",
        "  train_dataset = TensorDataset(X_train_tensor, M_train_tensor)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "  X_test_tensor = torch.tensor(df2_std.values).float()\n",
        "  M_test_tensor = get_mask(X_test_tensor)  # This will mark all values in the target column as missing\n",
        "  test_dataset = TensorDataset(X_test_tensor, M_test_tensor)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "  # Step 4: Initialize and train the GAIN model\n",
        "  stopper = EarlyStopper(patience=2, min_delta=0.001)\n",
        "  model = GAIN(train_loader=train_loader)\n",
        "\n",
        "  optimizer_G = torch.optim.Adam(model.G.parameters())\n",
        "  optimizer_D = torch.optim.Adam(model.D.parameters())\n",
        "  model.set_optimizer(optimizer=optimizer_G, generator=True)\n",
        "  model.set_optimizer(optimizer=optimizer_D, generator=False)\n",
        "\n",
        "  model.to(DEVICE)\n",
        "  model.train(n_epoches=epochs, verbose=True, stopper=stopper)\n",
        "\n",
        "  # Step 5: Use the trained model to predict (impute) target values for df2\n",
        "  predictions = []\n",
        "\n",
        "  for x_test_batch, m_batch in test_loader:\n",
        "      x_batch_imputed = model.imputation(x=x_test_batch, m=m_batch)\n",
        "      x_batch_imputed = x_batch_imputed.cpu().numpy()\n",
        "      predictions.append(x_batch_imputed)\n",
        "\n",
        "  # Combine predictions and inverse transform\n",
        "  predictions_combined = np.vstack(predictions)\n",
        "  # predictions_original_scale = scaler.inverse_transform(predictions_combined)\n",
        "\n",
        "  # Extract the target column predictions\n",
        "  target_column_index = df1.columns.get_loc(target)\n",
        "  df2_predictions = predictions_combined[:, target_column_index]\n",
        "\n",
        "  return df2_predictions"
      ],
      "metadata": {
        "id": "EY7VQ3v_5DZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions from the Torch neural network\n",
        "def get_predictionsTorch(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    # split data for training and testing.\n",
        "    training_features, validation_features, training_target, validation_target = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
        "\n",
        "    training_features = torch.FloatTensor(training_features)\n",
        "    training_target = torch.FloatTensor(training_target)\n",
        "    validation_features = torch.FloatTensor(validation_features)\n",
        "    validation_target = torch.FloatTensor(validation_target)\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(len(predictors), 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "    # MSE loss\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # train the model\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model(training_features)\n",
        "        loss = loss_func(prediction, training_target.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # get predictions\n",
        "    model.eval()\n",
        "    test_data = torch.FloatTensor(df_test[predictors].to_numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_data)\n",
        "        predictions = predictions.numpy().flatten()\n",
        "\n",
        "        val_predictions = model(validation_features)\n",
        "        val_predictions = val_predictions.numpy().flatten()\n",
        "\n",
        "\n",
        "    # exit if correlation between predictions and item is bad\n",
        "    mse = mean_squared_error(val_predictions, validation_target)\n",
        "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
        "    if (mse > 0.03):\n",
        "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
        "        print('Aborting program')\n",
        "        sys.exit(-1)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "_gMeCv-KNks1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # orthogonalization method\n",
        "# all data is preprocessed and df test has been appended target preds\n",
        "def orthogonalize(candidates, df_test, orthogonal_vars):\n",
        "        orth_scores = {}\n",
        "        assert 'proxy_0.95' in df_test.columns, \"Column 'B' does not exist in DataFrame\"\n",
        "        for c in candidates:\n",
        "            candset = df_test[[c, 'predicted_target']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
        "            candcol = candset[c]\n",
        "\n",
        "            X = sm.add_constant(candcol)\n",
        "            temp_orth_scores = []\n",
        "            for orth_var in orthogonal_vars:\n",
        "                orthset = df_test[[orth_var]].copy().dropna()\n",
        "                common_indices = candset.index.intersection(orthset.index)\n",
        "                if common_indices.empty:\n",
        "                    continue\n",
        "                orth_col = orthset.loc[common_indices, orth_var]\n",
        "                if np.var(orth_col) == 0:\n",
        "                    print(\"ortho:\", orth_var, \"candidate\", c)\n",
        "                    continue # zero variance leads to divide by zero error\n",
        "                candcol_common = candset.loc[common_indices, c]\n",
        "\n",
        "                X_common = sm.add_constant(candcol_common)\n",
        "                model = sm.OLS(orth_col, X_common).fit()\n",
        "                temp_orth_scores.append(model.rsquared)\n",
        "\n",
        "            if temp_orth_scores:\n",
        "                orth_scores[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
        "            else:\n",
        "                orth_scores[c] = 0\n",
        "        return orth_scores"
      ],
      "metadata": {
        "id": "Qc5l-fCSkfWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, neural_net=\"original\", drop=True):\n",
        "    if candidates is None:\n",
        "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
        "\n",
        "\n",
        "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
        "\n",
        "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
        "    #print(f\"Candidates: {candidates}\")\n",
        "\n",
        "\n",
        "    # drop any rows that are missing data from target\n",
        "    df_train = df_train.dropna(subset=target)\n",
        "\n",
        "    if neural_net == \"torch\":\n",
        "      predicted_scores = get_predictionsTorch(df_train, df_test, predictors, target)\n",
        "    else:\n",
        "      predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
        "\n",
        "\n",
        "    df_test['predicted_target'] = predicted_scores\n",
        "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c in candidates:\n",
        "        candset = df_test[[c, 'predicted_target']].copy().dropna()\n",
        "        if candset.empty:\n",
        "            continue\n",
        "\n",
        "        pred_scores = candset['predicted_target']\n",
        "        candcol = candset[c]\n",
        "\n",
        "        X_pred = sm.add_constant(candcol)\n",
        "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
        "        results[c] = {\n",
        "            'R_squared': model_pred.rsquared,\n",
        "            'p_value': model_pred.pvalues.iloc[1],\n",
        "            'coef': model_pred.params.iloc[1]\n",
        "        }\n",
        "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------\n",
        "\n",
        "    best_proxies = []\n",
        "\n",
        "    if orthogonal_vars:\n",
        "        orth_scores = orthogonalize(candidates, df_test, orthogonal_vars)\n",
        "        proxy_scores = {}\n",
        "        for c in candidates:\n",
        "            try:\n",
        "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_scores[c])\n",
        "            except KeyError as e:\n",
        "                continue\n",
        "\n",
        "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, score = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
        "    else:\n",
        "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, metrics = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
        "\n",
        "    print(\"PROXY SCORE !!!!!\", proxy_scores['proxy_0.95'])\n",
        "\n",
        "    return best_proxies"
      ],
      "metadata": {
        "id": "a-r5DNEokhf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return a new df that is a copy of df, with: rescale all columns to be\n",
        "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
        "# rows that are missing at least one predictor.\n",
        "def data_rescale(df, df2, predictors, target):\n",
        "    df = df.copy() # preserve immutability\n",
        "    df2 = df2.copy()\n",
        "\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "    df2 = df2.dropna(axis=1, how='all')\n",
        "\n",
        "    print(\"SHAPE AFTER DROPPING ALL NA\", df.shape)\n",
        "\n",
        "    # Select only the numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "    numeric_cols2 = df2.select_dtypes(include=['number']).columns\n",
        "\n",
        "\n",
        "    df = df.dropna(subset=[target])\n",
        "    df2 = df2.dropna(subset=[target])\n",
        "\n",
        "    print(\"SHAPE AFTER DROPPING TARGET\", df.shape)\n",
        "\n",
        "    # # drop any rows that are missing at least one predictor\n",
        "    df = df.dropna(subset=predictors)\n",
        "    print(\"SHAPE AFTER DROPPING PREDS\", df.shape)\n",
        "\n",
        "\n",
        "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
        "    # Initialize the scaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
        "    scaled_values2 = scaler.fit_transform(df2[numeric_cols2])\n",
        "\n",
        "\n",
        "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
        "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
        "    scaled_df2 = pd.DataFrame(scaled_values2, columns=numeric_cols2, index=df2.index)\n",
        "\n",
        "\n",
        "    scaled_df.fillna(scaled_df.mean(), inplace=True)\n",
        "    scaled_df2.fillna(scaled_df2.mean(), inplace=True)\n",
        "\n",
        "\n",
        "    return scaled_df, scaled_df2"
      ],
      "metadata": {
        "id": "ynJNdpekPqTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_proxies(target_column, target_correlation, noise_level=0.1):\n",
        "\n",
        "   # Convert target_column to numpy array and standardize\n",
        "    print(\"HOW MANY NAN\", target_column.isna().sum())\n",
        "\n",
        "    target = np.array(target_column)\n",
        "    length = len(target)\n",
        "\n",
        "    target = (target - np.mean(target)) / np.std(target)\n",
        "    print(\"TARGET LEN\", len(target))\n",
        "\n",
        "    synthetic_proxies = {}\n",
        "\n",
        "    # Generate independent standard normal variable\n",
        "    z = np.random.standard_normal(length)\n",
        "\n",
        "    # Create correlated variable using the correlation formula\n",
        "    proxy = target_correlation * target + np.sqrt(1 - target_correlation**2) * z\n",
        "\n",
        "    # Add controlled noise\n",
        "    proxy = proxy + np.random.normal(0, noise_level, length)\n",
        "\n",
        "    # Standardize final proxy\n",
        "    proxy = (proxy - np.mean(proxy)) / np.std(proxy)\n",
        "\n",
        "    synthetic_proxies[f'proxy_{target_correlation:.2f}'] = proxy\n",
        "\n",
        "    return synthetic_proxies"
      ],
      "metadata": {
        "id": "WabG6t15Pr9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df, target, target_correlation):\n",
        "  df = df.copy()\n",
        "\n",
        "  # add synthetic proxies to test set\n",
        "  target_column = df[target]\n",
        "  synthetic_proxies = generate_synthetic_proxies(target_column, target_correlation)\n",
        "  for name, proxy in synthetic_proxies.items():\n",
        "    df[name] = proxy\n",
        "\n",
        "  # drop target from test set\n",
        "  df = df.drop(columns=[target])\n",
        "  assert 'proxy_0.95' in df.columns, \"Column 'B' does not exist in DataFrame\"\n",
        "\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "jLnpMEMFKdo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnUMZlYeqfW"
      },
      "source": [
        "# Stage 1: Testing Mean Penalty Approach with Several Target Correlations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_visualize_monte_carlo(df1, df2, weights, num_iterations, target, target_correlations, predictors, neural_net):\n",
        "    selection_trackers = []\n",
        "    proxy_names = []\n",
        "\n",
        "\n",
        "    # Run Monte Carlo for each target correlation\n",
        "    for target_correlation in target_correlations:\n",
        "\n",
        "        # rescale data\n",
        "        df1, df2 = data_rescale(df1, df2, predictors, target)\n",
        "\n",
        "        df2 = prepare_dataset(df2, target, target_correlation)\n",
        "\n",
        "        selection_tracker = {orth_weight: {} for orth_weight in weights}\n",
        "        assert 'proxy_0.95' in df2.columns, \"Column 'B' does not exist in DataFrame\"\n",
        "\n",
        "        # Run iterations for each weight\n",
        "        for orth_weight in weights:\n",
        "            print(f\"Testing with orthogonality weight: {orth_weight}\")\n",
        "            print(f\"Testing with target correlation: {target_correlation}\")\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                print(f\"Running iteration {i+1}/{num_iterations}\")\n",
        "                top_proxies = proxy_finder(df_train=df1,\n",
        "                                         df_test=df2,\n",
        "                                         target=target,\n",
        "                                         predictors=predictors,\n",
        "                                         num_proxies=50,\n",
        "                                         orth_weight=orth_weight,\n",
        "                                         orthogonal_vars=predictors,\n",
        "                                         neural_net=neural_net\n",
        "                                         )\n",
        "\n",
        "                # Update selection tracker for top pick\n",
        "                for rank, proxy in enumerate(top_proxies, 1):\n",
        "                    if rank == 1:\n",
        "                        selection_tracker[orth_weight][proxy] = selection_tracker[orth_weight].get(proxy, 0) + 1\n",
        "\n",
        "        selection_trackers.append(selection_tracker)\n",
        "        proxy_names.append(f'proxy_{target_correlation:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "    # SAVE TO CSV --------------------\n",
        "    data = []\n",
        "\n",
        "    for target_correlation, selection_tracker in zip(proxy_names, selection_trackers):\n",
        "        for orth_weight, proxies in selection_tracker.items():\n",
        "            for proxy, count in proxies.items():\n",
        "                data.append({\n",
        "                    'Target Correlation': target_correlation,\n",
        "                    'Orthogonality Weight': orth_weight,\n",
        "                    'Proxy': proxy,\n",
        "                    'Count': count\n",
        "                })\n",
        "\n",
        "    df_selection_tracker = pd.DataFrame(data)\n",
        "    df_selection_tracker.to_csv('selection_tracker.csv', index=False)\n",
        "    # SAVE TO CSV --------------------\n",
        "\n",
        "\n",
        "    # # Visualization\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # # Plot results for each target correlation\n",
        "    # for index, tracker in enumerate(selection_trackers):\n",
        "    #     results = []\n",
        "    #     for orth_weight, proxies in tracker.items():\n",
        "    #         for proxy, frequency in proxies.items():\n",
        "    #             results.append({\n",
        "    #                 'orth_weight': orth_weight,\n",
        "    #                 'proxy': proxy,\n",
        "    #                 'frequency': (frequency / num_iterations) * 100\n",
        "    #             })\n",
        "\n",
        "    #     results_df = pd.DataFrame(results)\n",
        "    #     pivot_data = results_df.pivot(index='orth_weight', columns='proxy', values='frequency')\n",
        "    #     pivot_data.fillna(0, inplace=True)\n",
        "    #     print(pivot_data)\n",
        "\n",
        "    #     # Plot each proxy as a separate line\n",
        "    #     name = proxy_names[index]\n",
        "    #     plt.plot(pivot_data.index, pivot_data[name], marker='o', label=name, linewidth=2)\n",
        "\n",
        "    # # Create the line plot\n",
        "    # plt.xlabel('Orthogonality Weight')\n",
        "    # plt.ylabel('Selection Frequency')\n",
        "    # plt.title('Selection Frequency vs Orthogonality Weight')\n",
        "    # plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "fgTQXgXRSS33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change parameters as needed\n",
        "df1 = pd.read_stata(\"/content/yougov_recode_03052025.dta\")\n",
        "df2 = pd.read_stata(\"/content/anes_recode_03052025.dta\")\n",
        "#weights = [0, 0.55, 0.56, 0.57, 0.58, 0.59, 0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70, 0.71, 0.72, 0.73, 0.74, 0.75]\n",
        "weights = [0.65]\n",
        "target_correlations = [0.95]\n",
        "num_iterations = 1\n",
        "target = 'educ'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                  # 'educ',\n",
        "                   'ideo7',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'abortion',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport'\n",
        "                   ]\n",
        "run_and_visualize_monte_carlo(df1, df2, weights, num_iterations, target, target_correlations, predictors, neural_net=\"GAIN\")"
      ],
      "metadata": {
        "id": "NdGZvvMUSzCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b195c8-708a-4aad-f2c1-958b1ea9f415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAPE AFTER DROPPING ALL NA (3400, 22)\n",
            "SHAPE AFTER DROPPING TARGET (2442, 22)\n",
            "SHAPE AFTER DROPPING PREDS (1312, 22)\n",
            "HOW MANY NAN 0\n",
            "TARGET LEN 8149\n",
            "Testing with orthogonality weight: 0.65\n",
            "Testing with target correlation: 0.95\n",
            "Running iteration 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 11/11 [00:00<00:00, 83.65batch/s, mse_test=nan, mse_train=0.193]\n",
            "Epoch 1: 100%|██████████| 11/11 [00:00<00:00, 97.15batch/s, mse_test=nan, mse_train=0.189]\n",
            "Epoch 2: 100%|██████████| 11/11 [00:00<00:00, 73.47batch/s, mse_test=nan, mse_train=0.185]\n",
            "Epoch 3: 100%|██████████| 11/11 [00:00<00:00, 81.83batch/s, mse_test=nan, mse_train=0.179]\n",
            "Epoch 4: 100%|██████████| 11/11 [00:00<00:00, 100.46batch/s, mse_test=nan, mse_train=0.171]\n",
            "Epoch 5: 100%|██████████| 11/11 [00:00<00:00, 73.40batch/s, mse_test=nan, mse_train=0.16]\n",
            "Epoch 6: 100%|██████████| 11/11 [00:00<00:00, 70.18batch/s, mse_test=nan, mse_train=0.147]\n",
            "Epoch 7: 100%|██████████| 11/11 [00:00<00:00, 53.68batch/s, mse_test=nan, mse_train=0.133]\n",
            "Epoch 8: 100%|██████████| 11/11 [00:00<00:00, 52.09batch/s, mse_test=nan, mse_train=0.12]\n",
            "Epoch 9: 100%|██████████| 11/11 [00:00<00:00, 62.16batch/s, mse_test=nan, mse_train=0.11]\n",
            "Epoch 10: 100%|██████████| 11/11 [00:00<00:00, 56.15batch/s, mse_test=nan, mse_train=0.102]\n",
            "Epoch 11: 100%|██████████| 11/11 [00:00<00:00, 53.92batch/s, mse_test=nan, mse_train=0.0963]\n",
            "Epoch 12: 100%|██████████| 11/11 [00:00<00:00, 71.48batch/s, mse_test=nan, mse_train=0.092]\n",
            "Epoch 13: 100%|██████████| 11/11 [00:00<00:00, 62.92batch/s, mse_test=nan, mse_train=0.0889]\n",
            "Epoch 14: 100%|██████████| 11/11 [00:00<00:00, 59.07batch/s, mse_test=nan, mse_train=0.0867]\n",
            "Epoch 15: 100%|██████████| 11/11 [00:00<00:00, 73.03batch/s, mse_test=nan, mse_train=0.0852]\n",
            "Epoch 16: 100%|██████████| 11/11 [00:00<00:00, 77.75batch/s, mse_test=nan, mse_train=0.0841]\n",
            "Epoch 17: 100%|██████████| 11/11 [00:00<00:00, 78.60batch/s, mse_test=nan, mse_train=0.0832]\n",
            "Epoch 18: 100%|██████████| 11/11 [00:00<00:00, 66.06batch/s, mse_test=nan, mse_train=0.0825]\n",
            "Epoch 19: 100%|██████████| 11/11 [00:00<00:00, 63.99batch/s, mse_test=nan, mse_train=0.0819]\n",
            "Epoch 20: 100%|██████████| 11/11 [00:00<00:00, 65.50batch/s, mse_test=nan, mse_train=0.0814]\n",
            "Epoch 21: 100%|██████████| 11/11 [00:00<00:00, 75.04batch/s, mse_test=nan, mse_train=0.0809]\n",
            "Epoch 22: 100%|██████████| 11/11 [00:00<00:00, 53.80batch/s, mse_test=nan, mse_train=0.0805]\n",
            "Epoch 23: 100%|██████████| 11/11 [00:00<00:00, 54.41batch/s, mse_test=nan, mse_train=0.08]\n",
            "Epoch 24: 100%|██████████| 11/11 [00:00<00:00, 60.99batch/s, mse_test=nan, mse_train=0.0795]\n",
            "Epoch 25: 100%|██████████| 11/11 [00:00<00:00, 63.99batch/s, mse_test=nan, mse_train=0.0791]\n",
            "Epoch 26: 100%|██████████| 11/11 [00:00<00:00, 66.32batch/s, mse_test=nan, mse_train=0.0785]\n",
            "Epoch 27: 100%|██████████| 11/11 [00:00<00:00, 51.57batch/s, mse_test=nan, mse_train=0.078]\n",
            "Epoch 28: 100%|██████████| 11/11 [00:00<00:00, 68.77batch/s, mse_test=nan, mse_train=0.0774]\n",
            "Epoch 29: 100%|██████████| 11/11 [00:00<00:00, 68.56batch/s, mse_test=nan, mse_train=0.0768]\n",
            "Epoch 30: 100%|██████████| 11/11 [00:00<00:00, 59.33batch/s, mse_test=nan, mse_train=0.0761]\n",
            "Epoch 31: 100%|██████████| 11/11 [00:00<00:00, 57.35batch/s, mse_test=nan, mse_train=0.0754]\n",
            "Epoch 32: 100%|██████████| 11/11 [00:00<00:00, 65.46batch/s, mse_test=nan, mse_train=0.0747]\n",
            "Epoch 33: 100%|██████████| 11/11 [00:00<00:00, 66.02batch/s, mse_test=nan, mse_train=0.0739]\n",
            "Epoch 34: 100%|██████████| 11/11 [00:00<00:00, 61.05batch/s, mse_test=nan, mse_train=0.0731]\n",
            "Epoch 35: 100%|██████████| 11/11 [00:00<00:00, 67.96batch/s, mse_test=nan, mse_train=0.0723]\n",
            "Epoch 36: 100%|██████████| 11/11 [00:00<00:00, 67.85batch/s, mse_test=nan, mse_train=0.0714]\n",
            "Epoch 37: 100%|██████████| 11/11 [00:00<00:00, 82.43batch/s, mse_test=nan, mse_train=0.0705]\n",
            "Epoch 38: 100%|██████████| 11/11 [00:00<00:00, 80.84batch/s, mse_test=nan, mse_train=0.0695]\n",
            "Epoch 39: 100%|██████████| 11/11 [00:00<00:00, 84.08batch/s, mse_test=nan, mse_train=0.0682]\n",
            "Epoch 40: 100%|██████████| 11/11 [00:00<00:00, 90.60batch/s, mse_test=nan, mse_train=0.0671]\n",
            "Epoch 41: 100%|██████████| 11/11 [00:00<00:00, 94.57batch/s, mse_test=nan, mse_train=0.0659]\n",
            "Epoch 42: 100%|██████████| 11/11 [00:00<00:00, 87.41batch/s, mse_test=nan, mse_train=0.0647]\n",
            "Epoch 43: 100%|██████████| 11/11 [00:00<00:00, 76.71batch/s, mse_test=nan, mse_train=0.0636]\n",
            "Epoch 44: 100%|██████████| 11/11 [00:00<00:00, 54.70batch/s, mse_test=nan, mse_train=0.0625]\n",
            "Epoch 45: 100%|██████████| 11/11 [00:00<00:00, 62.64batch/s, mse_test=nan, mse_train=0.0615]\n",
            "Epoch 46: 100%|██████████| 11/11 [00:00<00:00, 69.71batch/s, mse_test=nan, mse_train=0.0605]\n",
            "Epoch 47: 100%|██████████| 11/11 [00:00<00:00, 80.13batch/s, mse_test=nan, mse_train=0.0597]\n",
            "Epoch 48: 100%|██████████| 11/11 [00:00<00:00, 65.38batch/s, mse_test=nan, mse_train=0.0588]\n",
            "Epoch 49: 100%|██████████| 11/11 [00:00<00:00, 65.82batch/s, mse_test=nan, mse_train=0.0581]\n",
            "Epoch 50: 100%|██████████| 11/11 [00:00<00:00, 65.65batch/s, mse_test=nan, mse_train=0.0573]\n",
            "Epoch 51: 100%|██████████| 11/11 [00:00<00:00, 65.85batch/s, mse_test=nan, mse_train=0.0567]\n",
            "Epoch 52: 100%|██████████| 11/11 [00:00<00:00, 64.38batch/s, mse_test=nan, mse_train=0.056]\n",
            "Epoch 53: 100%|██████████| 11/11 [00:00<00:00, 86.25batch/s, mse_test=nan, mse_train=0.0554]\n",
            "Epoch 54: 100%|██████████| 11/11 [00:00<00:00, 82.99batch/s, mse_test=nan, mse_train=0.0549]\n",
            "Epoch 55: 100%|██████████| 11/11 [00:00<00:00, 92.69batch/s, mse_test=nan, mse_train=0.0543]\n",
            "Epoch 56: 100%|██████████| 11/11 [00:00<00:00, 63.47batch/s, mse_test=nan, mse_train=0.0538]\n",
            "Epoch 57: 100%|██████████| 11/11 [00:00<00:00, 63.83batch/s, mse_test=nan, mse_train=0.0533]\n",
            "Epoch 58: 100%|██████████| 11/11 [00:00<00:00, 59.97batch/s, mse_test=nan, mse_train=0.0528]\n",
            "Epoch 59: 100%|██████████| 11/11 [00:00<00:00, 74.42batch/s, mse_test=nan, mse_train=0.0524]\n",
            "Epoch 60: 100%|██████████| 11/11 [00:00<00:00, 93.78batch/s, mse_test=nan, mse_train=0.0519]\n",
            "Epoch 61: 100%|██████████| 11/11 [00:00<00:00, 95.63batch/s, mse_test=nan, mse_train=0.0514]\n",
            "Epoch 62: 100%|██████████| 11/11 [00:00<00:00, 76.31batch/s, mse_test=nan, mse_train=0.0509]\n",
            "Epoch 63: 100%|██████████| 11/11 [00:00<00:00, 68.42batch/s, mse_test=nan, mse_train=0.0504]\n",
            "Epoch 64: 100%|██████████| 11/11 [00:00<00:00, 65.66batch/s, mse_test=nan, mse_train=0.0499]\n",
            "Epoch 65: 100%|██████████| 11/11 [00:00<00:00, 78.99batch/s, mse_test=nan, mse_train=0.0494]\n",
            "Epoch 66: 100%|██████████| 11/11 [00:00<00:00, 81.04batch/s, mse_test=nan, mse_train=0.0488]\n",
            "Epoch 67: 100%|██████████| 11/11 [00:00<00:00, 87.75batch/s, mse_test=nan, mse_train=0.0483]\n",
            "Epoch 68: 100%|██████████| 11/11 [00:00<00:00, 83.33batch/s, mse_test=nan, mse_train=0.0477]\n",
            "Epoch 69: 100%|██████████| 11/11 [00:00<00:00, 74.04batch/s, mse_test=nan, mse_train=0.0471]\n",
            "Epoch 70: 100%|██████████| 11/11 [00:00<00:00, 67.40batch/s, mse_test=nan, mse_train=0.0465]\n",
            "Epoch 71: 100%|██████████| 11/11 [00:00<00:00, 54.95batch/s, mse_test=nan, mse_train=0.0459]\n",
            "Epoch 72: 100%|██████████| 11/11 [00:00<00:00, 77.36batch/s, mse_test=nan, mse_train=0.0453]\n",
            "Epoch 73: 100%|██████████| 11/11 [00:00<00:00, 74.71batch/s, mse_test=nan, mse_train=0.0448]\n",
            "Epoch 74: 100%|██████████| 11/11 [00:00<00:00, 85.07batch/s, mse_test=nan, mse_train=0.0444]\n",
            "Epoch 75: 100%|██████████| 11/11 [00:00<00:00, 79.29batch/s, mse_test=nan, mse_train=0.0439]\n",
            "Epoch 76: 100%|██████████| 11/11 [00:00<00:00, 62.95batch/s, mse_test=nan, mse_train=0.0434]\n",
            "Epoch 77: 100%|██████████| 11/11 [00:00<00:00, 65.23batch/s, mse_test=nan, mse_train=0.043]\n",
            "Epoch 78: 100%|██████████| 11/11 [00:00<00:00, 68.11batch/s, mse_test=nan, mse_train=0.0425]\n",
            "Epoch 79: 100%|██████████| 11/11 [00:00<00:00, 75.35batch/s, mse_test=nan, mse_train=0.0421]\n",
            "Epoch 80: 100%|██████████| 11/11 [00:00<00:00, 88.16batch/s, mse_test=nan, mse_train=0.0417]\n",
            "Epoch 81: 100%|██████████| 11/11 [00:00<00:00, 88.53batch/s, mse_test=nan, mse_train=0.0412]\n",
            "Epoch 82: 100%|██████████| 11/11 [00:00<00:00, 66.05batch/s, mse_test=nan, mse_train=0.0408]\n",
            "Epoch 83: 100%|██████████| 11/11 [00:00<00:00, 69.95batch/s, mse_test=nan, mse_train=0.0403]\n",
            "Epoch 84: 100%|██████████| 11/11 [00:00<00:00, 66.79batch/s, mse_test=nan, mse_train=0.0399]\n",
            "Epoch 85: 100%|██████████| 11/11 [00:00<00:00, 80.57batch/s, mse_test=nan, mse_train=0.0393]\n",
            "Epoch 86: 100%|██████████| 11/11 [00:00<00:00, 69.01batch/s, mse_test=nan, mse_train=0.0388]\n",
            "Epoch 87: 100%|██████████| 11/11 [00:00<00:00, 69.84batch/s, mse_test=nan, mse_train=0.0382]\n",
            "Epoch 88: 100%|██████████| 11/11 [00:00<00:00, 55.83batch/s, mse_test=nan, mse_train=0.0376]\n",
            "Epoch 89: 100%|██████████| 11/11 [00:00<00:00, 51.98batch/s, mse_test=nan, mse_train=0.0371]\n",
            "Epoch 90: 100%|██████████| 11/11 [00:00<00:00, 65.28batch/s, mse_test=nan, mse_train=0.0365]\n",
            "Epoch 91: 100%|██████████| 11/11 [00:00<00:00, 90.27batch/s, mse_test=nan, mse_train=0.036]\n",
            "Epoch 92: 100%|██████████| 11/11 [00:00<00:00, 79.63batch/s, mse_test=nan, mse_train=0.0355]\n",
            "Epoch 93: 100%|██████████| 11/11 [00:00<00:00, 83.64batch/s, mse_test=nan, mse_train=0.0349]\n",
            "Epoch 94: 100%|██████████| 11/11 [00:00<00:00, 78.36batch/s, mse_test=nan, mse_train=0.0344]\n",
            "Epoch 95: 100%|██████████| 11/11 [00:00<00:00, 61.26batch/s, mse_test=nan, mse_train=0.0339]\n",
            "Epoch 96: 100%|██████████| 11/11 [00:00<00:00, 63.88batch/s, mse_test=nan, mse_train=0.0334]\n",
            "Epoch 97: 100%|██████████| 11/11 [00:00<00:00, 65.28batch/s, mse_test=nan, mse_train=0.0329]\n",
            "Epoch 98: 100%|██████████| 11/11 [00:00<00:00, 68.47batch/s, mse_test=nan, mse_train=0.0324]\n",
            "Epoch 99: 100%|██████████| 11/11 [00:00<00:00, 76.05batch/s, mse_test=nan, mse_train=0.0319]\n",
            "Epoch 100: 100%|██████████| 11/11 [00:00<00:00, 67.65batch/s, mse_test=nan, mse_train=0.0315]\n",
            "Epoch 101: 100%|██████████| 11/11 [00:00<00:00, 74.77batch/s, mse_test=nan, mse_train=0.031]\n",
            "Epoch 102: 100%|██████████| 11/11 [00:00<00:00, 72.54batch/s, mse_test=nan, mse_train=0.0306]\n",
            "Epoch 103: 100%|██████████| 11/11 [00:00<00:00, 72.79batch/s, mse_test=nan, mse_train=0.0302]\n",
            "Epoch 104: 100%|██████████| 11/11 [00:00<00:00, 67.88batch/s, mse_test=nan, mse_train=0.0298]\n",
            "Epoch 105: 100%|██████████| 11/11 [00:00<00:00, 71.23batch/s, mse_test=nan, mse_train=0.0294]\n",
            "Epoch 106: 100%|██████████| 11/11 [00:00<00:00, 74.89batch/s, mse_test=nan, mse_train=0.0291]\n",
            "Epoch 107: 100%|██████████| 11/11 [00:00<00:00, 47.11batch/s, mse_test=nan, mse_train=0.0288]\n",
            "Epoch 108: 100%|██████████| 11/11 [00:00<00:00, 57.88batch/s, mse_test=nan, mse_train=0.0285]\n",
            "Epoch 109: 100%|██████████| 11/11 [00:00<00:00, 49.99batch/s, mse_test=nan, mse_train=0.0282]\n",
            "Epoch 110: 100%|██████████| 11/11 [00:00<00:00, 50.12batch/s, mse_test=nan, mse_train=0.0279]\n",
            "Epoch 111: 100%|██████████| 11/11 [00:00<00:00, 75.38batch/s, mse_test=nan, mse_train=0.0277]\n",
            "Epoch 112: 100%|██████████| 11/11 [00:00<00:00, 46.49batch/s, mse_test=nan, mse_train=0.0274]\n",
            "Epoch 113: 100%|██████████| 11/11 [00:00<00:00, 70.71batch/s, mse_test=nan, mse_train=0.0272]\n",
            "Epoch 114: 100%|██████████| 11/11 [00:00<00:00, 67.27batch/s, mse_test=nan, mse_train=0.027]\n",
            "Epoch 115: 100%|██████████| 11/11 [00:00<00:00, 78.95batch/s, mse_test=nan, mse_train=0.0268]\n",
            "Epoch 116: 100%|██████████| 11/11 [00:00<00:00, 88.32batch/s, mse_test=nan, mse_train=0.0266]\n",
            "Epoch 117: 100%|██████████| 11/11 [00:00<00:00, 81.08batch/s, mse_test=nan, mse_train=0.0264]\n",
            "Epoch 118: 100%|██████████| 11/11 [00:00<00:00, 50.57batch/s, mse_test=nan, mse_train=0.0262]\n",
            "Epoch 119: 100%|██████████| 11/11 [00:00<00:00, 47.96batch/s, mse_test=nan, mse_train=0.026]\n",
            "Epoch 120: 100%|██████████| 11/11 [00:00<00:00, 49.74batch/s, mse_test=nan, mse_train=0.0258]\n",
            "Epoch 121: 100%|██████████| 11/11 [00:00<00:00, 50.51batch/s, mse_test=nan, mse_train=0.0256]\n",
            "Epoch 122: 100%|██████████| 11/11 [00:00<00:00, 54.80batch/s, mse_test=nan, mse_train=0.0255]\n",
            "Epoch 123: 100%|██████████| 11/11 [00:00<00:00, 49.88batch/s, mse_test=nan, mse_train=0.0253]\n",
            "Epoch 124: 100%|██████████| 11/11 [00:00<00:00, 54.56batch/s, mse_test=nan, mse_train=0.0251]\n",
            "Epoch 125: 100%|██████████| 11/11 [00:00<00:00, 72.97batch/s, mse_test=nan, mse_train=0.0249]\n",
            "Epoch 126: 100%|██████████| 11/11 [00:00<00:00, 74.18batch/s, mse_test=nan, mse_train=0.0247]\n",
            "Epoch 127: 100%|██████████| 11/11 [00:00<00:00, 67.04batch/s, mse_test=nan, mse_train=0.0245]\n",
            "Epoch 128: 100%|██████████| 11/11 [00:00<00:00, 46.53batch/s, mse_test=nan, mse_train=0.0244]\n",
            "Epoch 129: 100%|██████████| 11/11 [00:00<00:00, 64.89batch/s, mse_test=nan, mse_train=0.0242]\n",
            "Epoch 130: 100%|██████████| 11/11 [00:00<00:00, 59.60batch/s, mse_test=nan, mse_train=0.024]\n",
            "Epoch 131: 100%|██████████| 11/11 [00:00<00:00, 67.42batch/s, mse_test=nan, mse_train=0.0238]\n",
            "Epoch 132: 100%|██████████| 11/11 [00:00<00:00, 70.15batch/s, mse_test=nan, mse_train=0.0236]\n",
            "Epoch 133: 100%|██████████| 11/11 [00:00<00:00, 57.34batch/s, mse_test=nan, mse_train=0.0234]\n",
            "Epoch 134: 100%|██████████| 11/11 [00:00<00:00, 51.91batch/s, mse_test=nan, mse_train=0.0232]\n",
            "Epoch 135: 100%|██████████| 11/11 [00:00<00:00, 49.95batch/s, mse_test=nan, mse_train=0.023]\n",
            "Epoch 136: 100%|██████████| 11/11 [00:00<00:00, 60.07batch/s, mse_test=nan, mse_train=0.0228]\n",
            "Epoch 137: 100%|██████████| 11/11 [00:00<00:00, 61.15batch/s, mse_test=nan, mse_train=0.0226]\n",
            "Epoch 138: 100%|██████████| 11/11 [00:00<00:00, 45.81batch/s, mse_test=nan, mse_train=0.0224]\n",
            "Epoch 139: 100%|██████████| 11/11 [00:00<00:00, 54.68batch/s, mse_test=nan, mse_train=0.0222]\n",
            "Epoch 140: 100%|██████████| 11/11 [00:00<00:00, 67.46batch/s, mse_test=nan, mse_train=0.022]\n",
            "Epoch 141: 100%|██████████| 11/11 [00:00<00:00, 78.62batch/s, mse_test=nan, mse_train=0.0218]\n",
            "Epoch 142: 100%|██████████| 11/11 [00:00<00:00, 82.15batch/s, mse_test=nan, mse_train=0.0215]\n",
            "Epoch 143: 100%|██████████| 11/11 [00:00<00:00, 61.43batch/s, mse_test=nan, mse_train=0.0213]\n",
            "Epoch 144: 100%|██████████| 11/11 [00:00<00:00, 54.05batch/s, mse_test=nan, mse_train=0.0211]\n",
            "Epoch 145: 100%|██████████| 11/11 [00:00<00:00, 51.54batch/s, mse_test=nan, mse_train=0.0209]\n",
            "Epoch 146: 100%|██████████| 11/11 [00:00<00:00, 61.89batch/s, mse_test=nan, mse_train=0.0207]\n",
            "Epoch 147: 100%|██████████| 11/11 [00:00<00:00, 69.95batch/s, mse_test=nan, mse_train=0.0205]\n",
            "Epoch 148: 100%|██████████| 11/11 [00:00<00:00, 75.56batch/s, mse_test=nan, mse_train=0.0203]\n",
            "Epoch 149: 100%|██████████| 11/11 [00:00<00:00, 56.85batch/s, mse_test=nan, mse_train=0.0201]\n",
            "Epoch 150: 100%|██████████| 11/11 [00:00<00:00, 59.07batch/s, mse_test=nan, mse_train=0.02]\n",
            "Epoch 151: 100%|██████████| 11/11 [00:00<00:00, 61.84batch/s, mse_test=nan, mse_train=0.0198]\n",
            "Epoch 152: 100%|██████████| 11/11 [00:00<00:00, 52.35batch/s, mse_test=nan, mse_train=0.0196]\n",
            "Epoch 153: 100%|██████████| 11/11 [00:00<00:00, 73.51batch/s, mse_test=nan, mse_train=0.0194]\n",
            "Epoch 154: 100%|██████████| 11/11 [00:00<00:00, 71.54batch/s, mse_test=nan, mse_train=0.0192]\n",
            "Epoch 155: 100%|██████████| 11/11 [00:00<00:00, 62.43batch/s, mse_test=nan, mse_train=0.0191]\n",
            "Epoch 156: 100%|██████████| 11/11 [00:00<00:00, 72.60batch/s, mse_test=nan, mse_train=0.0189]\n",
            "Epoch 157: 100%|██████████| 11/11 [00:00<00:00, 76.75batch/s, mse_test=nan, mse_train=0.0187]\n",
            "Epoch 158: 100%|██████████| 11/11 [00:00<00:00, 73.65batch/s, mse_test=nan, mse_train=0.0186]\n",
            "Epoch 159: 100%|██████████| 11/11 [00:00<00:00, 70.80batch/s, mse_test=nan, mse_train=0.0184]\n",
            "Epoch 160: 100%|██████████| 11/11 [00:00<00:00, 90.78batch/s, mse_test=nan, mse_train=0.0183]\n",
            "Epoch 161: 100%|██████████| 11/11 [00:00<00:00, 63.96batch/s, mse_test=nan, mse_train=0.0181]\n",
            "Epoch 162: 100%|██████████| 11/11 [00:00<00:00, 67.34batch/s, mse_test=nan, mse_train=0.018]\n",
            "Epoch 163: 100%|██████████| 11/11 [00:00<00:00, 45.78batch/s, mse_test=nan, mse_train=0.0178]\n",
            "Epoch 164: 100%|██████████| 11/11 [00:00<00:00, 50.00batch/s, mse_test=nan, mse_train=0.0177]\n",
            "Epoch 165: 100%|██████████| 11/11 [00:00<00:00, 50.45batch/s, mse_test=nan, mse_train=0.0175]\n",
            "Epoch 166: 100%|██████████| 11/11 [00:00<00:00, 70.87batch/s, mse_test=nan, mse_train=0.0174]\n",
            "Epoch 167: 100%|██████████| 11/11 [00:00<00:00, 69.61batch/s, mse_test=nan, mse_train=0.0173]\n",
            "Epoch 168: 100%|██████████| 11/11 [00:00<00:00, 79.74batch/s, mse_test=nan, mse_train=0.0172]\n",
            "Epoch 169: 100%|██████████| 11/11 [00:00<00:00, 81.31batch/s, mse_test=nan, mse_train=0.0171]\n",
            "Epoch 170: 100%|██████████| 11/11 [00:00<00:00, 66.67batch/s, mse_test=nan, mse_train=0.0169]\n",
            "Epoch 171: 100%|██████████| 11/11 [00:00<00:00, 67.71batch/s, mse_test=nan, mse_train=0.0168]\n",
            "Epoch 172: 100%|██████████| 11/11 [00:00<00:00, 62.15batch/s, mse_test=nan, mse_train=0.0167]\n",
            "Epoch 173: 100%|██████████| 11/11 [00:00<00:00, 62.89batch/s, mse_test=nan, mse_train=0.0166]\n",
            "Epoch 174: 100%|██████████| 11/11 [00:00<00:00, 45.79batch/s, mse_test=nan, mse_train=0.0165]\n",
            "Epoch 175: 100%|██████████| 11/11 [00:00<00:00, 53.93batch/s, mse_test=nan, mse_train=0.0164]\n",
            "Epoch 176: 100%|██████████| 11/11 [00:00<00:00, 48.13batch/s, mse_test=nan, mse_train=0.0164]\n",
            "Epoch 177: 100%|██████████| 11/11 [00:00<00:00, 56.63batch/s, mse_test=nan, mse_train=0.0163]\n",
            "Epoch 178: 100%|██████████| 11/11 [00:00<00:00, 72.25batch/s, mse_test=nan, mse_train=0.0162]\n",
            "Epoch 179: 100%|██████████| 11/11 [00:00<00:00, 79.17batch/s, mse_test=nan, mse_train=0.0161]\n",
            "Epoch 180: 100%|██████████| 11/11 [00:00<00:00, 70.73batch/s, mse_test=nan, mse_train=0.0161]\n",
            "Epoch 181: 100%|██████████| 11/11 [00:00<00:00, 80.53batch/s, mse_test=nan, mse_train=0.016]\n",
            "Epoch 182: 100%|██████████| 11/11 [00:00<00:00, 82.22batch/s, mse_test=nan, mse_train=0.0159]\n",
            "Epoch 183: 100%|██████████| 11/11 [00:00<00:00, 68.15batch/s, mse_test=nan, mse_train=0.0159]\n",
            "Epoch 184: 100%|██████████| 11/11 [00:00<00:00, 72.45batch/s, mse_test=nan, mse_train=0.0158]\n",
            "Epoch 185: 100%|██████████| 11/11 [00:00<00:00, 67.28batch/s, mse_test=nan, mse_train=0.0157]\n",
            "Epoch 186: 100%|██████████| 11/11 [00:00<00:00, 52.48batch/s, mse_test=nan, mse_train=0.0157]\n",
            "Epoch 187: 100%|██████████| 11/11 [00:00<00:00, 68.45batch/s, mse_test=nan, mse_train=0.0156]\n",
            "Epoch 188: 100%|██████████| 11/11 [00:00<00:00, 78.01batch/s, mse_test=nan, mse_train=0.0156]\n",
            "Epoch 189: 100%|██████████| 11/11 [00:00<00:00, 69.54batch/s, mse_test=nan, mse_train=0.0155]\n",
            "Epoch 190: 100%|██████████| 11/11 [00:00<00:00, 66.05batch/s, mse_test=nan, mse_train=0.0155]\n",
            "Epoch 191: 100%|██████████| 11/11 [00:00<00:00, 75.01batch/s, mse_test=nan, mse_train=0.0154]\n",
            "Epoch 192: 100%|██████████| 11/11 [00:00<00:00, 76.28batch/s, mse_test=nan, mse_train=0.0154]\n",
            "Epoch 193: 100%|██████████| 11/11 [00:00<00:00, 71.23batch/s, mse_test=nan, mse_train=0.0154]\n",
            "Epoch 194: 100%|██████████| 11/11 [00:00<00:00, 88.75batch/s, mse_test=nan, mse_train=0.0153]\n",
            "Epoch 195: 100%|██████████| 11/11 [00:00<00:00, 79.75batch/s, mse_test=nan, mse_train=0.0153]\n",
            "Epoch 196: 100%|██████████| 11/11 [00:00<00:00, 68.50batch/s, mse_test=nan, mse_train=0.0152]\n",
            "Epoch 197: 100%|██████████| 11/11 [00:00<00:00, 75.89batch/s, mse_test=nan, mse_train=0.0152]\n",
            "Epoch 198: 100%|██████████| 11/11 [00:00<00:00, 77.22batch/s, mse_test=nan, mse_train=0.0152]\n",
            "Epoch 199: 100%|██████████| 11/11 [00:00<00:00, 80.69batch/s, mse_test=nan, mse_train=0.0151]\n",
            "Epoch 200: 100%|██████████| 11/11 [00:00<00:00, 92.44batch/s, mse_test=nan, mse_train=0.0151]\n",
            "Epoch 201: 100%|██████████| 11/11 [00:00<00:00, 93.19batch/s, mse_test=nan, mse_train=0.0151]\n",
            "Epoch 202: 100%|██████████| 11/11 [00:00<00:00, 92.15batch/s, mse_test=nan, mse_train=0.015]\n",
            "Epoch 203: 100%|██████████| 11/11 [00:00<00:00, 83.97batch/s, mse_test=nan, mse_train=0.015]\n",
            "Epoch 204: 100%|██████████| 11/11 [00:00<00:00, 81.90batch/s, mse_test=nan, mse_train=0.015]\n",
            "Epoch 205: 100%|██████████| 11/11 [00:00<00:00, 74.68batch/s, mse_test=nan, mse_train=0.0149]\n",
            "Epoch 206: 100%|██████████| 11/11 [00:00<00:00, 62.31batch/s, mse_test=nan, mse_train=0.0149]\n",
            "Epoch 207: 100%|██████████| 11/11 [00:00<00:00, 77.88batch/s, mse_test=nan, mse_train=0.0149]\n",
            "Epoch 208: 100%|██████████| 11/11 [00:00<00:00, 94.87batch/s, mse_test=nan, mse_train=0.0148]\n",
            "Epoch 209: 100%|██████████| 11/11 [00:00<00:00, 94.36batch/s, mse_test=nan, mse_train=0.0148]\n",
            "Epoch 210: 100%|██████████| 11/11 [00:00<00:00, 79.12batch/s, mse_test=nan, mse_train=0.0148]\n",
            "Epoch 211: 100%|██████████| 11/11 [00:00<00:00, 75.85batch/s, mse_test=nan, mse_train=0.0147]\n",
            "Epoch 212: 100%|██████████| 11/11 [00:00<00:00, 82.17batch/s, mse_test=nan, mse_train=0.0147]\n",
            "Epoch 213: 100%|██████████| 11/11 [00:00<00:00, 81.66batch/s, mse_test=nan, mse_train=0.0147]\n",
            "Epoch 214: 100%|██████████| 11/11 [00:00<00:00, 77.61batch/s, mse_test=nan, mse_train=0.0146]\n",
            "Epoch 215: 100%|██████████| 11/11 [00:00<00:00, 77.01batch/s, mse_test=nan, mse_train=0.0146]\n",
            "Epoch 216: 100%|██████████| 11/11 [00:00<00:00, 71.23batch/s, mse_test=nan, mse_train=0.0146]\n",
            "Epoch 217: 100%|██████████| 11/11 [00:00<00:00, 60.58batch/s, mse_test=nan, mse_train=0.0146]\n",
            "Epoch 218: 100%|██████████| 11/11 [00:00<00:00, 70.06batch/s, mse_test=nan, mse_train=0.0145]\n",
            "Epoch 219: 100%|██████████| 11/11 [00:00<00:00, 64.37batch/s, mse_test=nan, mse_train=0.0145]\n",
            "Epoch 220: 100%|██████████| 11/11 [00:00<00:00, 68.84batch/s, mse_test=nan, mse_train=0.0145]\n",
            "Epoch 221: 100%|██████████| 11/11 [00:00<00:00, 70.76batch/s, mse_test=nan, mse_train=0.0144]\n",
            "Epoch 222: 100%|██████████| 11/11 [00:00<00:00, 64.47batch/s, mse_test=nan, mse_train=0.0144]\n",
            "Epoch 223: 100%|██████████| 11/11 [00:00<00:00, 62.68batch/s, mse_test=nan, mse_train=0.0144]\n",
            "Epoch 224: 100%|██████████| 11/11 [00:00<00:00, 74.00batch/s, mse_test=nan, mse_train=0.0143]\n",
            "Epoch 225: 100%|██████████| 11/11 [00:00<00:00, 76.13batch/s, mse_test=nan, mse_train=0.0143]\n",
            "Epoch 226: 100%|██████████| 11/11 [00:00<00:00, 67.45batch/s, mse_test=nan, mse_train=0.0143]\n",
            "Epoch 227: 100%|██████████| 11/11 [00:00<00:00, 67.79batch/s, mse_test=nan, mse_train=0.0142]\n",
            "Epoch 228: 100%|██████████| 11/11 [00:00<00:00, 69.70batch/s, mse_test=nan, mse_train=0.0142]\n",
            "Epoch 229: 100%|██████████| 11/11 [00:00<00:00, 66.08batch/s, mse_test=nan, mse_train=0.0142]\n",
            "Epoch 230: 100%|██████████| 11/11 [00:00<00:00, 91.53batch/s, mse_test=nan, mse_train=0.0141]\n",
            "Epoch 231: 100%|██████████| 11/11 [00:00<00:00, 81.41batch/s, mse_test=nan, mse_train=0.0141]\n",
            "Epoch 232: 100%|██████████| 11/11 [00:00<00:00, 66.80batch/s, mse_test=nan, mse_train=0.0141]\n",
            "Epoch 233: 100%|██████████| 11/11 [00:00<00:00, 70.05batch/s, mse_test=nan, mse_train=0.014]\n",
            "Epoch 234: 100%|██████████| 11/11 [00:00<00:00, 50.34batch/s, mse_test=nan, mse_train=0.014]\n",
            "Epoch 235: 100%|██████████| 11/11 [00:00<00:00, 58.04batch/s, mse_test=nan, mse_train=0.014]\n",
            "Epoch 236: 100%|██████████| 11/11 [00:00<00:00, 66.23batch/s, mse_test=nan, mse_train=0.0139]\n",
            "Epoch 237: 100%|██████████| 11/11 [00:00<00:00, 73.23batch/s, mse_test=nan, mse_train=0.0139]\n",
            "Epoch 238: 100%|██████████| 11/11 [00:00<00:00, 68.86batch/s, mse_test=nan, mse_train=0.0138]\n",
            "Epoch 239: 100%|██████████| 11/11 [00:00<00:00, 84.24batch/s, mse_test=nan, mse_train=0.0138]\n",
            "Epoch 240: 100%|██████████| 11/11 [00:00<00:00, 69.17batch/s, mse_test=nan, mse_train=0.0138]\n",
            "Epoch 241: 100%|██████████| 11/11 [00:00<00:00, 76.67batch/s, mse_test=nan, mse_train=0.0137]\n",
            "Epoch 242: 100%|██████████| 11/11 [00:00<00:00, 73.79batch/s, mse_test=nan, mse_train=0.0137]\n",
            "Epoch 243: 100%|██████████| 11/11 [00:00<00:00, 74.92batch/s, mse_test=nan, mse_train=0.0136]\n",
            "Epoch 244: 100%|██████████| 11/11 [00:00<00:00, 71.69batch/s, mse_test=nan, mse_train=0.0136]\n",
            "Epoch 245: 100%|██████████| 11/11 [00:00<00:00, 61.33batch/s, mse_test=nan, mse_train=0.0135]\n",
            "Epoch 246: 100%|██████████| 11/11 [00:00<00:00, 58.24batch/s, mse_test=nan, mse_train=0.0135]\n",
            "Epoch 247: 100%|██████████| 11/11 [00:00<00:00, 54.37batch/s, mse_test=nan, mse_train=0.0134]\n",
            "Epoch 248: 100%|██████████| 11/11 [00:00<00:00, 85.50batch/s, mse_test=nan, mse_train=0.0134]\n",
            "Epoch 249: 100%|██████████| 11/11 [00:00<00:00, 84.58batch/s, mse_test=nan, mse_train=0.0133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy 1 for educ: immigrant_citizenship with score: 0.03605068273702876\n",
            "Proxy 2 for educ: immigrant_deport with score: 0.006120182030859775\n",
            "Proxy 3 for educ: V201507x with score: 0.00484217718253338\n",
            "Proxy 4 for educ: V201631s with score: 0.0024622882924979048\n",
            "Proxy 5 for educ: V201630s with score: 0.002435874109782282\n",
            "Proxy 6 for educ: V201571 with score: 0.0021872641664657795\n",
            "Proxy 7 for educ: V201537b with score: 0.002105833771636178\n",
            "Proxy 8 for educ: V201251 with score: 0.001910780385066018\n",
            "Proxy 9 for educ: V201631i with score: 0.0018186518970115084\n",
            "Proxy 10 for educ: V201587 with score: 0.001620512489430625\n",
            "Proxy 11 for educ: V201631d with score: 0.0009533535724146668\n",
            "Proxy 12 for educ: V201630n with score: 0.0008477247129152763\n",
            "Proxy 13 for educ: V201629a with score: 0.0008377868822021628\n",
            "Proxy 14 for educ: V201146 with score: 0.0008278180898785457\n",
            "Proxy 15 for educ: V201631j with score: 0.0006985138919454791\n",
            "Proxy 16 for educ: V201220 with score: 0.0006238468228488058\n",
            "Proxy 17 for educ: V201223 with score: 0.000571366338528606\n",
            "Proxy 18 for educ: V201016 with score: 0.0005597655082313631\n",
            "Proxy 19 for educ: V201631h with score: 0.0005475079012519479\n",
            "Proxy 20 for educ: V201630p with score: 0.0004910185757611484\n",
            "Proxy 21 for educ: V201022 with score: 0.0004743290998124427\n",
            "Proxy 22 for educ: V201567 with score: 0.000430846693273068\n",
            "Proxy 23 for educ: V201232 with score: 0.0004059214941095398\n",
            "Proxy 24 for educ: V201014a with score: 0.00040487563684049035\n",
            "Proxy 25 for educ: V201625 with score: 0.0003336647646138279\n",
            "Proxy 26 for educ: V201045 with score: 0.0003303676648708842\n",
            "Proxy 27 for educ: V201203 with score: 0.00029675585427728617\n",
            "Proxy 28 for educ: V201586 with score: 0.00022810874637670944\n",
            "Proxy 29 for educ: V201014d with score: 0.00021529922744129568\n",
            "Proxy 30 for educ: V201634a with score: 0.00021184539164583176\n",
            "Proxy 31 for educ: V201537d with score: 0.00017627842638243672\n",
            "Proxy 32 for educ: V201064 with score: 0.00017288336384403412\n",
            "Proxy 33 for educ: V201149 with score: 0.00015687461353147084\n",
            "Proxy 34 for educ: V201524 with score: 0.0001496919307526711\n",
            "Proxy 35 for educ: V201509 with score: 0.0001459891181834149\n",
            "Proxy 36 for educ: V201125 with score: 0.0001239677043868903\n",
            "Proxy 37 for educ: V201399 with score: 0.00011624470496884856\n",
            "Proxy 38 for educ: V201631e with score: 0.00011246804961072548\n",
            "Proxy 39 for educ: index with score: 8.18758662146146e-05\n",
            "Proxy 40 for educ: V201056y with score: 7.615098535722064e-05\n",
            "Proxy 41 for educ: V201632s with score: 6.824826968231386e-05\n",
            "Proxy 42 for educ: V201658m with score: 3.631853145675823e-05\n",
            "Proxy 43 for educ: V201518 with score: 3.4252031801562655e-05\n",
            "Proxy 44 for educ: V201632e with score: 1.822036191152508e-05\n",
            "Proxy 45 for educ: V201658h with score: 1.5800073386035263e-05\n",
            "Proxy 46 for educ: V201551 with score: 9.041715833962842e-06\n",
            "Proxy 47 for educ: V201658a with score: 7.819209249093626e-06\n",
            "Proxy 48 for educ: V201632g with score: 3.09569441882539e-06\n",
            "Proxy 49 for educ: V201632q with score: 2.533812993088521e-06\n",
            "Proxy 50 for educ: V201050y with score: 5.477914870100096e-07\n",
            "PROXY SCORE !!!!! ('proxy_0.95', -0.015698688082295097)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_stata(\"/content/yougov_recode_03052025.dta\")\n",
        "df2 = pd.read_stata(\"/content/anes_recode_03052025.dta\")\n",
        "target = 'educ'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                  # 'educ',\n",
        "                   'ideo7',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'abortion',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport'\n",
        "                   ]"
      ],
      "metadata": {
        "id": "aGobAX0eQBaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1, df2 = data_rescale(df1, df2, predictors, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgD46X7YWNEd",
        "outputId": "297a6d5d-5449-4825-a6b1-a7b5a8894e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAPE AFTER DROPPING ALL NA (3400, 22)\n",
            "SHAPE AFTER DROPPING TARGET (2442, 22)\n",
            "SHAPE AFTER DROPPING PREDS (1312, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_col = df2['educ']\n",
        "print(target_col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gUbhMlMZxch",
        "outputId": "4b791383-6359-46e4-ccf0-52e449b90918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       0.714286\n",
            "1       0.285714\n",
            "2       0.142857\n",
            "3       0.428571\n",
            "4       1.000000\n",
            "          ...   \n",
            "8275    0.428571\n",
            "8276    0.714286\n",
            "8277    0.142857\n",
            "8278    0.571429\n",
            "8279    0.857143\n",
            "Name: educ, Length: 8149, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = prepare_dataset(df2, 'educ', 0.95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_RtYqbpQGFV",
        "outputId": "22f86a01-96d7-405f-d0a3-4b186a0f0b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOW MANY NAN 0\n",
            "TARGET LEN 8149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2['proxy_0.95'])\n",
        "highly = df2['proxy_0.95']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic8gz93bUlnO",
        "outputId": "c5bb2cc4-2ae9-4e6c-f02e-bb19694e5d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       0.765113\n",
            "1      -0.362602\n",
            "2      -1.311245\n",
            "3      -0.429265\n",
            "4       1.237096\n",
            "          ...   \n",
            "8275   -0.075746\n",
            "8276    0.966438\n",
            "8277   -1.083665\n",
            "8278    0.366594\n",
            "8279    1.649712\n",
            "Name: proxy_0.95, Length: 8149, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_col.corr(highly))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpflYbtPaFf_",
        "outputId": "77ff5212-301b-4494-9ada-fe6c7435c6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9457903814376333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = get_predictions(df1, df2, predictors, target)\n",
        "\n",
        "print(predictions)\n",
        "print(len(predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EczlLlcKbr46",
        "outputId": "b18dfd69-aaa5-4ccd-deab-99001b1797d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "[0.14606088 0.4272429  0.22965828 ... 0.32547882 0.23860644 0.3007361 ]\n",
            "8149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1['educ'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg-3yHpugTOv",
        "outputId": "df46fb32-1282-4ffd-a513-87e68cb2d98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2       0.4\n",
            "5       1.0\n",
            "6       1.0\n",
            "7       0.8\n",
            "11      0.4\n",
            "       ... \n",
            "3265    0.2\n",
            "3266    0.8\n",
            "3268    0.8\n",
            "3270    0.6\n",
            "3274    0.2\n",
            "Name: educ, Length: 1312, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2['proxy_0.95'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Joj2OoNNgah1",
        "outputId": "66c6d217-f702-410c-c4fb-76654a571e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       0.765113\n",
            "1      -0.362602\n",
            "2      -1.311245\n",
            "3      -0.429265\n",
            "4       1.237096\n",
            "          ...   \n",
            "8275   -0.075746\n",
            "8276    0.966438\n",
            "8277   -1.083665\n",
            "8278    0.366594\n",
            "8279    1.649712\n",
            "Name: proxy_0.95, Length: 8149, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictionsdf = pd.Series(predictions)\n",
        "print(predictionsdf.corr(highly))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw6CC-HKb7ei",
        "outputId": "317135aa-a38e-445e-9c26-458d93dc88e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008405879763471328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proxy = df2['proxy_0.95']\n",
        "print(target_col.corr(proxy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJxSMO1c3Zn",
        "outputId": "be2bee38-0573-4b77-c1a3-5bb758613a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9457903814376333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(proxy.corr(predictionsdf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV6RfV51dFEg",
        "outputId": "4a900d3b-4e3a-4c53-d01a-f04df14b28bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008405879763471326\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
