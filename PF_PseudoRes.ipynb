{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92922b8c",
      "metadata": {
        "id": "92922b8c"
      },
      "outputs": [],
      "source": [
        "### orth method gets predicted residuals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224f669f",
      "metadata": {
        "id": "224f669f"
      },
      "outputs": [],
      "source": [
        "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
        "\n",
        "    # validate proxies and st item\n",
        "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
        "\n",
        "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
        "\n",
        "    for c in predictors:\n",
        "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
        "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset\n",
        "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'\n",
        "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'\n",
        "\n",
        "    for c in candidates:\n",
        "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
        "\n",
        "    if (orthogonal_vars != None):\n",
        "        for c in orthogonal_vars:\n",
        "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a41179e",
      "metadata": {
        "id": "5a41179e"
      },
      "outputs": [],
      "source": [
        "# return a new df that is a copy of df, with: rescale all columns to be\n",
        "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
        "# rows that are missing at least one predictor.\n",
        "def data_rescale(df, predictors):\n",
        "    df = df.copy() # preserve immutability\n",
        "\n",
        "    # Select only the numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # drop any rows that are missing at least one predictor\n",
        "    df = df.dropna(subset=predictors)\n",
        "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
        "    # Initialize the scaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
        "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
        "\n",
        "    return scaled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222f7fdf",
      "metadata": {
        "id": "222f7fdf"
      },
      "outputs": [],
      "source": [
        "### GET PREDICTIONS\n",
        "\n",
        "# Neural network definition\n",
        "def build_nn_model(input_dim, learning_rate=0.001, l2_lambda=0.001):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, kernel_regularizer=l2(l2_lambda))\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# return a trained neural network to predict df[item] using df[predictors_df1]\n",
        "# report error and crash if predictors don't predict item\n",
        "def train_nn_model(X_train, y_train, input_dim, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
        "    model = build_nn_model(input_dim, learning_rate, l2_lambda)\n",
        "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
        "    return model\n",
        "\n",
        "# get predictions from the neural network. Takes in\n",
        "def get_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
        "\n",
        "    # split data for training and testing.\n",
        "    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
        "    X_test = df_test[predictors].to_numpy()\n",
        "\n",
        "    # train network and get predictions\n",
        "    model = train_nn_model(X_train_train, y_train_train, len(predictors), epochs, learning_rate, l2_lambda)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # exit if correlation between predictions and item is bad\n",
        "    mse = mean_squared_error(model.predict(X_train_test), y_train_test)\n",
        "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
        "    if (mse > 0.03):\n",
        "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
        "        print('Aborting program')\n",
        "        sys.exit(-1)\n",
        "\n",
        "   # print(f\"Predictions before flattening: {predictions[:10]}\") #DEBUG\n",
        "   # print('predictions after flattening: ', predictions.flatten()[:10])#DEBUG\n",
        "\n",
        "    return predictions.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mMwPZK4hIe03",
      "metadata": {
        "id": "mMwPZK4hIe03"
      },
      "outputs": [],
      "source": [
        "   ''' Train a linear regression model on df_train using orth_vars to predict target.\n",
        "    Then, using the learned model, predict on df_test and return the predictions\n",
        "    as a pandas Series aligned with df_test.index.\n",
        "    '''\n",
        "def get_linear_predictions(orth_vars, target, df_train, df_test):\n",
        "#### fit model\n",
        "    X_train = df_train[orth_vars]\n",
        "    y_train = df_train[target]\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "#### infer on test set\n",
        "    X_test = df_test[orth_vars]\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Return the predictions as a pandas Series, aligned to df_test.index\n",
        "    return pd.Series(predictions, index=df_test.index, name=\"predicted_target_linear\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9fbf828",
      "metadata": {
        "id": "b9fbf828"
      },
      "outputs": [],
      "source": [
        "# orthogonalization method\n",
        "# all data is preprocessed and df test has been appended target preds\n",
        "def orthogonalize(candidates, df_test, orth_weight):\n",
        "        results = []\n",
        "        df_test['predicted_target_residuals'] = (1 - orth_weight) * df_test['predicted_target'] - (orth_weight * df_test['predicted_target_linear'])\n",
        "        for c in candidates:\n",
        "            candset = df_test[[c, 'predicted_target_residuals']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
        "            candcol = candset[c]\n",
        "\n",
        "            X = sm.add_constant(candcol)\n",
        "            model = sm.OLS(df_test['predicted_target_residuals'], X).fit() ### ols model regressing c on predicted target residuals\n",
        "            results.append(model.rsquared)\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbc7703",
      "metadata": {
        "id": "1bbc7703"
      },
      "outputs": [],
      "source": [
        "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.62, candidates=None, orthogonal_vars=None):\n",
        "    if candidates is None:\n",
        "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
        "\n",
        "\n",
        "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
        "\n",
        "    # Predict status threat scores in df_test\n",
        "    df_train = data_rescale(df_train, predictors) # rescale and drop any columsn missing predictors\n",
        "    df_test = data_rescale(df_test, predictors)\n",
        "    df_train = df_train.dropna(subset=target)\n",
        "\n",
        "    predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
        "\n",
        "    df_test['predicted_target'] = predicted_scores\n",
        "\n",
        "    best_proxies = []\n",
        "\n",
        "    df_test['predicted_target_linear'] = get_linear_predictions(orthogonal_vars, target, df_train, df_test)\n",
        "    results = orthogonalize(candidates, df_test, orth_weight) ### results[c]: Adj R^2 of c~t_res\n",
        "\n",
        "    proxy_scores = {}\n",
        "    i = 0\n",
        "    for c in candidates:\n",
        "        try:\n",
        "            proxy_scores[c] = (c, results[i])\n",
        "            i += 1\n",
        "        except KeyError as e:\n",
        "            continue\n",
        "\n",
        "    sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for i in range(min(num_proxies, len(sorted_results))):\n",
        "        proxy, score = sorted_results[i]\n",
        "        best_proxies.append(proxy)\n",
        "        print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
        "\n",
        "    return best_proxies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b487cea5",
      "metadata": {
        "id": "b487cea5",
        "outputId": "ab0ff4cc-cf4b-466e-93a1-428c2c3b23d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\")\n",
        "\n",
        "\n",
        "# Suppress numpy invalid operation warnings\n",
        "np.seterr(invalid='ignore')\n",
        "\n",
        "#datafile_train =  /path/to/dftrain\n",
        "#datafile_test =  /path/to/dftest\n",
        "#df_train = pd.read_stata(datafile_train)\n",
        "#df_test = pd.read_stata(datafile_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6372b2d",
      "metadata": {
        "id": "f6372b2d"
      },
      "outputs": [],
      "source": [
        "### example usage\n",
        "target = 'christian_nationalism'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'white',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                  'faminc_new'\n",
        "                   ]\n",
        "\n",
        "orthogonal_vars = [\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                    'faminc_new'\n",
        "                 ]\n",
        "\n",
        "best_proxies = proxy_finder(df_train, df_test, target, predictors, orthogonal_vars=orthogonal_vars, num_proxies=20)\n",
        "print(best_proxies)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
