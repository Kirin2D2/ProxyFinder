{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uWUQYLoYUZb"
      },
      "source": [
        "# Monte Carlo Testing for Proxy Finder Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sys\n",
        "from gain import GAIN\n",
        "from usage_example import *\n",
        "import utils\n",
        "import models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nC-Hn0_GjtJ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
        "    # validate proxies and st item\n",
        "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
        "\n",
        "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
        "\n",
        "    for c in predictors:\n",
        "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
        "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset\n",
        "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'\n",
        "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'\n",
        "\n",
        "    for c in candidates:\n",
        "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
        "\n",
        "    if (orthogonal_vars != None):\n",
        "        for c in orthogonal_vars:\n",
        "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'"
      ],
      "metadata": {
        "id": "nd3YKbPqka9T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(df_train, df_test, predictors, target, epochs=10, learning_rate=0.001, l2_lambda=0.001):\n",
        "  # CODE IMPLEMENTATION ASSISTED BY GENERATIVE AI\n",
        "\n",
        "  # Set parameters\n",
        "\n",
        "  DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  TRAIN_SIZE = 1.0  # Using all of df1 for training\n",
        "\n",
        "\n",
        "  df1 = df_train.copy()\n",
        "  df2 = df_test.copy()\n",
        "\n",
        "  # drop everything but predictors and target from df1\n",
        "  target_col_df1 = df1[target]\n",
        "  df1 = df1[predictors]\n",
        "  df1[target] = target_col_df1\n",
        "\n",
        "  # drop everything but predictors from df2\n",
        "  df2 = df2[predictors]\n",
        "  # add missing target\n",
        "  df2[target] = np.nan\n",
        "\n",
        "  combined_df = pd.concat([df1, df2])\n",
        "\n",
        "  # Step 3: Normalize the data\n",
        "  scaler = MinMaxScaler()\n",
        "  combined_data_std = scaler.fit_transform(combined_df)\n",
        "\n",
        "  # Split back into df1 (training) and df2 (prediction)\n",
        "  df1_std = combined_data_std[:len(df1)]\n",
        "  df2_std = combined_data_std[len(df1):]\n",
        "\n",
        "  # Create tensors and masks\n",
        "  X_train_tensor = torch.tensor(df1_std).float()\n",
        "  M_train_tensor = get_mask(X_train_tensor)  # This creates mask with 0s for observed values, 1s for missing values\n",
        "  train_dataset = TensorDataset(X_train_tensor, M_train_tensor)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "  X_test_tensor = torch.tensor(df2_std).float()\n",
        "  M_test_tensor = get_mask(X_test_tensor)  # This will mark all values in the target column as missing\n",
        "  test_dataset = TensorDataset(X_test_tensor, M_test_tensor)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "  # Step 4: Initialize and train the GAIN model\n",
        "  stopper = EarlyStopper(patience=2, min_delta=0.001)\n",
        "  model = GAIN(train_loader=train_loader)\n",
        "\n",
        "  optimizer_G = torch.optim.Adam(model.G.parameters())\n",
        "  optimizer_D = torch.optim.Adam(model.D.parameters())\n",
        "  model.set_optimizer(optimizer=optimizer_G, generator=True)\n",
        "  model.set_optimizer(optimizer=optimizer_D, generator=False)\n",
        "\n",
        "  model.to(DEVICE)\n",
        "  model.train(n_epoches=epochs, verbose=True, stopper=stopper)\n",
        "\n",
        "  # Step 5: Use the trained model to predict (impute) target values for df2\n",
        "  predictions = []\n",
        "\n",
        "  for x_test_batch, m_batch in test_loader:\n",
        "      x_batch_imputed = model.imputation(x=x_test_batch, m=m_batch)\n",
        "      x_batch_imputed = x_batch_imputed.cpu().numpy()\n",
        "      predictions.append(x_batch_imputed)\n",
        "\n",
        "  # Combine predictions and inverse transform\n",
        "  predictions_combined = np.vstack(predictions)\n",
        "  predictions_original_scale = scaler.inverse_transform(predictions_combined)\n",
        "\n",
        "  # Extract the target column predictions\n",
        "  target_column_index = df1.columns.get_loc(target)\n",
        "  df2_predictions = predictions_original_scale[:, target_column_index]\n",
        "\n",
        "  return df2_predictions"
      ],
      "metadata": {
        "id": "EY7VQ3v_5DZ7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions from the Torch neural network\n",
        "def get_predictionsTorch(df_train, df_test, predictors, target, epochs=10, learning_rate=0.001, l2_lambda=0.001):\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    # split data for training and testing.\n",
        "    training_features, validation_features, training_target, validation_target = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
        "\n",
        "    training_features = torch.FloatTensor(training_features)\n",
        "    training_target = torch.FloatTensor(training_target)\n",
        "    validation_features = torch.FloatTensor(validation_features)\n",
        "    validation_target = torch.FloatTensor(validation_target)\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(len(predictors), 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "    # MSE loss\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # train the model\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model(training_features)\n",
        "        loss = loss_func(prediction, training_target.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # get predictions\n",
        "    model.eval()\n",
        "    test_data = torch.FloatTensor(df_test[predictors].to_numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_data)\n",
        "        predictions = predictions.numpy().flatten()\n",
        "\n",
        "        val_predictions = model(validation_features)\n",
        "        val_predictions = val_predictions.numpy().flatten()\n",
        "\n",
        "\n",
        "    # exit if correlation between predictions and item is bad\n",
        "    mse = mean_squared_error(val_predictions, validation_target)\n",
        "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
        "    if (mse > 0.03):\n",
        "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
        "        print('Aborting program')\n",
        "        sys.exit(-1)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "_gMeCv-KNks1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # orthogonalization method\n",
        "# all data is preprocessed and df test has been appended target preds\n",
        "def orthogonalize(candidates, df_test, orthogonal_vars):\n",
        "        orth_scores = {}\n",
        "        for c in candidates:\n",
        "            candset = df_test[[c, 'predicted_target']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
        "            candcol = candset[c]\n",
        "\n",
        "            X = sm.add_constant(candcol)\n",
        "            temp_orth_scores = []\n",
        "            for orth_var in orthogonal_vars:\n",
        "                orthset = df_test[[orth_var]].copy().dropna()\n",
        "                common_indices = candset.index.intersection(orthset.index)\n",
        "                if common_indices.empty:\n",
        "                    continue\n",
        "                orth_col = orthset.loc[common_indices, orth_var]\n",
        "                if np.var(orth_col) == 0:\n",
        "                    print(\"ortho:\", orth_var, \"candidate\", c)\n",
        "                    continue # zero variance leads to divide by zero error\n",
        "                candcol_common = candset.loc[common_indices, c]\n",
        "\n",
        "                X_common = sm.add_constant(candcol_common)\n",
        "                model = sm.OLS(orth_col, X_common).fit()\n",
        "                temp_orth_scores.append(model.rsquared)\n",
        "\n",
        "            if temp_orth_scores:\n",
        "                orth_scores[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
        "            else:\n",
        "                orth_scores[c] = 0\n",
        "        return orth_scores"
      ],
      "metadata": {
        "id": "Qc5l-fCSkfWc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, neural_net=\"original\", drop=True):\n",
        "    if candidates is None:\n",
        "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
        "\n",
        "\n",
        "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
        "\n",
        "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
        "    #print(f\"Candidates: {candidates}\")\n",
        "\n",
        "    df_train = data_rescale(df_train, predictors, target, drop)\n",
        "    df_test = data_rescale(df_test, predictors, target, drop)\n",
        "    # drop any rows that are missing data from target\n",
        "    df_train = df_train.dropna(subset=target)\n",
        "\n",
        "    if neural_net == \"torch\":\n",
        "      predicted_scores = get_predictionsTorch(df_train, df_test, predictors, target)\n",
        "    else:\n",
        "      predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
        "\n",
        "\n",
        "    df_test['predicted_target'] = predicted_scores\n",
        "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c in candidates:\n",
        "        candset = df_test[[c, 'predicted_target']].copy().dropna()\n",
        "        if candset.empty:\n",
        "            continue\n",
        "\n",
        "        pred_scores = candset['predicted_target']\n",
        "        candcol = candset[c]\n",
        "\n",
        "        X_pred = sm.add_constant(candcol)\n",
        "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
        "        results[c] = {\n",
        "            'R_squared': model_pred.rsquared,\n",
        "            'p_value': model_pred.pvalues.iloc[1],\n",
        "            'coef': model_pred.params.iloc[1]\n",
        "        }\n",
        "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------\n",
        "\n",
        "    best_proxies = []\n",
        "\n",
        "    if orthogonal_vars:\n",
        "        orth_scores = orthogonalize(candidates, df_test, orthogonal_vars)\n",
        "        proxy_scores = {}\n",
        "        for c in candidates:\n",
        "            try:\n",
        "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_scores[c])\n",
        "            except KeyError as e:\n",
        "                continue\n",
        "\n",
        "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, score = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
        "    else:\n",
        "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, metrics = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
        "\n",
        "    return best_proxies"
      ],
      "metadata": {
        "id": "a-r5DNEokhf0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return a new df that is a copy of df, with: rescale all columns to be\n",
        "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
        "# rows that are missing at least one predictor.\n",
        "def data_rescale(df, predictors, target, drop=True):\n",
        "    df = df.copy() # preserve immutability\n",
        "\n",
        "    # Select only the numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    if drop:\n",
        "      # drop any rows that are missing at least one predictor\n",
        "      df = df.dropna(subset=predictors)\n",
        "\n",
        "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
        "    # Initialize the scaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
        "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
        "\n",
        "    return scaled_df"
      ],
      "metadata": {
        "id": "ynJNdpekPqTm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_proxies(target_column, target_correlation, noise_level=0.1):\n",
        "\n",
        "   # Convert target_column to numpy array and standardize\n",
        "    target = np.array(target_column)\n",
        "    length = len(target)\n",
        "\n",
        "    target = (target - np.mean(target)) / np.std(target)\n",
        "\n",
        "    synthetic_proxies = {}\n",
        "\n",
        "    # Generate independent standard normal variable\n",
        "    z = np.random.standard_normal(length)\n",
        "\n",
        "    # Create correlated variable using the correlation formula\n",
        "    proxy = target_correlation * target + np.sqrt(1 - target_correlation**2) * z\n",
        "\n",
        "    # Add controlled noise\n",
        "    proxy = proxy + np.random.normal(0, noise_level, length)\n",
        "\n",
        "    # Standardize final proxy\n",
        "    proxy = (proxy - np.mean(proxy)) / np.std(proxy)\n",
        "\n",
        "    synthetic_proxies[f'proxy_{target_correlation:.2f}'] = proxy\n",
        "\n",
        "    return synthetic_proxies"
      ],
      "metadata": {
        "id": "WabG6t15Pr9Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df, target, target_correlation):\n",
        "\n",
        "  # add synthetic proxies to test set\n",
        "  target_column = df[target]\n",
        "  synthetic_proxies = generate_synthetic_proxies(target_column, target_correlation)\n",
        "  for name, proxy in synthetic_proxies.items():\n",
        "    df[name] = proxy\n",
        "\n",
        "  # drop target from test set\n",
        "  df = df.drop(columns=[target])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "jLnpMEMFKdo2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnUMZlYeqfW"
      },
      "source": [
        "# Stage 1: Testing Mean Penalty Approach with Several Target Correlations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_and_visualize_monte_carlo(df1, df2, weights, num_iterations, target, target_correlations, predictors, neural_net):\n",
        "    selection_trackers = []\n",
        "    proxy_names = []\n",
        "\n",
        "\n",
        "    # Run Monte Carlo for each target correlation\n",
        "    for target_correlation in target_correlations:\n",
        "\n",
        "        df2 = df2.dropna(subset=target)\n",
        "\n",
        "        df2 = prepare_dataset(df2, target, target_correlation)\n",
        "\n",
        "        selection_tracker = {orth_weight: {} for orth_weight in weights}\n",
        "\n",
        "        # Run iterations for each weight\n",
        "        for orth_weight in weights:\n",
        "            print(f\"Testing with orthogonality weight: {orth_weight}\")\n",
        "            print(f\"Testing with target correlation: {target_correlation}\")\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                print(f\"Running iteration {i+1}/{num_iterations}\")\n",
        "                top_proxies = proxy_finder(df_train=df1,\n",
        "                                         df_test=df2,\n",
        "                                         target=target,\n",
        "                                         predictors=predictors,\n",
        "                                         num_proxies=50,\n",
        "                                         orth_weight=orth_weight,\n",
        "                                         orthogonal_vars=predictors,\n",
        "                                         neural_net=neural_net\n",
        "                                         )\n",
        "\n",
        "                # Update selection tracker for top pick\n",
        "                for rank, proxy in enumerate(top_proxies, 1):\n",
        "                    if rank == 1:\n",
        "                        selection_tracker[orth_weight][proxy] = selection_tracker[orth_weight].get(proxy, 0) + 1\n",
        "\n",
        "        selection_trackers.append(selection_tracker)\n",
        "        proxy_names.append(f'proxy_{target_correlation:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "    # SAVE TO CSV --------------------\n",
        "    data = []\n",
        "\n",
        "    for target_correlation, selection_tracker in zip(proxy_names, selection_trackers):\n",
        "        for orth_weight, proxies in selection_tracker.items():\n",
        "            for proxy, count in proxies.items():\n",
        "                data.append({\n",
        "                    'Target Correlation': target_correlation,\n",
        "                    'Orthogonality Weight': orth_weight,\n",
        "                    'Proxy': proxy,\n",
        "                    'Count': count\n",
        "                })\n",
        "\n",
        "    df_selection_tracker = pd.DataFrame(data)\n",
        "    df_selection_tracker.to_csv('selection_tracker.csv', index=False)\n",
        "    # SAVE TO CSV --------------------\n",
        "\n",
        "\n",
        "    # # Visualization\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # # Plot results for each target correlation\n",
        "    # for index, tracker in enumerate(selection_trackers):\n",
        "    #     results = []\n",
        "    #     for orth_weight, proxies in tracker.items():\n",
        "    #         for proxy, frequency in proxies.items():\n",
        "    #             results.append({\n",
        "    #                 'orth_weight': orth_weight,\n",
        "    #                 'proxy': proxy,\n",
        "    #                 'frequency': (frequency / num_iterations) * 100\n",
        "    #             })\n",
        "\n",
        "    #     results_df = pd.DataFrame(results)\n",
        "    #     pivot_data = results_df.pivot(index='orth_weight', columns='proxy', values='frequency')\n",
        "    #     pivot_data.fillna(0, inplace=True)\n",
        "    #     print(pivot_data)\n",
        "\n",
        "    #     # Plot each proxy as a separate line\n",
        "    #     name = proxy_names[index]\n",
        "    #     plt.plot(pivot_data.index, pivot_data[name], marker='o', label=name, linewidth=2)\n",
        "\n",
        "    # # Create the line plot\n",
        "    # plt.xlabel('Orthogonality Weight')\n",
        "    # plt.ylabel('Selection Frequency')\n",
        "    # plt.title('Selection Frequency vs Orthogonality Weight')\n",
        "    # plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "fgTQXgXRSS33"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change parameters as needed\n",
        "df1 = pd.read_stata(\"/content/temp_yougov.dta\")\n",
        "df2 = pd.read_stata(\"/content/temp_yougov.dta\")\n",
        "weights = [0.65]\n",
        "target_correlations = [0.90]\n",
        "num_iterations = 2\n",
        "target = 'christian_nationalism'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'white',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new'\n",
        "                   ]\n",
        "run_and_visualize_monte_carlo(df1, df2, weights, num_iterations, target, target_correlations, predictors, neural_net=\"GAIN\")"
      ],
      "metadata": {
        "id": "NdGZvvMUSzCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41aa1ec9-c390-428b-f675-2110a9fc9d0d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with orthogonality weight: 0.65\n",
            "Testing with target correlation: 0.9\n",
            "Running iteration 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 27/27 [00:00<00:00, 60.63batch/s, mse_test=nan, mse_train=0.121]\n",
            "Epoch 1: 100%|██████████| 27/27 [00:00<00:00, 51.14batch/s, mse_test=nan, mse_train=0.112]\n",
            "Epoch 2: 100%|██████████| 27/27 [00:00<00:00, 59.39batch/s, mse_test=nan, mse_train=0.101]\n",
            "Epoch 3: 100%|██████████| 27/27 [00:00<00:00, 51.82batch/s, mse_test=nan, mse_train=0.0878]\n",
            "Epoch 4: 100%|██████████| 27/27 [00:00<00:00, 53.73batch/s, mse_test=nan, mse_train=0.0741]\n",
            "Epoch 5: 100%|██████████| 27/27 [00:00<00:00, 58.96batch/s, mse_test=nan, mse_train=0.0636]\n",
            "Epoch 6: 100%|██████████| 27/27 [00:00<00:00, 63.24batch/s, mse_test=nan, mse_train=0.0591]\n",
            "Epoch 7: 100%|██████████| 27/27 [00:00<00:00, 69.13batch/s, mse_test=nan, mse_train=0.0576]\n",
            "Epoch 8: 100%|██████████| 27/27 [00:00<00:00, 61.04batch/s, mse_test=nan, mse_train=0.0566]\n",
            "Epoch 9: 100%|██████████| 27/27 [00:00<00:00, 74.69batch/s, mse_test=nan, mse_train=0.0554]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy 1 for christian_nationalism: pff_dt with score: 0.06107999339938533\n",
            "Proxy 2 for christian_nationalism: presvote20post with score: 0.05569108631677\n",
            "Proxy 3 for christian_nationalism: election_fairnness with score: 0.05529401873697745\n",
            "Proxy 4 for christian_nationalism: senvote20post with score: 0.049086552121809596\n",
            "Proxy 5 for christian_nationalism: housevote20post with score: 0.04744241600133167\n",
            "Proxy 6 for christian_nationalism: proxy_0.90 with score: 0.043817715455087364\n",
            "Proxy 7 for christian_nationalism: ideo7 with score: 0.042431184578074516\n",
            "Proxy 8 for christian_nationalism: pid7 with score: 0.031702240061910414\n",
            "Proxy 9 for christian_nationalism: immigrant_deport with score: 0.027027539557470087\n",
            "Proxy 10 for christian_nationalism: pff_jb with score: 0.026034049617909116\n",
            "Proxy 11 for christian_nationalism: immigrant_citizenship with score: 0.021569112872402513\n",
            "Proxy 12 for christian_nationalism: index with score: -0.0012736918007953887\n",
            "Proxy 13 for christian_nationalism: turnout20post with score: -0.0037714675983391496\n",
            "Proxy 14 for christian_nationalism: auth_grid_3 with score: -0.00593778819214727\n",
            "Proxy 15 for christian_nationalism: auth_grid_1 with score: -0.008632030726390669\n",
            "Proxy 16 for christian_nationalism: auth_grid_2 with score: -0.01935735397070664\n",
            "Proxy 17 for christian_nationalism: educ with score: -0.027562792965135904\n",
            "Proxy 18 for christian_nationalism: faminc_new with score: -0.03777268284450309\n",
            "Proxy 19 for christian_nationalism: partisan_violence with score: -0.038530518448175385\n",
            "Proxy 20 for christian_nationalism: hispanic with score: -0.04722855581677236\n",
            "Proxy 21 for christian_nationalism: white with score: -0.049301653443034776\n",
            "Running iteration 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 27/27 [00:00<00:00, 77.11batch/s, mse_test=nan, mse_train=0.119]\n",
            "Epoch 1: 100%|██████████| 27/27 [00:00<00:00, 71.54batch/s, mse_test=nan, mse_train=0.106]\n",
            "Epoch 2: 100%|██████████| 27/27 [00:00<00:00, 64.06batch/s, mse_test=nan, mse_train=0.0967]\n",
            "Epoch 3: 100%|██████████| 27/27 [00:00<00:00, 63.64batch/s, mse_test=nan, mse_train=0.0877]\n",
            "Epoch 4: 100%|██████████| 27/27 [00:00<00:00, 75.35batch/s, mse_test=nan, mse_train=0.0756]\n",
            "Epoch 5: 100%|██████████| 27/27 [00:00<00:00, 79.51batch/s, mse_test=nan, mse_train=0.0652]\n",
            "Epoch 6: 100%|██████████| 27/27 [00:00<00:00, 65.52batch/s, mse_test=nan, mse_train=0.06]\n",
            "Epoch 7: 100%|██████████| 27/27 [00:00<00:00, 60.86batch/s, mse_test=nan, mse_train=0.0577]\n",
            "Epoch 8: 100%|██████████| 27/27 [00:00<00:00, 59.46batch/s, mse_test=nan, mse_train=0.0559]\n",
            "Epoch 9: 100%|██████████| 27/27 [00:00<00:00, 62.25batch/s, mse_test=nan, mse_train=0.054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy 1 for christian_nationalism: presvote20post with score: 0.058111487650129134\n",
            "Proxy 2 for christian_nationalism: senvote20post with score: 0.05445986111500986\n",
            "Proxy 3 for christian_nationalism: housevote20post with score: 0.05365647697882739\n",
            "Proxy 4 for christian_nationalism: pff_dt with score: 0.05189146363203498\n",
            "Proxy 5 for christian_nationalism: election_fairnness with score: 0.04545841453452154\n",
            "Proxy 6 for christian_nationalism: pid7 with score: 0.0441585366026537\n",
            "Proxy 7 for christian_nationalism: ideo7 with score: 0.04040126194528579\n",
            "Proxy 8 for christian_nationalism: immigrant_deport with score: 0.03269330575315127\n",
            "Proxy 9 for christian_nationalism: proxy_0.90 with score: 0.03047707659711829\n",
            "Proxy 10 for christian_nationalism: pff_jb with score: 0.030399028330995448\n",
            "Proxy 11 for christian_nationalism: immigrant_citizenship with score: 0.014971121150878619\n",
            "Proxy 12 for christian_nationalism: index with score: -0.0003260895597275038\n",
            "Proxy 13 for christian_nationalism: turnout20post with score: -0.004173196831627789\n",
            "Proxy 14 for christian_nationalism: auth_grid_1 with score: -0.01356674125434211\n",
            "Proxy 15 for christian_nationalism: auth_grid_3 with score: -0.016000327864700493\n",
            "Proxy 16 for christian_nationalism: auth_grid_2 with score: -0.02472166190920265\n",
            "Proxy 17 for christian_nationalism: white with score: -0.03015514509485884\n",
            "Proxy 18 for christian_nationalism: educ with score: -0.03303788236623749\n",
            "Proxy 19 for christian_nationalism: faminc_new with score: -0.03742619159731807\n",
            "Proxy 20 for christian_nationalism: partisan_violence with score: -0.03941104879995447\n",
            "Proxy 21 for christian_nationalism: hispanic with score: -0.043964813819674826\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}