{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNEfinAyzBPT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F6HdxmM7zOgf"
      },
      "outputs": [],
      "source": [
        "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
        "\n",
        "    # validate proxies and st item\n",
        "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
        "\n",
        "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
        "\n",
        "    for c in predictors:\n",
        "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
        "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset\n",
        "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'\n",
        "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'\n",
        "\n",
        "    for c in candidates:\n",
        "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
        "\n",
        "    if (orthogonal_vars != None):\n",
        "        for c in orthogonal_vars:\n",
        "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6t0KwAa2zVUi"
      },
      "outputs": [],
      "source": [
        "# return a new df that is a copy of df, with: rescale all columns to be\n",
        "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
        "# rows that are missing at least one predictor.\n",
        "def data_rescale(df, predictors):\n",
        "    df = df.copy() # preserve immutability\n",
        "\n",
        "    # Select only the numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # drop any rows that are missing at least one predictor\n",
        "    df = df.dropna(subset=predictors)\n",
        "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
        "    # Initialize the scaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
        "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
        "\n",
        "    return scaled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yFCX1ZD0zWOW"
      },
      "outputs": [],
      "source": [
        "### GET PREDICTIONS\n",
        "\n",
        "# Neural network definition\n",
        "def build_nn_model(input_dim, learning_rate=0.001, l2_lambda=0.001):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, kernel_regularizer=l2(l2_lambda))\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# return a trained neural network to predict df[item] using df[predictors_df1]\n",
        "# report error and crash if predictors don't predict item\n",
        "def train_nn_model(X_train, y_train, input_dim, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
        "    model = build_nn_model(input_dim, learning_rate, l2_lambda)\n",
        "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
        "    return model\n",
        "\n",
        "# get predictions from the neural network. Takes in\n",
        "def get_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
        "\n",
        "    # split data for training and testing.\n",
        "    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
        "    X_test = df_test[predictors].to_numpy()\n",
        "\n",
        "    # train network and get predictions\n",
        "    model = train_nn_model(X_train_train, y_train_train, len(predictors), epochs, learning_rate, l2_lambda)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # exit if correlation between predictions and item is bad\n",
        "    mse = mean_squared_error(model.predict(X_train_test), y_train_test)\n",
        "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
        "    if (mse > 0.03):\n",
        "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
        "        print('Aborting program')\n",
        "        sys.exit(-1)\n",
        "\n",
        "   # print(f\"Predictions before flattening: {predictions[:10]}\") #DEBUG\n",
        "   # print('predictions after flattening: ', predictions.flatten()[:10])#DEBUG\n",
        "\n",
        "    return predictions.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qn4a2sJNzaGZ"
      },
      "outputs": [],
      "source": [
        "# orthogonalization method\n",
        "# all data is preprocessed and df test has been appended target preds\n",
        "def orthogonalize(candidates, df_test, orthogonal_vars, tau):\n",
        "        orth_scores = {}\n",
        "        for c in candidates:\n",
        "            candset = df_test[[c, 'predicted_target']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
        "            candcol = candset[c]\n",
        "\n",
        "            X = sm.add_constant(candcol)\n",
        "            temp_orth_scores = []\n",
        "            for orth_var in orthogonal_vars:\n",
        "                orthset = df_test[[orth_var]].copy().dropna()\n",
        "                common_indices = candset.index.intersection(orthset.index)\n",
        "                if common_indices.empty:\n",
        "                    continue\n",
        "                orth_col = orthset.loc[common_indices, orth_var]\n",
        "                if np.var(orth_col) == 0:\n",
        "                    print(\"ortho:\", orth_var, \"candidate\", c)\n",
        "                    continue # zero variance leads to divide by zero error\n",
        "                candcol_common = candset.loc[common_indices, c]\n",
        "\n",
        "                X_common = sm.add_constant(candcol_common)\n",
        "                model = sm.OLS(orth_col, X_common).fit()\n",
        "                r_squared = model.rsquared\n",
        "                if np.abs(r_squared) > tau:\n",
        "                  temp_orth_scores.append(r_squared)\n",
        "\n",
        "            if temp_orth_scores:\n",
        "                orth_scores[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
        "            else:\n",
        "                orth_scores[c] = 0\n",
        "        return orth_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AeFG9xdEzcbl"
      },
      "outputs": [],
      "source": [
        "def proxy_finder_threshold_ortho(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, tau=0.15):\n",
        "    if candidates is None:\n",
        "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
        "\n",
        "\n",
        "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
        "\n",
        "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
        "    #print(f\"Candidates: {candidates}\")\n",
        "\n",
        "    # Predict status threat scores in df_test\n",
        "    df_train = data_rescale(df_train, predictors)\n",
        "    df_test = data_rescale(df_test, predictors)\n",
        "    df_train = df_train.dropna(subset=target)\n",
        "\n",
        "    # Check for NaN entries in the specified columns DEBUG\n",
        "    #for index, row in df_train.iterrows():\n",
        "     #   if row[target].isnull().any():\n",
        "      #      print(f\"Entry is NaN in row {index}\")\n",
        "\n",
        "   # print(df_train.head) ## debug\n",
        "  #  print(df_test.head)\n",
        "    predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
        "\n",
        "    df_test['predicted_target'] = predicted_scores\n",
        "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c in candidates:\n",
        "        candset = df_test[[c, 'predicted_target']].copy().dropna()\n",
        "        if candset.empty:\n",
        "            continue\n",
        "\n",
        "        pred_scores = candset['predicted_target']\n",
        "        candcol = candset[c]\n",
        "\n",
        "        X_pred = sm.add_constant(candcol)\n",
        "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
        "        results[c] = {\n",
        "            'R_squared': model_pred.rsquared,\n",
        "            'p_value': model_pred.pvalues[1],\n",
        "            'coef': model_pred.params[1]\n",
        "        }\n",
        "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------\n",
        "\n",
        "    best_proxies = []\n",
        "\n",
        "    if orthogonal_vars:\n",
        "        orth_scores = orthogonalize(candidates, df_test, orthogonal_vars, tau)\n",
        "        proxy_scores = {}\n",
        "        for c in candidates:\n",
        "            try:\n",
        "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_scores[c])\n",
        "            except KeyError as e:\n",
        "                continue\n",
        "\n",
        "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, score = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
        "    else:\n",
        "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, metrics = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
        "\n",
        "    return best_proxies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VGaIEWaFzii2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\")\n",
        "\n",
        "\n",
        "# Suppress numpy invalid operation warnings\n",
        "np.seterr(invalid='ignore')\n",
        "\n",
        "datafile_train =  \"/content/temp_yougov.dta\"\n",
        "datafile_test =  \"/content/temp_yougov.dta\"\n",
        "df_train = pd.read_stata(datafile_train)\n",
        "df_test = pd.read_stata(datafile_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZ9nBrqzlRK",
        "outputId": "8d9a82ec-6535-4417-d25f-8bb8c390c1bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "target = 'christian_nationalism'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'white',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                  'faminc_new'\n",
        "                   ]\n",
        "\n",
        "orthogonal_vars = [\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                    'faminc_new'\n",
        "                 ]\n",
        "\n",
        "\n",
        "\n",
        "best_proxies = proxy_finder_threshold_ortho(df_train, df_test, target, predictors, orth_weight=0.65, orthogonal_vars=orthogonal_vars, num_proxies=20)\n",
        "#print(best_proxies)\n",
        "### orth weight 0.9 --> version, how many interviews, etx\n",
        "### 0.85 same thing\n",
        "### 0.8\n",
        "### 0.75 bad\n",
        "### 0.7 bad"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
