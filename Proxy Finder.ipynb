{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "22bfc518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\kirin\\Untitled Folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768c6c7",
   "metadata": {},
   "source": [
    "Idea: Find a proxy for status threat to use in datasets that have no such measure but have common demographic variables such as sdo, christian nationalism, authoritarianism, ....  \n",
    "Steps:\n",
    "1. Use regression to predict status threat (perhaps single status threat item) based on other measures such that each measure has a corresponding measure in the new dataset.\n",
    "2. Regress predicted status threat on 'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology'\n",
    "3. use residuals as ST\\perp\n",
    "\n",
    "\n",
    "Then, iterate through each candidate proxy. Select proxy to minimize objective function: MSE?  \n",
    "\n",
    "MSE = \\sum_{i \\in (study participants)} (ST\\perp _{i} - candidate_{i})^2\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92922b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show only 5 colmns/rows\n",
    "pd.set_option('display.max_columns', 5)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "pd.set_option('display.max_colwidth', 5)\n",
    "pd.set_option('display.width', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "755bf76a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set display options to show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "224f669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_finder_validate(item, candidates, df1, df2):\n",
    "   \n",
    "   # assert (df1 != None)\n",
    "   # assert (df2 != None)\n",
    "\n",
    "    # validate proxies and st item\n",
    "    assert item in df1.columns\n",
    "\n",
    "    for c in candidates:\n",
    "        assert c in df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a41179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale all columns to be between 0 and 1, inclusive. Drop any non-numeric columns.\n",
    "def data_rescale(df):\n",
    "   \n",
    "    # Select only the numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e92016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a linear regression model to predict df[item] using df[predictors_df1]\n",
    "# report error and crash if predictors don't predict item\n",
    "def get_model(predictors_df1, item, test_pct, df):\n",
    "\n",
    "    # create linear model such that item_predicted = B0 + B1X1 + ... + BnXn\n",
    "    prepped_data = df[predictors_df1 + [item]].dropna(axis=0)\n",
    "    X = prepped_data[predictors_df1].to_numpy()\n",
    "\n",
    "    y = prepped_data[item].to_numpy()\n",
    "\n",
    "\n",
    "    #split into train and test data\n",
    "\n",
    "    test_size = (int) (X.shape[0] * test_pct)\n",
    "    train_size = (int) (X.shape[0] * (1-test_pct))\n",
    "\n",
    "    x_train = X[:-train_size]\n",
    "    x_test = X[-test_size:]\n",
    "\n",
    "    # Split the targets into training/testing sets\n",
    "    y_train = y[:-train_size]\n",
    "    y_test = y[-test_size:]\n",
    "\n",
    "\n",
    "    # run linear regression\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x_train, y_train)\n",
    "    \n",
    "    if (mean_squared_error(regr.predict(x_test), y_test) > 0.05):\n",
    "        print('predictors cannot predict item in df1')\n",
    "        assert(False)\n",
    "    \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222f7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns predicted item in df2\n",
    "def predict_item(df2, predictors_df2, model):\n",
    "  \n",
    "    # item_predicted = B0 + B1X1 + ... + BnXn\n",
    "    X = df2[predictors_df2] \n",
    "    \n",
    "    return np.dot(X, model.coef_) + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbc7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the best-fitting proxy out of candidates variables for predicted value of item in df1 using predictors_df1 variables\n",
    "# to find approximation for item in df2 using predictors_df2 variables. Approximation in df2 is purged of influence of \n",
    "# orthogonal_vars, if specified. If candidates not specified, consider all columns with numerical data to be candidates.\n",
    "def proxy_finder(df1, df2, item, predictors_df1, predictors_df2, num_proxies=1, candidates=None, orthogonal_vars=None):\n",
    "    #test size for linear regression training\n",
    "    test_size = 0.2\n",
    "    \n",
    "    if (candidates == None):\n",
    "        candidates = list(df2.select_dtypes(include='number').columns)\n",
    "    \n",
    "    # validate parameters and construct df2 prediction for item\n",
    "    proxy_finder_validate(item, candidates, df1, df2)\n",
    "    df1 = data_rescale(df1) # ensure each df is scaled between 0,1\n",
    "    df2 = data_rescale(df2)\n",
    "    regr = get_model(predictors_df1, item, test_size, df1)\n",
    "    item_pred = predict_item(df2, predictors_df2, regr)\n",
    "    \n",
    "    df2['item_pred'] = item_pred\n",
    "\n",
    "    # perform regression analysis for each candidate proxy\n",
    "    results = {}\n",
    "    \n",
    "    for c in candidates:\n",
    "        \n",
    "        # drop rows from item_pred and df2[c]\n",
    "        candset = df2[[c, 'item_pred']].copy()\n",
    "        \n",
    "        candset = candset.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        item_pred_drop = candset['item_pred']\n",
    "        candcol = candset[c]\n",
    "        \n",
    "        X = sm.add_constant(candcol)\n",
    "                            \n",
    "        model = sm.OLS(item_pred_drop, X).fit()\n",
    "        results[c] = {\n",
    "            'R_squared': model.rsquared,\n",
    "            'p_value': model.pvalues[1],  \n",
    "            'coef': model.params[1]\n",
    "        } \n",
    "  \n",
    "    # Select the proxy with the highest R-squared and significant p-value\n",
    "    # Sort the results by R_squared (descending) and p_value (ascending)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
    "    \n",
    "    best_proxies = []\n",
    "    \n",
    "    # add & print the top number_proxies\n",
    "    for i in range(min(num_proxies, len(sorted_results))):\n",
    "        proxy, metrics = sorted_results[i]\n",
    "        best_proxies.append(proxy)\n",
    "        print(f\"Proxy {i+1} for {item}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
    "    \n",
    "    return best_proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9166e2",
   "metadata": {},
   "source": [
    "### TOY RUN CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "596b7d2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_stata(datafile_proxy, convert_categoricals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# find and print suggested proxy\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m best_proxy \u001b[38;5;241m=\u001b[39m proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)\n",
      "Cell \u001b[1;32mIn[22], line 37\u001b[0m, in \u001b[0;36mproxy_finder\u001b[1;34m(df1, df2, item, predictors_df1, predictors_df2, num_proxies, candidates, orthogonal_vars)\u001b[0m\n\u001b[0;32m     32\u001b[0m     X \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(candcol)\n\u001b[0;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(item_pred_drop, X)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m     35\u001b[0m     results[c] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR_squared\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mrsquared,\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mpvalues[\u001b[38;5;241m1\u001b[39m],  \n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoef\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     39\u001b[0m     } \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Select the proxy with the highest R-squared and significant p-value\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Sort the results by R_squared (descending) and p_value (ascending)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR_squared\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1004\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     key \u001b[38;5;241m=\u001b[39m unpack_1tuple(key)\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[1;32m-> 1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# use this case to make sure the algorithm works\n",
    "\n",
    "# specific item we'd like to make a proxy for\n",
    "item = 'status_threat' \n",
    "\n",
    "# specific variables we use to predict the item in first dataframe\n",
    "predictors_df1 = [\n",
    "                   'psc1_W1_01',\n",
    "                   'party_ID',\n",
    "                   'age501',\n",
    "                   'education'] #CN AUTH RR SDO PID IDE + EDU AGE GEND\n",
    "\n",
    "# specific variables we use to predict the item in second dataframe. \n",
    "# These should correspond to the itemsin predictors_df1.\n",
    "predictors_df2 = [\n",
    "                   'psc1_W1_01',\n",
    "                   'party_ID',\n",
    "                   'age501',\n",
    "                   'education'\n",
    "                   ] \n",
    "\n",
    "# potential proxies\n",
    "candidates = ['christian_top',\n",
    "'age501',\n",
    "'education',\n",
    "'ideology',\n",
    "'christian_nationalism',\n",
    "'white_top',\n",
    "'status_threat',\n",
    "'SDO11',\n",
    "'social_dom11',\n",
    "'race_resent',\n",
    "'authoritarianism',\n",
    "'trumpfav'\n",
    "              ] \n",
    "\n",
    "# .dta file with item measure\n",
    "datafile_item = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "# .dta file we want to find a proxy in\n",
    "datafile_proxy = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "\n",
    "df1 = pd.read_stata(datafile_item)\n",
    "df2 = pd.read_stata(datafile_proxy, convert_categoricals=False)\n",
    "\n",
    "# find and print suggested proxy\n",
    "best_proxy = proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c3518",
   "metadata": {},
   "source": [
    "This is a nice sanity check. If we look for a proxy for status threat in the original status threat dataset, the best fitting proxy is status threat itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a63ae",
   "metadata": {},
   "source": [
    "### Status threat, GSS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2dcb147",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4149,0) and (8,) not aligned: 0 (dim 1) != 8 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_stata(datafile_st)\n\u001b[0;32m     53\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_stata(datafile_proxy, convert_categoricals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 56\u001b[0m proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)\n",
      "Cell \u001b[1;32mIn[65], line 11\u001b[0m, in \u001b[0;36mproxy_finder\u001b[1;34m(df1, df2, item, predictors_df1, predictors_df2, candidates, orthogonal_vars)\u001b[0m\n\u001b[0;32m      9\u001b[0m proxy_finder_validate(item, candidates, df1, df2)\n\u001b[0;32m     10\u001b[0m regr \u001b[38;5;241m=\u001b[39m get_model(predictors_df1, item, test_size, df1)\n\u001b[1;32m---> 11\u001b[0m item_pred \u001b[38;5;241m=\u001b[39m predict_item(df2, predictors_df2, regr)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# perform regression analysis for each candidate proxy\u001b[39;00m\n\u001b[0;32m     14\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[63], line 7\u001b[0m, in \u001b[0;36mpredict_item\u001b[1;34m(df2, predictors_df2, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_item\u001b[39m(df2, predictors_df2, model):\n\u001b[0;32m      3\u001b[0m   \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# item_predicted = B0 + B1X1 + ... + BnXn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     X \u001b[38;5;241m=\u001b[39m df2[predictors_df2]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;66;03m#ensure this drops rows with nans\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(X, model\u001b[38;5;241m.\u001b[39mcoef_) \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4149,0) and (8,) not aligned: 0 (dim 1) != 8 (dim 0)"
     ]
    }
   ],
   "source": [
    "# specific item we'd like to make a proxy for\n",
    "item = 'status_threat' \n",
    "\n",
    "# specific variables we use to predict the item in first dataframe\n",
    "predictors_df1 = [\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education'] \n",
    "\n",
    "# specific variables we use to predict the item in second dataframe. \n",
    "# These should correspond to the itemsin predictors_df1.\n",
    "predictors_df2 = [\n",
    "                  \n",
    "                   ] \n",
    "\n",
    "# variables we'd like to remove the influence of on predicted item\n",
    "orthogonal_vars = ['christian_nationalism', \n",
    "                   'authoritarianism', \n",
    "                   'social_dom11', \n",
    "                   'race_resent', \n",
    "                   'party_ID', \n",
    "                   'ideology']\n",
    "\n",
    "# potential proxies\n",
    "candidates = ['spocc10',\n",
    "              'sppres10', \n",
    "              'sppres80',\n",
    "              'spind10',\n",
    "              'prestg10',\n",
    "              'occ10',\n",
    "              'wrkstat',\n",
    "              'divorce',\n",
    "              'paocc10',\n",
    "              'papres10',\n",
    "              'maocc10',\n",
    "              'mapres10',\n",
    "              'paind10',\n",
    "              'maind10',\n",
    "              ] \n",
    "\n",
    "# .dta file with item measure\n",
    "datafile_item = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "# .dta file we want to find a proxy in\n",
    "datafile_proxy = r'C:\\Users\\kirin\\Downloads\\GSS2022.dta'\n",
    "\n",
    "\n",
    "df1 = pd.read_stata(datafile_st)\n",
    "df2 = pd.read_stata(datafile_proxy, convert_categoricals=False)\n",
    "\n",
    "# find and print suggested proxy\n",
    "proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e667502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset:\n",
      "year\n",
      "id\n",
      "wrkstat\n",
      "hrs1\n",
      "hrs2\n",
      "evwork\n",
      "wrkslf\n",
      "occ10\n",
      "prestg10\n",
      "indus10\n",
      "marital\n",
      "martype\n",
      "divorce\n",
      "widowed\n",
      "spwrksta\n",
      "sphrs1\n",
      "sphrs2\n",
      "spevwork\n",
      "cowrksta\n",
      "coevwork\n",
      "cohrs1\n",
      "cohrs2\n",
      "spwrkslf\n",
      "sppres80\n",
      "spocc10\n",
      "sppres10\n",
      "spind10\n",
      "coocc10\n",
      "coind10\n",
      "pawrkslf\n",
      "paocc10\n",
      "papres10\n",
      "paind10\n",
      "mawrkslf\n",
      "maocc10\n",
      "mapres10\n",
      "maind10\n",
      "sibs\n",
      "childs\n",
      "age\n",
      "agekdbrn\n",
      "educ\n",
      "paeduc\n",
      "maeduc\n",
      "speduc\n",
      "coeduc\n",
      "codeg\n",
      "degree\n",
      "padeg\n",
      "madeg\n",
      "spdeg\n",
      "major1\n",
      "major2\n",
      "dipged\n",
      "sex\n",
      "race\n",
      "res16\n",
      "reg16\n",
      "mobile16\n",
      "family16\n",
      "famdif16\n",
      "mawrkgrw\n",
      "incom16\n",
      "born\n",
      "parborn\n",
      "granborn\n",
      "hompop\n",
      "babies\n",
      "preteen\n",
      "teens\n",
      "adults\n",
      "unrelat\n",
      "earnrs\n",
      "income\n",
      "rincome\n",
      "income16\n",
      "rincom16\n",
      "region\n",
      "xnorcsiz\n",
      "srcbelt\n",
      "size\n",
      "partyid\n",
      "vote16\n",
      "pres16\n",
      "if16who\n",
      "polviews\n",
      "natspac\n",
      "natenvir\n",
      "natheal\n",
      "natcity\n",
      "natcrime\n",
      "natdrug\n",
      "nateduc\n",
      "natrace\n",
      "natarms\n",
      "nataid\n",
      "natfare\n",
      "natroad\n",
      "natsoc\n",
      "natmass\n",
      "natpark\n",
      "natchld\n",
      "natsci\n",
      "natenrgy\n",
      "natspacy\n",
      "natenviy\n",
      "nathealy\n",
      "natcityy\n",
      "natcrimy\n",
      "natdrugy\n",
      "nateducy\n",
      "natracey\n",
      "natarmsy\n",
      "nataidy\n",
      "natfarey\n",
      "eqwlth\n",
      "tax\n",
      "spkath\n",
      "colath\n",
      "libath\n",
      "spkrac\n",
      "colrac\n",
      "librac\n",
      "spkcom\n",
      "colcom\n",
      "libcom\n",
      "spkmslm\n",
      "colmslm\n",
      "libmslm\n",
      "cappun\n",
      "gunlaw\n",
      "courts\n",
      "grass\n",
      "relig\n",
      "denom\n",
      "other\n",
      "jew\n",
      "fund\n",
      "attend\n",
      "reliten\n",
      "postlife\n",
      "pray\n",
      "popespks\n",
      "relig16\n",
      "denom16\n",
      "oth16\n",
      "jew16\n",
      "fund16\n",
      "sprel\n",
      "spden\n",
      "spother\n",
      "spjew\n",
      "spfund\n",
      "corel\n",
      "coden\n",
      "coother\n",
      "cojew\n",
      "cofund\n",
      "prayer\n",
      "bible\n",
      "racopen\n",
      "raclive\n",
      "affrmact\n",
      "wrkwayup\n",
      "happy\n",
      "hapmar\n",
      "hapcohab\n",
      "health\n",
      "life\n",
      "helpful\n",
      "fair\n",
      "trust\n",
      "confinan\n",
      "conbus\n",
      "conclerg\n",
      "coneduc\n",
      "confed\n",
      "conlabor\n",
      "conpress\n",
      "conmedic\n",
      "contv\n",
      "conjudge\n",
      "consci\n",
      "conlegis\n",
      "conarmy\n",
      "obey\n",
      "popular\n",
      "thnkself\n",
      "workhard\n",
      "helpoth\n",
      "socrel\n",
      "socommun\n",
      "socfrend\n",
      "socbar\n",
      "aged\n",
      "weekswrk\n",
      "partfull\n",
      "joblose\n",
      "jobfind\n",
      "satjob\n",
      "richwork\n",
      "class\n",
      "rank\n",
      "satfin\n",
      "finalter\n",
      "finrela\n",
      "wksub\n",
      "wksubs\n",
      "wksub1\n",
      "wksubs1\n",
      "wksup\n",
      "wksups\n",
      "wksup1\n",
      "wksups1\n",
      "unemp\n",
      "union\n",
      "union1\n",
      "getahead\n",
      "parsol\n",
      "kidssol\n",
      "fepol\n",
      "abdefect\n",
      "abnomore\n",
      "abhlth\n",
      "abpoor\n",
      "abrape\n",
      "absingle\n",
      "abany\n",
      "chldidel\n",
      "pillok\n",
      "sexeduc\n",
      "divlaw\n",
      "premarsx\n",
      "teensex\n",
      "xmarsex\n",
      "homosex\n",
      "pornlaw\n",
      "xmovie\n",
      "spanking\n",
      "letdie1\n",
      "suicide1\n",
      "suicide2\n",
      "suicide3\n",
      "suicide4\n",
      "polhitok\n",
      "polabuse\n",
      "polmurdr\n",
      "polescap\n",
      "polattak\n",
      "fear\n",
      "owngun\n",
      "pistol\n",
      "shotgun\n",
      "rifle\n",
      "rowngun\n",
      "hunt\n",
      "hunt1\n",
      "news\n",
      "tvhours\n",
      "phone\n",
      "coop\n",
      "comprend\n",
      "form\n",
      "fechld\n",
      "fepresch\n",
      "fefam\n",
      "racdif1\n",
      "racdif2\n",
      "racdif3\n",
      "racdif4\n",
      "helppoor\n",
      "helpnot\n",
      "helpsick\n",
      "helpblk\n",
      "god\n",
      "reborn\n",
      "savesoul\n",
      "wlthwhts\n",
      "wlthblks\n",
      "wlthhsps\n",
      "workwhts\n",
      "workblks\n",
      "workhsps\n",
      "intlwhts\n",
      "intlblks\n",
      "intlhsps\n",
      "liveblks\n",
      "marblk\n",
      "marasian\n",
      "marhisp\n",
      "marwht\n",
      "racwork\n",
      "discaff\n",
      "yousup\n",
      "spwksup\n",
      "fejobaff\n",
      "discaffm\n",
      "discaffw\n",
      "fehire\n",
      "relpersn\n",
      "sprtprsn\n",
      "othlang\n",
      "spklang\n",
      "betrlang\n",
      "letinhsp\n",
      "letinasn\n",
      "compuse\n",
      "webmob\n",
      "emailmin\n",
      "emailhr\n",
      "usewww\n",
      "wwwhr\n",
      "wwwmin\n",
      "huclean\n",
      "wrktype\n",
      "yearsjob\n",
      "waypaid\n",
      "wrksched\n",
      "moredays\n",
      "mustwork\n",
      "wrkhome\n",
      "whywkhme\n",
      "famwkoff\n",
      "wkvsfam\n",
      "famvswk\n",
      "hrsrelax\n",
      "secondwk\n",
      "learnnew\n",
      "workfast\n",
      "workdiff\n",
      "overwork\n",
      "knowwhat\n",
      "myskills\n",
      "respect\n",
      "trustman\n",
      "safetywk\n",
      "safefrst\n",
      "teamsafe\n",
      "safehlth\n",
      "proudemp\n",
      "prodctiv\n",
      "wksmooth\n",
      "trdunion\n",
      "partteam\n",
      "wkdecide\n",
      "toofewwk\n",
      "promteok\n",
      "opdevel\n",
      "hlpequip\n",
      "haveinfo\n",
      "wkfreedm\n",
      "fringeok\n",
      "supcares\n",
      "condemnd\n",
      "promtefr\n",
      "cowrkint\n",
      "jobsecok\n",
      "suphelp\n",
      "wrktime\n",
      "cowrkhlp\n",
      "manvsemp\n",
      "hvylift\n",
      "handmove\n",
      "wkpraise\n",
      "fairearn\n",
      "rincblls\n",
      "laidoff\n",
      "jobfind1\n",
      "trynewjb\n",
      "wkageism\n",
      "wkracism\n",
      "wksexism\n",
      "wkharsex\n",
      "wkharoth\n",
      "health1\n",
      "physhlth\n",
      "mntlhlth\n",
      "hlthdays\n",
      "usedup\n",
      "backpain\n",
      "painarms\n",
      "hurtatwk\n",
      "spvtrfair\n",
      "strredpg\n",
      "phyeffrt\n",
      "slpprblm\n",
      "satjob1\n",
      "knowschd\n",
      "usetech\n",
      "stress12\n",
      "hyperten\n",
      "arthrtis\n",
      "diabetes\n",
      "depress\n",
      "weight\n",
      "height\n",
      "ntwkhard\n",
      "misswork\n",
      "lifenow\n",
      "lifein5\n",
      "disrspct\n",
      "poorserv\n",
      "notsmart\n",
      "afraidof\n",
      "threaten\n",
      "abmoral\n",
      "abhelp1\n",
      "abhelp2\n",
      "abhelp3\n",
      "abhelp4\n",
      "workfor1\n",
      "ownstock\n",
      "stockops\n",
      "extrapay\n",
      "compperf\n",
      "deptperf\n",
      "indperf\n",
      "extrayr\n",
      "numemps\n",
      "wrkslffam\n",
      "nextgen\n",
      "toofast\n",
      "advfront\n",
      "scientgo\n",
      "scienthe\n",
      "scientbe\n",
      "buyvalue\n",
      "compwage\n",
      "empinput\n",
      "slfmangd\n",
      "emptrain\n",
      "wealth\n",
      "esop\n",
      "defpensn\n",
      "ratetone\n",
      "posslq\n",
      "posslqy\n",
      "marcohab\n",
      "healthissp\n",
      "endsmeet\n",
      "goodlife\n",
      "famsuffr\n",
      "homekid\n",
      "housewrk\n",
      "wrkbaby\n",
      "wrksch\n",
      "marlegit\n",
      "marmakid\n",
      "marpakid\n",
      "marhomo\n",
      "numkids\n",
      "kidnofre\n",
      "hubbywk1\n",
      "meovrwrk\n",
      "cohabok\n",
      "laundry1\n",
      "caresik1\n",
      "shop1\n",
      "cooking1\n",
      "rhhwork\n",
      "sphhwork\n",
      "happy7\n",
      "ssfchild\n",
      "ssmchild\n",
      "kidsocst\n",
      "paidlv\n",
      "paidlvdv\n",
      "famwkbst\n",
      "famwklst\n",
      "careprov\n",
      "carecost\n",
      "eldhelp\n",
      "eldcost\n",
      "hhclean1\n",
      "tiredhm1\n",
      "jobvsfa1\n",
      "tiredwk1\n",
      "famvswk1\n",
      "rfamlook\n",
      "spfalook\n",
      "stress\n",
      "supervis\n",
      "localnum\n",
      "hapunhap\n",
      "madenkid\n",
      "relactiv\n",
      "immcrime\n",
      "immjobs\n",
      "letin1a\n",
      "partners\n",
      "matesex\n",
      "frndsex\n",
      "acqntsex\n",
      "pikupsex\n",
      "paidsex\n",
      "othersex\n",
      "sexsex\n",
      "sexfreq\n",
      "numwomen\n",
      "nummen\n",
      "partnrs5\n",
      "sexsex5\n",
      "evpaidsx\n",
      "evstray\n",
      "condom\n",
      "relatsex\n",
      "evidu\n",
      "idu30\n",
      "evcrack\n",
      "crack30\n",
      "hivtest\n",
      "hivtest1\n",
      "hivtest2\n",
      "sexornt\n",
      "realinc\n",
      "realrinc\n",
      "coninc\n",
      "conrinc\n",
      "ethnic\n",
      "eth1\n",
      "eth2\n",
      "eth3\n",
      "hispanic\n",
      "racecen1\n",
      "racecen2\n",
      "racecen3\n",
      "uscitzn\n",
      "fucitzn\n",
      "yearsusa\n",
      "mnthsusa\n",
      "vetyears\n",
      "dwelling\n",
      "dwelown\n",
      "dwelown16\n",
      "worda\n",
      "wordb\n",
      "wordc\n",
      "wordd\n",
      "worde\n",
      "wordf\n",
      "wordg\n",
      "wordh\n",
      "wordi\n",
      "wordj\n",
      "wordsum\n",
      "relate1\n",
      "gender1\n",
      "old1\n",
      "mar1\n",
      "away1\n",
      "where1\n",
      "relate2\n",
      "gender2\n",
      "old2\n",
      "mar2\n",
      "away2\n",
      "where2\n",
      "relate3\n",
      "gender3\n",
      "old3\n",
      "mar3\n",
      "away3\n",
      "where3\n",
      "relate4\n",
      "gender4\n",
      "old4\n",
      "mar4\n",
      "away4\n",
      "where4\n",
      "relate5\n",
      "gender5\n",
      "old5\n",
      "mar5\n",
      "away5\n",
      "where5\n",
      "relate6\n",
      "gender6\n",
      "old6\n",
      "mar6\n",
      "away6\n",
      "where6\n",
      "relate7\n",
      "gender7\n",
      "old7\n",
      "mar7\n",
      "away7\n",
      "where7\n",
      "relate8\n",
      "gender8\n",
      "old8\n",
      "mar8\n",
      "away8\n",
      "where8\n",
      "relate9\n",
      "gender9\n",
      "old9\n",
      "mar9\n",
      "away9\n",
      "where9\n",
      "relate10\n",
      "gender10\n",
      "old10\n",
      "mar10\n",
      "away10\n",
      "where10\n",
      "relate11\n",
      "gender11\n",
      "old11\n",
      "mar11\n",
      "away11\n",
      "where11\n",
      "relate12\n",
      "gender12\n",
      "old12\n",
      "mar12\n",
      "away12\n",
      "where12\n",
      "relate13\n",
      "gender13\n",
      "old13\n",
      "mar13\n",
      "away13\n",
      "where13\n",
      "relate14\n",
      "away14\n",
      "where14\n",
      "relhhd1\n",
      "relhhd2\n",
      "relhhd3\n",
      "relhhd4\n",
      "relhhd5\n",
      "relhhd6\n",
      "relhhd7\n",
      "relhhd8\n",
      "relhhd9\n",
      "relhhd10\n",
      "relhhd11\n",
      "relhhd12\n",
      "relhhd13\n",
      "relhhd14\n",
      "respnum\n",
      "hhtype\n",
      "hhtype1\n",
      "famgen\n",
      "rplace\n",
      "rvisitor\n",
      "visitors\n",
      "relhh1\n",
      "relhh2\n",
      "relhh3\n",
      "relhh4\n",
      "relhh5\n",
      "relhh6\n",
      "relhh7\n",
      "relhh8\n",
      "relhh9\n",
      "relhh10\n",
      "relhh11\n",
      "relhh12\n",
      "relhh13\n",
      "relhh14\n",
      "relsp1\n",
      "relsp2\n",
      "relsp3\n",
      "relsp4\n",
      "relsp5\n",
      "relsp6\n",
      "relsp7\n",
      "relsp8\n",
      "relsp9\n",
      "relsp10\n",
      "relsp11\n",
      "relsp12\n",
      "relsp13\n",
      "relsp14\n",
      "dateintv\n",
      "sei10\n",
      "pasei10\n",
      "masei10\n",
      "spsei10\n",
      "cosei10\n",
      "copres10\n",
      "uswary\n",
      "cohort\n",
      "marcohrt\n",
      "zodiac\n",
      "whoelse1\n",
      "whoelse2\n",
      "whoelse3\n",
      "whoelse4\n",
      "whoelse5\n",
      "whoelse6\n",
      "mode\n",
      "consent\n",
      "adminconsent\n",
      "letdie1y\n",
      "ballot\n",
      "version\n",
      "issp\n",
      "formwt\n",
      "sampcode\n",
      "sample\n",
      "oversamp\n",
      "spanself\n",
      "spanint\n",
      "spaneng\n",
      "hlthstrt\n",
      "huadd\n",
      "huaddwhy\n",
      "dwellpre\n",
      "kidsinhh\n",
      "respond\n",
      "incuspop\n",
      "neisafe\n",
      "rlooks\n",
      "rgroomed\n",
      "rhlthend\n",
      "vstrat\n",
      "vpsu\n",
      "famdif16y\n",
      "pawrkslf2\n",
      "mawrkslf2\n",
      "ethworld1\n",
      "ethworld2\n",
      "ethworld3\n",
      "ethworld4\n",
      "ethworld5\n",
      "ethworld6\n",
      "ethworld7\n",
      "ethworld8\n",
      "ethworld9\n",
      "ethregion1\n",
      "ethregion2\n",
      "ethregion3\n",
      "ethregion4\n",
      "ethregion5\n",
      "ethregion6\n",
      "ethregion7\n",
      "ethregion8\n",
      "ethregion9\n",
      "ethregion10\n",
      "ethregion11\n",
      "ethregion12\n",
      "ethregion13\n",
      "ethregion14\n",
      "ethregion15\n",
      "ethregion16\n",
      "ethregion17\n",
      "ethregion18\n",
      "ethregion19\n",
      "ethregion20\n",
      "ethregion21\n",
      "ethregion22\n",
      "ethregion23\n",
      "ethregion24\n",
      "ethregion25\n",
      "ethregion26\n",
      "ethregion27\n",
      "ethregion28\n",
      "ethregion29\n",
      "ethregion30\n",
      "ethregion31\n",
      "ethregion32\n",
      "ethregion33\n",
      "ethregion34\n",
      "ethregion35\n",
      "ethregion36\n",
      "ethregion37\n",
      "ethregion38\n",
      "ethregion39\n",
      "ethregion40\n",
      "ethregion41\n",
      "ethregion42\n",
      "ethregion43\n",
      "ethregion44\n",
      "ethregion45\n",
      "ethregion46\n",
      "ethregion47\n",
      "ethregion48\n",
      "ethregion49\n",
      "ethregion50\n",
      "ethregion51\n",
      "ethregion52\n",
      "ethregion53\n",
      "ethregion54\n",
      "ethregion55\n",
      "ethregion56\n",
      "ethregion57\n",
      "ethregion58\n",
      "ethregion59\n",
      "ethregion60\n",
      "ethregion61\n",
      "ethregion62\n",
      "ethregion63\n",
      "ethregion64\n",
      "ethregion65\n",
      "ethregion66\n",
      "ethregion67\n",
      "ethregion68\n",
      "ethregion69\n",
      "ethregion70\n",
      "ethregion71\n",
      "ethregion72\n",
      "ethregion73\n",
      "ethregion74\n",
      "ethregion75\n",
      "ethregion76\n",
      "ethregion77\n",
      "ethregion78\n",
      "ethregion79\n",
      "ethregion80\n",
      "ethregion81\n",
      "ethregion82\n",
      "ethregion83\n",
      "ethregion84\n",
      "ethregion85\n",
      "ethregion86\n",
      "ethregion87\n",
      "ethregion88\n",
      "ethregion89\n",
      "ethregion90\n",
      "ethregion91\n",
      "ethregion92\n",
      "ethregion93\n",
      "ethregion94\n",
      "ethregion96\n",
      "wrkgovt1\n",
      "wrkgovt2\n",
      "spkathy\n",
      "libathy\n",
      "spkracy\n",
      "libracy\n",
      "spkcomy\n",
      "colcomy\n",
      "libcomy\n",
      "spkmslmy\n",
      "libmslmy\n",
      "polhitoky\n",
      "polabusey\n",
      "polattaky\n",
      "raceacs1\n",
      "raceacs2\n",
      "raceacs3\n",
      "raceacs4\n",
      "raceacs5\n",
      "raceacs6\n",
      "raceacs7\n",
      "raceacs8\n",
      "raceacs9\n",
      "raceacs10\n",
      "raceacs11\n",
      "raceacs12\n",
      "raceacs13\n",
      "raceacs14\n",
      "raceacs15\n",
      "raceacs16\n",
      "abdefectg\n",
      "abnomoreg\n",
      "abhlthg\n",
      "abpoorg\n",
      "abrapeg\n",
      "absingleg\n",
      "suicide1g\n",
      "suicide2g\n",
      "suicide3g\n",
      "suicide4g\n",
      "maborn\n",
      "paborn\n",
      "sexbirth1\n",
      "sexnow1\n",
      "hivafraid\n",
      "hivimmrl\n",
      "hivdscrm\n",
      "ptnrornt\n",
      "ptnrsxbrth\n",
      "ptnrsxnow\n",
      "conpharvacy\n",
      "confedvacy\n",
      "wtssps\n",
      "wtssnrps\n",
      "uswaryv\n",
      "prayerv\n",
      "courtsv\n",
      "discaffwv\n",
      "racopenv\n",
      "getaheadv\n",
      "divlawv\n",
      "helpfulv\n",
      "fairv\n",
      "trustv\n",
      "agedv\n",
      "grassv\n",
      "relitenv\n",
      "biblev\n",
      "postlifev\n",
      "kidssolv\n",
      "uscitznv\n",
      "fucitznv\n",
      "fepolv\n",
      "uswarynv\n",
      "prayernv\n",
      "courtsnv\n",
      "discaffwnv\n",
      "racopennv\n",
      "getaheadnv\n",
      "divlawnv\n",
      "helpfulnv\n",
      "fairnv\n",
      "trustnv\n",
      "agednv\n",
      "grassnv\n",
      "relitennv\n",
      "biblenv\n",
      "postlifenv\n",
      "kidssolnv\n",
      "uscitznnv\n",
      "fucitznnv\n",
      "fepolnv\n",
      "abanyg\n",
      "fileversion\n",
      "racerank1\n",
      "racerank2\n",
      "racerank3\n",
      "racopeny\n",
      "adoptus\n",
      "immfate\n",
      "vote20\n",
      "pres20\n",
      "if20who\n",
      "wordk\n",
      "wordl\n",
      "wordn\n",
      "fechld2\n",
      "fepresch2\n",
      "rspgndr\n",
      "prntlk\n",
      "prntfnce\n",
      "prntcre\n",
      "prntply\n",
      "prntbhav\n",
      "prntadvs\n",
      "prntmdl\n",
      "orginc\n",
      "plan1\n",
      "sharehhw\n",
      "clsrltv\n",
      "rsprltv1\n",
      "rsprltv2\n",
      "eldfnce\n",
      "rlyrltv\n",
      "frndfam\n",
      "cabgndr\n",
      "univgndr\n",
      "execgndr\n",
      "yrfnce\n",
      "nmbrkids\n",
      "conhlth\n",
      "hlthbtr\n",
      "hlthmore\n",
      "hlthgov\n",
      "hlthinf\n",
      "hlthtax\n",
      "hlthctzn\n",
      "hlthdmg\n",
      "hlthacc1\n",
      "hlthacc2\n",
      "hlthacc3\n",
      "hlthacc4\n",
      "hlthbeh\n",
      "hlthenv\n",
      "hlthgene\n",
      "hlthpoor\n",
      "altmed\n",
      "doctrst\n",
      "docskls\n",
      "docearn\n",
      "hlthweb\n",
      "hlthwblif\n",
      "hlthwbanx\n",
      "hlthwbvax\n",
      "webhltbeh\n",
      "webdocexp\n",
      "websympt\n",
      "webdradv\n",
      "webrely\n",
      "vaxdoharm\n",
      "immunbetr\n",
      "hlthprb\n",
      "hlthpain\n",
      "hlthdep\n",
      "hlthconf\n",
      "hlthnot\n",
      "docvst\n",
      "docalt\n",
      "medpay\n",
      "medcommt\n",
      "medwtlst\n",
      "medbest\n",
      "hlthsat\n",
      "docsat1\n",
      "altsat\n",
      "smokeday\n",
      "drinkday1\n",
      "physact\n",
      "frtvegs\n",
      "disblty\n",
      "weight_issp\n",
      "shutbus\n",
      "stayhome\n",
      "mobilsurv\n",
      "reqmasks\n",
      "bangather\n",
      "stockval1\n",
      "stockyr\n",
      "stockyrval\n",
      "stockoptyr\n",
      "stoptyramt\n",
      "extr2021\n",
      "extraval1\n",
      "yearval1\n",
      "numorg1\n",
      "perfrt\n",
      "chngtime\n",
      "wrkmeangfl\n",
      "strmgtsup\n",
      "psysamephys\n",
      "allorglevel\n",
      "feelnerv\n",
      "worry\n",
      "feeldown\n",
      "nointerest\n",
      "svyenjoy\n",
      "svyid1\n",
      "svyid2\n",
      "baselinestatus\n",
      "amerstatus\n",
      "yrlvmus\n",
      "yrartxbt\n",
      "yrmovie\n",
      "artsout\n",
      "yrcreat\n",
      "yrrdg\n",
      "yrtour\n",
      "yrstmus\n",
      "yrarmus\n",
      "yrstpo\n",
      "yrarpo\n",
      "yrclass\n",
      "yrpod\n",
      "cvdlvmus\n",
      "cvdart\n",
      "cvdmov\n",
      "cvdcreat\n",
      "cvdrdg\n",
      "cvdtour\n",
      "cvdstmus\n",
      "cvdarmus\n",
      "cvdstpo\n",
      "cvdarpo\n",
      "cvdclass\n",
      "cvdpod\n",
      "neastatus\n",
      "wrkwayup_next\n",
      "blkmblty\n",
      "blkdsrv\n",
      "blktry\n",
      "brv5\n",
      "brv5sp\n",
      "brv5par\n",
      "brv5grand\n",
      "brv5child\n",
      "brv5sib\n",
      "brv5oth\n",
      "brv5spnum\n",
      "brv5partnum\n",
      "brv5dadnum\n",
      "brv5momnum\n",
      "brv5filnum\n",
      "brv5milnum\n",
      "brv5gmanum\n",
      "brv5gpanum\n",
      "brv5sonnum\n",
      "brv5daunum\n",
      "brv5chinum\n",
      "brv5bronum\n",
      "brv5sisnum\n",
      "brv5silnum\n",
      "brv5cuznum\n",
      "brv5frndnum\n",
      "brv5cowknum\n",
      "brv5othnum\n",
      "brv16\n",
      "brv16sp\n",
      "brv16par\n",
      "brv16sib\n",
      "brv16grand\n",
      "brv16oth\n",
      "brv16spnum\n",
      "brv16partnum\n",
      "brv16dadnum\n",
      "brv16momnum\n",
      "brv16filnum\n",
      "brv16milnum\n",
      "brv16gmanum\n",
      "brv16gpanum\n",
      "brv16bronum\n",
      "brv16sisnum\n",
      "brv16silnum\n",
      "brv16cuznum\n",
      "brv16frndnum\n",
      "brv16othnum\n",
      "gestate\n",
      "abgender\n",
      "abbelief\n",
      "vaxhstncy\n",
      "vaxkids\n",
      "vaxsafe\n",
      "fluvax\n",
      "covid12\n",
      "covemply\n",
      "pandinc\n",
      "pandmet\n",
      "biokids\n",
      "malekids\n",
      "firstkidsex\n",
      "nonbinkids\n",
      "femself\n",
      "mascself\n",
      "nextstatus\n",
      "worksick\n",
      "fund_next\n",
      "hompop_exp\n",
      "modesequence\n",
      "rheight\n",
      "instype01\n",
      "instype02\n",
      "instype03\n",
      "instype04\n",
      "totalincentive\n",
      "babies_exp\n",
      "preteen_exp\n",
      "teens_exp\n",
      "adults_exp\n",
      "childs_exp\n",
      "childsinhh\n",
      "respnumh\n",
      "hefinfo1\n",
      "famgen_exp\n",
      "adultsinhh\n",
      "hhtype1_exp\n",
      "wtssps_nea\n",
      "wtssnrps_nea\n",
      "wtssps_next\n",
      "wtssnrps_next\n",
      "wtssps_as\n",
      "wtssnrps_as\n"
     ]
    }
   ],
   "source": [
    "# Print all column names to verify\n",
    "print(\"Column names in the dataset:\")\n",
    "column_names = df2.columns.tolist()\n",
    "print(\"\\n\".join(column_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8961ee8",
   "metadata": {},
   "source": [
    "### ANES 2020 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1988ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, we'll have to make approximations for predictors that are not explicitly measured in the second (ANES) \n",
    "#dataset. \n",
    "# .dta file we want to find a proxy in\n",
    "filepath_proxy = r'C:\\Users\\kirin\\Downloads\\anes2020\\anes_timeseries_2020_stata_20220210.dta'\n",
    "df2 = pd.read_stata(filepath_proxy, convert_categoricals=False)\n",
    "\n",
    "df2['psc1_W1_01'] = df2[['V202311', 'V202312', 'V202304']].mean(axis=1)\n",
    "\n",
    "df2['christian_nationalism'] = df2['V202169']\n",
    "\n",
    "df2['authoritarianism'] = df2[['V202163', 'V202302', 'V202158', 'V202170', 'V202159']].mean(axis=1)\n",
    "\n",
    "df2['social_dom11'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "df2['race_resent'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "df2['race_resent'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "df2['race_resent'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e8ef8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy 1 for status_threat: V202312 with R_squared: 0.9783593534925631 and p_value: 0.0\n",
      "Proxy 2 for status_threat: V202311 with R_squared: 0.7836887583576582 and p_value: 0.0\n",
      "Proxy 3 for status_threat: V202326 with R_squared: 0.7622119691187061 and p_value: 0.0\n",
      "Proxy 4 for status_threat: V202337 with R_squared: 0.7621547750669253 and p_value: 0.0\n",
      "Proxy 5 for status_threat: V202339 with R_squared: 0.7593616347111407 and p_value: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['V202312', 'V202311', 'V202326', 'V202337', 'V202339']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specific item we'd like to make a proxy for\n",
    "item = 'status_threat' \n",
    "\n",
    "# specific variables we use to predict the item in first dataframe\n",
    "predictors_df1 = [\n",
    "                   'psc1_W1_01',\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education'] \n",
    "\n",
    "\n",
    "# specific variables we use to predict the item in second dataframe. \n",
    "# These should correspond to the itemsin predictors_df1.\n",
    "predictors_df2 = [\n",
    "                  'V202312', #psc item\n",
    "                  'V202169', # rate christians\n",
    "                  'V201507x' #age\n",
    "                   ] \n",
    "\n",
    "# variables we'd like to remove the influence of on predicted item\n",
    "#orthogonal_vars = []\n",
    "\n",
    "# potential proxies\n",
    "#candidates = [\n",
    " #             ] \n",
    "\n",
    "# .dta file with item measure\n",
    "filepath_item = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_stata(filepath_item)\n",
    "\n",
    "\n",
    "# find and print suggested proxy\n",
    "proxy_finder(df1, df2, item, predictors_df1, predictors_df2, num_proxies=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01828651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional check for predictive power of other measures\n",
    "\n",
    "check_results = {}\n",
    "\n",
    "for var in orthogonal_vars:\n",
    "    model_check = sm.OLS(df2[best_proxy], sm.add_constant(df2[var])).fit()\n",
    "    check_results[var] = {\n",
    "        'R_squared': model_check.rsquared,\n",
    "        'p_value': model_check.pvalues[1]  # p-value for the orthogonal variable\n",
    "    }\n",
    "\n",
    "print(\"Predictive power of other measures on the selected proxy:\")\n",
    "print(pd.DataFrame(check_results).transpose())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
