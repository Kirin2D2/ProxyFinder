{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bfc518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\kirin\\OURSIP_summer24\\ProxyFinder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", current_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768c6c7",
   "metadata": {},
   "source": [
    "Idea: Find a proxy for status threat to use in datasets that have no such measure but have common demographic variables such as sdo, christian nationalism, authoritarianism, ....  \n",
    "Steps:\n",
    "1. Use regression to predict status threat (perhaps single status threat item) based on other measures such that each measure has a corresponding measure in the new dataset.\n",
    "2. Regress predicted status threat on 'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology'\n",
    "3. use residuals as ST\\perp\n",
    "\n",
    "\n",
    "Then, iterate through each candidate proxy. Select proxy to minimize objective function: MSE?  \n",
    "\n",
    "MSE = \\sum_{i \\in (study participants)} (ST\\perp _{i} - candidate_{i})^2\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92922b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show only 5 colmns/rows\n",
    "pd.set_option('display.max_columns', 5)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "pd.set_option('display.max_colwidth', 5)\n",
    "pd.set_option('display.width', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755bf76a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set display options to show all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224f669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_finder_validate(item, candidates, df1, df2):\n",
    "   \n",
    "   # assert (df1 != None)\n",
    "   # assert (df2 != None)\n",
    "\n",
    "    # validate proxies and st item\n",
    "    assert item in df1.columns\n",
    "\n",
    "    for c in candidates:\n",
    "        assert c in df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a41179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale all columns to be between 0 and 1, inclusive. Drop any non-numeric columns.\n",
    "def data_rescale(df):\n",
    "   \n",
    "    # Select only the numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e92016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a linear regression model to predict df[item] using df[predictors_df1]\n",
    "# report error and crash if predictors don't predict item\n",
    "def get_model(predictors_df1, item, test_pct, df):\n",
    "\n",
    "    # create linear model such that item_predicted = B0 + B1X1 + ... + BnXn\n",
    "    prepped_data = df[predictors_df1 + [item]].dropna(axis=0)\n",
    "    X = prepped_data[predictors_df1].to_numpy()\n",
    "\n",
    "    y = prepped_data[item].to_numpy()\n",
    "\n",
    "\n",
    "    #split into train and test data\n",
    "\n",
    "    test_size = (int) (X.shape[0] * test_pct)\n",
    "    train_size = (int) (X.shape[0] * (1-test_pct))\n",
    "\n",
    "    x_train = X[:-train_size]\n",
    "    x_test = X[-test_size:]\n",
    "\n",
    "    # Split the targets into training/testing sets\n",
    "    y_train = y[:-train_size]\n",
    "    y_test = y[-test_size:]\n",
    "\n",
    "\n",
    "    # run linear regression\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x_train, y_train)\n",
    "    \n",
    "    if (mean_squared_error(regr.predict(x_test), y_test) > 0.05):\n",
    "        print('predictors cannot predict item in df1')\n",
    "        assert(False)\n",
    "    \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "222f7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns predicted item in df2\n",
    "def predict_item(df2, predictors_df2, model):\n",
    "  \n",
    "    # item_predicted = B0 + B1X1 + ... + BnXn\n",
    "    X = df2[predictors_df2] \n",
    "    \n",
    "    return np.dot(X, model.coef_) + model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bbc7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the best-fitting proxy out of candidates variables for predicted value of item in df1 using predictors_df1 variables\n",
    "# to find approximation for item in df2 using predictors_df2 variables. Approximation in df2 is purged of influence of \n",
    "# orthogonal_vars, if specified. If candidates not specified, consider all columns with numerical data to be candidates.\n",
    "def proxy_finder(df1, df2, item, predictors_df1, predictors_df2, num_proxies=1, candidates=None, orthogonal_vars=None):\n",
    "    #test size for linear regression training\n",
    "    test_size = 0.2\n",
    "    \n",
    "    if (candidates == None):\n",
    "        candidates = list(df2.select_dtypes(include='number').columns)\n",
    "    \n",
    "    # validate parameters and construct df2 prediction for item\n",
    "    proxy_finder_validate(item, candidates, df1, df2)\n",
    "    df1 = data_rescale(df1) # ensure each df is scaled between 0,1\n",
    "    df2 = data_rescale(df2)\n",
    "    regr = get_model(predictors_df1, item, test_size, df1)\n",
    "    item_pred = predict_item(df2, predictors_df2, regr)\n",
    "    \n",
    "    df2['item_pred'] = item_pred\n",
    "\n",
    "    # perform regression analysis for each candidate proxy\n",
    "    results = {}\n",
    "    \n",
    "    for c in candidates:\n",
    "        \n",
    "        # drop rows from item_pred and df2[c]\n",
    "        candset = df2[[c, 'item_pred']].copy()\n",
    "        \n",
    "        candset = candset.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        item_pred_drop = candset['item_pred']\n",
    "        candcol = candset[c]\n",
    "        \n",
    "        X = sm.add_constant(candcol)\n",
    "                            \n",
    "        model = sm.OLS(item_pred_drop, X).fit()\n",
    "        results[c] = {\n",
    "            'R_squared': model.rsquared,\n",
    "            'p_value': model.pvalues[1],  \n",
    "            'coef': model.params[1]\n",
    "        } \n",
    "  \n",
    "    # Select the proxy with the highest R-squared and significant p-value\n",
    "    # Sort the results by R_squared (descending) and p_value (ascending)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
    "    \n",
    "    best_proxies = []\n",
    "    \n",
    "    # add & print the top number_proxies\n",
    "    for i in range(min(num_proxies, len(sorted_results))):\n",
    "        proxy, metrics = sorted_results[i]\n",
    "        best_proxies.append(proxy)\n",
    "        print(f\"Proxy {i+1} for {item}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
    "    \n",
    "    return best_proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9166e2",
   "metadata": {},
   "source": [
    "### TOY RUN CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596b7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'p_value': model.pvalues[1],\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'coef': model.params[1]\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'p_value': model.pvalues[1],\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'coef': model.params[1]\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'p_value': model.pvalues[1],\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'coef': model.params[1]\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'p_value': model.pvalues[1],\n",
      "C:\\Users\\kirin\\AppData\\Local\\Temp\\ipykernel_7560\\3328101821.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'coef': model.params[1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_stata(datafile_proxy)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# find and print suggested proxy\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m best_proxy \u001b[38;5;241m=\u001b[39m proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)\n",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m, in \u001b[0;36mproxy_finder\u001b[1;34m(df1, df2, item, predictors_df1, predictors_df2, num_proxies, candidates, orthogonal_vars)\u001b[0m\n\u001b[0;32m     30\u001b[0m     candcol \u001b[38;5;241m=\u001b[39m candset[c]\n\u001b[0;32m     32\u001b[0m     X \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(candcol)\n\u001b[1;32m---> 34\u001b[0m     model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(item_pred_drop, X)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m     35\u001b[0m     results[c] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR_squared\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mrsquared,\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mpvalues[\u001b[38;5;241m1\u001b[39m],  \n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoef\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     39\u001b[0m     } \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Select the proxy with the highest R-squared and significant p-value\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Sort the results by R_squared (descending) and p_value (ascending)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:924\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    923\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    925\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:749\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    750\u001b[0m                           weights\u001b[38;5;241m=\u001b[39mweights, hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    751\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    752\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:203\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[0;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\data.py:88\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_constant(hasconst)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\data.py:132\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# detect where the constant is\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     check_implicit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     exog_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(exog_max)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDataError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexog contains inf or nans\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2820\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2705\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2707\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2708\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mmaximum, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[0;32m   2821\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32mc:\\Users\\kirin\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# use this case to make sure the algorithm works\n",
    "\n",
    "# specific item we'd like to make a proxy for\n",
    "item = 'status_threat' \n",
    "\n",
    "# specific variables we use to predict the item in first dataframe\n",
    "predictors_df1 = [\n",
    "                   'psc1_W1_01',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'age501',\n",
    "                   'education'] #CN AUTH RR SDO PID IDE + EDU AGE GEND\n",
    "\n",
    "# specific variables we use to predict the item in second dataframe. \n",
    "# These should correspond to the itemsin predictors_df1.\n",
    "predictors_df2 = [\n",
    "                   'psc1_W1_01',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'age501',\n",
    "                   'education'\n",
    "                   ] \n",
    "\n",
    "# potential proxies\n",
    "candidates = ['christian_top',\n",
    "'age501',\n",
    "'education',\n",
    "'ideology',\n",
    "'christian_nationalism',\n",
    "'white_top',\n",
    "'status_threat',\n",
    "'SDO11',\n",
    "'social_dom11',\n",
    "'race_resent',\n",
    "'authoritarianism',\n",
    "'trumpfav'\n",
    "              ] \n",
    "\n",
    "# .dta file with item measure\n",
    "datafile_item = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "# .dta file we want to find a proxy in\n",
    "datafile_proxy = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "\n",
    "df1 = pd.read_stata(datafile_item)\n",
    "df2 = pd.read_stata(datafile_proxy)\n",
    "\n",
    "# find and print suggested proxy\n",
    "best_proxy = proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c3518",
   "metadata": {},
   "source": [
    "This is a nice sanity check. If we look for a proxy for status threat in the original status threat dataset, the best fitting proxy is status threat itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a63ae",
   "metadata": {},
   "source": [
    "### Status threat, GSS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific item we'd like to make a proxy for\n",
    "item = 'status_threat' \n",
    "\n",
    "# specific variables we use to predict the item in first dataframe\n",
    "predictors_df1 = [\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education'] \n",
    "\n",
    "# specific variables we use to predict the item in second dataframe. \n",
    "# These should correspond to the itemsin predictors_df1.\n",
    "predictors_df2 = [\n",
    "                  \n",
    "                   ] \n",
    "\n",
    "# variables we'd like to remove the influence of on predicted item\n",
    "orthogonal_vars = ['christian_nationalism', \n",
    "                   'authoritarianism', \n",
    "                   'social_dom11', \n",
    "                   'race_resent', \n",
    "                   'party_ID', \n",
    "                   'ideology']\n",
    "\n",
    "# potential proxies\n",
    "candidates = ['spocc10',\n",
    "              'sppres10', \n",
    "              'sppres80',\n",
    "              'spind10',\n",
    "              'prestg10',\n",
    "              'occ10',\n",
    "              'wrkstat',\n",
    "              'divorce',\n",
    "              'paocc10',\n",
    "              'papres10',\n",
    "              'maocc10',\n",
    "              'mapres10',\n",
    "              'paind10',\n",
    "              'maind10',\n",
    "              ] \n",
    "\n",
    "# .dta file with item measure\n",
    "datafile_item = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "# .dta file we want to find a proxy in\n",
    "datafile_proxy = r'C:\\Users\\kirin\\Downloads\\GSS2022.dta'\n",
    "\n",
    "\n",
    "df1 = pd.read_stata(datafile_st)\n",
    "df2 = pd.read_stata(datafile_proxy, convert_categoricals=False)\n",
    "\n",
    "# find and print suggested proxy\n",
    "proxy_finder(df1, df2, item, predictors_df1, predictors_df2, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e667502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print all column names to verify\n",
    "print(\"Column names in the dataset:\")\n",
    "column_names = df2.columns.tolist()\n",
    "print(\"\\n\".join(column_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8961ee8",
   "metadata": {},
   "source": [
    "### ANES 2020 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1988ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, we'll have to make approximations for predictors that are not explicitly measured in the second (ANES) \n",
    "#dataset. \n",
    "# .dta file we want to find a proxy in\n",
    "filepath_proxy = r'C:\\Users\\kirin\\Downloads\\anes2020\\anes_timeseries_2020_stata_20220210.dta'\n",
    "df2 = pd.read_stata(filepath_proxy, convert_categoricals=False)\n",
    "\n",
    "df2['psc1_W1_01'] = df2[['V202311', 'V202312', 'V202304']].mean(axis=1)\n",
    "\n",
    "df2['christian_nationalism'] = df2['V202169']\n",
    "\n",
    "df2['authoritarianism'] = df2[['V202163', 'V202302', 'V202158', 'V202170', 'V202159']].mean(axis=1)\n",
    "\n",
    "df2['social_dom11'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "df2['race_resent'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "df2['race_resent'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "df2['race_resent'] = df2[['column1', 'column2', 'column3']].mean(axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ef8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific item we'd like to make a proxy for\n",
    "item = 'status_threat' \n",
    "\n",
    "# specific variables we use to predict the item in first dataframe\n",
    "predictors_df1 = [\n",
    "                   'psc1_W1_01',\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education'] \n",
    "\n",
    "\n",
    "# specific variables we use to predict the item in second dataframe. \n",
    "# These should correspond to the itemsin predictors_df1.\n",
    "predictors_df2 = [\n",
    "                  'V202312', #psc item\n",
    "                  'V202169', # rate christians\n",
    "                  'V201507x' #age\n",
    "                   ] \n",
    "\n",
    "# variables we'd like to remove the influence of on predicted item\n",
    "#orthogonal_vars = []\n",
    "\n",
    "# potential proxies\n",
    "#candidates = [\n",
    " #             ] \n",
    "\n",
    "# .dta file with item measure\n",
    "filepath_item = r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_stata(filepath_item)\n",
    "\n",
    "\n",
    "# find and print suggested proxy\n",
    "proxy_finder(df1, df2, item, predictors_df1, predictors_df2, num_proxies=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01828651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional check for predictive power of other measures\n",
    "\n",
    "check_results = {}\n",
    "\n",
    "for var in orthogonal_vars:\n",
    "    model_check = sm.OLS(df2[best_proxy], sm.add_constant(df2[var])).fit()\n",
    "    check_results[var] = {\n",
    "        'R_squared': model_check.rsquared,\n",
    "        'p_value': model_check.pvalues[1]  # p-value for the orthogonal variable\n",
    "    }\n",
    "\n",
    "print(\"Predictive power of other measures on the selected proxy:\")\n",
    "print(pd.DataFrame(check_results).transpose())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
