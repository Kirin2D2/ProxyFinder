{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bJEjQ3My0jmc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sys\n",
        "from gain import GAIN\n",
        "from usage_example import *\n",
        "import utils\n",
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
        "\n",
        "    # validate proxies and st item\n",
        "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
        "\n",
        "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
        "\n",
        "    for c in predictors:\n",
        "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
        "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset\n",
        "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'\n",
        "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'\n",
        "\n",
        "    for c in candidates:\n",
        "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
        "\n",
        "    if (orthogonal_vars != None):\n",
        "        for c in orthogonal_vars:\n",
        "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'\n"
      ],
      "metadata": {
        "id": "5n6s_D5a0nBt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return a new df that is a copy of df, with: rescale all columns to be\n",
        "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
        "# rows that are missing at least one predictor.\n",
        "def data_rescale(df, predictors, target, drop=True):\n",
        "    df = df.copy() # preserve immutability\n",
        "\n",
        "    # Select only the numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    if drop:\n",
        "      # drop any rows that are missing at least one predictor\n",
        "      df = df.dropna(subset=predictors)\n",
        "\n",
        "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
        "    # Initialize the scaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
        "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
        "\n",
        "    return scaled_df"
      ],
      "metadata": {
        "id": "vjpKOYh97kzT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(df_train, df_test, predictors, target, epochs=50, learning_rate=0.001, l2_lambda=0.001):\n",
        "  # CODE IMPLEMENTATION ASSISTED BY GENERATIVE AI\n",
        "\n",
        "  # Set parameters\n",
        "  SEED = 13\n",
        "  DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  TRAIN_SIZE = 1.0  # Using all of df1 for training\n",
        "  np.random.seed(SEED)\n",
        "\n",
        "  df1 = df_train.copy()\n",
        "  df2 = df_test.copy()\n",
        "\n",
        "  # drop everything but predictors and target from df1\n",
        "  target_col_df1 = df1[target]\n",
        "  df1 = df1[predictors]\n",
        "  df1[target] = target_col_df1\n",
        "\n",
        "  # drop everything but predictors from df2\n",
        "  df2 = df2[predictors]\n",
        "  # add missing target\n",
        "  df2[target] = np.nan\n",
        "\n",
        "  combined_df = pd.concat([df1, df2])\n",
        "\n",
        "  # Step 3: Normalize the data\n",
        "  scaler = MinMaxScaler()\n",
        "  combined_data_std = scaler.fit_transform(combined_df)\n",
        "\n",
        "  # Split back into df1 (training) and df2 (prediction)\n",
        "  df1_std = combined_data_std[:len(df1)]\n",
        "  df2_std = combined_data_std[len(df1):]\n",
        "\n",
        "  # Create tensors and masks\n",
        "  X_train_tensor = torch.tensor(df1_std).float()\n",
        "  M_train_tensor = get_mask(X_train_tensor)  # This creates mask with 0s for observed values, 1s for missing values\n",
        "  train_dataset = TensorDataset(X_train_tensor, M_train_tensor)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "  X_test_tensor = torch.tensor(df2_std).float()\n",
        "  M_test_tensor = get_mask(X_test_tensor)  # This will mark all values in the target column as missing\n",
        "  test_dataset = TensorDataset(X_test_tensor, M_test_tensor)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "  # Step 4: Initialize and train the GAIN model\n",
        "  stopper = EarlyStopper(patience=2, min_delta=0.001)\n",
        "  model = GAIN(train_loader=train_loader, seed=SEED)\n",
        "\n",
        "  optimizer_G = torch.optim.Adam(model.G.parameters())\n",
        "  optimizer_D = torch.optim.Adam(model.D.parameters())\n",
        "  model.set_optimizer(optimizer=optimizer_G, generator=True)\n",
        "  model.set_optimizer(optimizer=optimizer_D, generator=False)\n",
        "\n",
        "  model.to(DEVICE)\n",
        "  model.train(n_epoches=100, verbose=True, stopper=stopper)\n",
        "\n",
        "  # Step 5: Use the trained model to predict (impute) target values for df2\n",
        "  predictions = []\n",
        "\n",
        "  for x_test_batch, m_batch in test_loader:\n",
        "      x_batch_imputed = model.imputation(x=x_test_batch, m=m_batch)\n",
        "      x_batch_imputed = x_batch_imputed.cpu().numpy()\n",
        "      predictions.append(x_batch_imputed)\n",
        "\n",
        "  # Combine predictions and inverse transform\n",
        "  predictions_combined = np.vstack(predictions)\n",
        "  predictions_original_scale = scaler.inverse_transform(predictions_combined)\n",
        "\n",
        "  # Extract the target column predictions\n",
        "  target_column_index = df1.columns.get_loc(target)\n",
        "  df2_predictions = predictions_original_scale[:, target_column_index]\n",
        "\n",
        "  return df2_predictions"
      ],
      "metadata": {
        "id": "xfyW95AtCBG4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # orthogonalization method\n",
        "# all data is preprocessed and df test has been appended target preds\n",
        "def orthogonalize(candidates, df_test, orthogonal_vars):\n",
        "        orth_scores = {}\n",
        "        for c in candidates:\n",
        "            candset = df_test[[c, 'predicted_target']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
        "            candcol = candset[c]\n",
        "\n",
        "            X = sm.add_constant(candcol)\n",
        "            temp_orth_scores = []\n",
        "            for orth_var in orthogonal_vars:\n",
        "                orthset = df_test[[orth_var]].copy().dropna()\n",
        "                common_indices = candset.index.intersection(orthset.index)\n",
        "                if common_indices.empty:\n",
        "                    continue\n",
        "                orth_col = orthset.loc[common_indices, orth_var]\n",
        "                if np.var(orth_col) == 0:\n",
        "                    print(\"ortho:\", orth_var, \"candidate\", c)\n",
        "                    continue # zero variance leads to divide by zero error\n",
        "                candcol_common = candset.loc[common_indices, c]\n",
        "\n",
        "                X_common = sm.add_constant(candcol_common)\n",
        "                model = sm.OLS(orth_col, X_common).fit()\n",
        "                temp_orth_scores.append(model.rsquared)\n",
        "\n",
        "            if temp_orth_scores:\n",
        "                orth_scores[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
        "            else:\n",
        "                orth_scores[c] = 0\n",
        "        return orth_scores"
      ],
      "metadata": {
        "id": "lQz-jpKg0yR_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, neural_net=\"original\", drop=True):\n",
        "    if candidates is None:\n",
        "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
        "\n",
        "\n",
        "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
        "\n",
        "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
        "    #print(f\"Candidates: {candidates}\")\n",
        "\n",
        "    df_train = data_rescale(df_train, predictors, target, drop)\n",
        "    df_test = data_rescale(df_test, predictors, target, drop)\n",
        "    # drop any rows that are missing data from target\n",
        "    df_train = df_train.dropna(subset=target)\n",
        "\n",
        "    if neural_net == \"torch\":\n",
        "      predicted_scores = get_predictionsTorch(df_train, df_test, predictors, target)\n",
        "    elif neural_net == \"tiered\":\n",
        "      predicted_scores = get_predictionsTiered(df_train, df_test, predictors, target)\n",
        "    else:\n",
        "      predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
        "\n",
        "\n",
        "    df_test['predicted_target'] = predicted_scores\n",
        "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c in candidates:\n",
        "        candset = df_test[[c, 'predicted_target']].copy().dropna()\n",
        "        if candset.empty:\n",
        "            continue\n",
        "\n",
        "        pred_scores = candset['predicted_target']\n",
        "        candcol = candset[c]\n",
        "\n",
        "        X_pred = sm.add_constant(candcol)\n",
        "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
        "        results[c] = {\n",
        "            'R_squared': model_pred.rsquared,\n",
        "            'p_value': model_pred.pvalues[1],\n",
        "            'coef': model_pred.params[1]\n",
        "        }\n",
        "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------\n",
        "\n",
        "    best_proxies = []\n",
        "\n",
        "    if orthogonal_vars:\n",
        "        orth_scores = orthogonalize(candidates, df_test, orthogonal_vars)\n",
        "        proxy_scores = {}\n",
        "        for c in candidates:\n",
        "            try:\n",
        "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_scores[c])\n",
        "            except KeyError as e:\n",
        "                continue\n",
        "\n",
        "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, score = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
        "    else:\n",
        "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, metrics = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
        "\n",
        "    return best_proxies"
      ],
      "metadata": {
        "id": "2FbR6O-v01eO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\")\n",
        "\n",
        "\n",
        "# Suppress numpy invalid operation warnings\n",
        "np.seterr(invalid='ignore')\n",
        "\n",
        "datafile_train =  \"/content/yougov_recoded.dta\"\n",
        "datafile_test =  \"/content/yougov_recoded.dta\"\n",
        "df_train = pd.read_stata(datafile_train)\n",
        "df_test = pd.read_stata(datafile_test)"
      ],
      "metadata": {
        "id": "hxex2qH203ig"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target = 'christian_nationalism'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new'\n",
        "                   ]\n",
        "\n",
        "orthogonal_vars = [\n",
        "                  'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_proxies = proxy_finder(df_train, df_test, target, predictors, orth_weight=0.60, orthogonal_vars=orthogonal_vars, num_proxies=8, neural_net=\"original\", drop=False)\n",
        "#print(best_proxies)\n",
        "### orth weight 0.9 --> version, how many interviews, etx\n",
        "### 0.85 same thing\n",
        "### 0.8\n",
        "### 0.75 bad\n",
        "### 0.7 bad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6VIfU_W06BD",
        "outputId": "af85603c-5f8d-4ba3-ff36-227012f2fa54"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 27/27 [00:00<00:00, 72.83batch/s, mse_test=nan, mse_train=0.12]\n",
            "Epoch 1: 100%|██████████| 27/27 [00:00<00:00, 75.68batch/s, mse_test=nan, mse_train=0.107]\n",
            "Epoch 2: 100%|██████████| 27/27 [00:00<00:00, 84.06batch/s, mse_test=nan, mse_train=0.0951]\n",
            "Epoch 3: 100%|██████████| 27/27 [00:00<00:00, 102.74batch/s, mse_test=nan, mse_train=0.0802]\n",
            "Epoch 4: 100%|██████████| 27/27 [00:00<00:00, 87.67batch/s, mse_test=nan, mse_train=0.0639]\n",
            "Epoch 5: 100%|██████████| 27/27 [00:00<00:00, 80.27batch/s, mse_test=nan, mse_train=0.0554]\n",
            "Epoch 6: 100%|██████████| 27/27 [00:00<00:00, 88.63batch/s, mse_test=nan, mse_train=0.0533]\n",
            "Epoch 7: 100%|██████████| 27/27 [00:00<00:00, 91.50batch/s, mse_test=nan, mse_train=0.0524]\n",
            "Epoch 8: 100%|██████████| 27/27 [00:00<00:00, 87.58batch/s, mse_test=nan, mse_train=0.0517]\n",
            "Epoch 9: 100%|██████████| 27/27 [00:00<00:00, 101.83batch/s, mse_test=nan, mse_train=0.0509]\n",
            "Epoch 10: 100%|██████████| 27/27 [00:00<00:00, 97.62batch/s, mse_test=nan, mse_train=0.05] \n",
            "Epoch 11: 100%|██████████| 27/27 [00:00<00:00, 83.73batch/s, mse_test=nan, mse_train=0.0488]\n",
            "Epoch 12: 100%|██████████| 27/27 [00:00<00:00, 79.27batch/s, mse_test=nan, mse_train=0.0472]\n",
            "Epoch 13: 100%|██████████| 27/27 [00:00<00:00, 80.48batch/s, mse_test=nan, mse_train=0.0451]\n",
            "Epoch 14: 100%|██████████| 27/27 [00:00<00:00, 89.79batch/s, mse_test=nan, mse_train=0.0428]\n",
            "Epoch 15: 100%|██████████| 27/27 [00:00<00:00, 98.92batch/s, mse_test=nan, mse_train=0.0405]\n",
            "Epoch 16: 100%|██████████| 27/27 [00:00<00:00, 102.15batch/s, mse_test=nan, mse_train=0.0384]\n",
            "Epoch 17: 100%|██████████| 27/27 [00:00<00:00, 89.60batch/s, mse_test=nan, mse_train=0.0368]\n",
            "Epoch 18: 100%|██████████| 27/27 [00:00<00:00, 82.20batch/s, mse_test=nan, mse_train=0.0356]\n",
            "Epoch 19: 100%|██████████| 27/27 [00:00<00:00, 79.94batch/s, mse_test=nan, mse_train=0.0346]\n",
            "Epoch 20: 100%|██████████| 27/27 [00:00<00:00, 81.90batch/s, mse_test=nan, mse_train=0.0338]\n",
            "Epoch 21: 100%|██████████| 27/27 [00:00<00:00, 87.20batch/s, mse_test=nan, mse_train=0.0331]\n",
            "Epoch 22: 100%|██████████| 27/27 [00:00<00:00, 95.22batch/s, mse_test=nan, mse_train=0.0325]\n",
            "Epoch 23: 100%|██████████| 27/27 [00:00<00:00, 91.28batch/s, mse_test=nan, mse_train=0.0319]\n",
            "Epoch 24: 100%|██████████| 27/27 [00:00<00:00, 80.31batch/s, mse_test=nan, mse_train=0.0314]\n",
            "Epoch 25: 100%|██████████| 27/27 [00:00<00:00, 81.10batch/s, mse_test=nan, mse_train=0.0309]\n",
            "Epoch 26: 100%|██████████| 27/27 [00:00<00:00, 80.06batch/s, mse_test=nan, mse_train=0.0304]\n",
            "Epoch 27: 100%|██████████| 27/27 [00:00<00:00, 100.69batch/s, mse_test=nan, mse_train=0.0299]\n",
            "Epoch 28: 100%|██████████| 27/27 [00:00<00:00, 98.29batch/s, mse_test=nan, mse_train=0.0294]\n",
            "Epoch 29: 100%|██████████| 27/27 [00:00<00:00, 97.10batch/s, mse_test=nan, mse_train=0.0289]\n",
            "Epoch 30: 100%|██████████| 27/27 [00:00<00:00, 80.65batch/s, mse_test=nan, mse_train=0.0284]\n",
            "Epoch 31: 100%|██████████| 27/27 [00:00<00:00, 76.80batch/s, mse_test=nan, mse_train=0.0279]\n",
            "Epoch 32: 100%|██████████| 27/27 [00:00<00:00, 75.22batch/s, mse_test=nan, mse_train=0.0275]\n",
            "Epoch 33: 100%|██████████| 27/27 [00:00<00:00, 64.54batch/s, mse_test=nan, mse_train=0.027]\n",
            "Epoch 34: 100%|██████████| 27/27 [00:00<00:00, 69.26batch/s, mse_test=nan, mse_train=0.0265]\n",
            "Epoch 35: 100%|██████████| 27/27 [00:00<00:00, 67.34batch/s, mse_test=nan, mse_train=0.0261]\n",
            "Epoch 36: 100%|██████████| 27/27 [00:00<00:00, 60.43batch/s, mse_test=nan, mse_train=0.0256]\n",
            "Epoch 37: 100%|██████████| 27/27 [00:00<00:00, 81.49batch/s, mse_test=nan, mse_train=0.0252]\n",
            "Epoch 38: 100%|██████████| 27/27 [00:00<00:00, 62.30batch/s, mse_test=nan, mse_train=0.0247]\n",
            "Epoch 39: 100%|██████████| 27/27 [00:00<00:00, 79.41batch/s, mse_test=nan, mse_train=0.0242]\n",
            "Epoch 40: 100%|██████████| 27/27 [00:00<00:00, 70.16batch/s, mse_test=nan, mse_train=0.0238]\n",
            "Epoch 41: 100%|██████████| 27/27 [00:00<00:00, 61.51batch/s, mse_test=nan, mse_train=0.0233]\n",
            "Epoch 42: 100%|██████████| 27/27 [00:00<00:00, 66.02batch/s, mse_test=nan, mse_train=0.0228]\n",
            "Epoch 43: 100%|██████████| 27/27 [00:00<00:00, 64.87batch/s, mse_test=nan, mse_train=0.0223]\n",
            "Epoch 44: 100%|██████████| 27/27 [00:00<00:00, 76.74batch/s, mse_test=nan, mse_train=0.0219]\n",
            "Epoch 45: 100%|██████████| 27/27 [00:00<00:00, 63.72batch/s, mse_test=nan, mse_train=0.0214]\n",
            "Epoch 46: 100%|██████████| 27/27 [00:00<00:00, 78.22batch/s, mse_test=nan, mse_train=0.0209]\n",
            "Epoch 47: 100%|██████████| 27/27 [00:00<00:00, 81.68batch/s, mse_test=nan, mse_train=0.0204]\n",
            "Epoch 48: 100%|██████████| 27/27 [00:00<00:00, 83.85batch/s, mse_test=nan, mse_train=0.02]\n",
            "Epoch 49: 100%|██████████| 27/27 [00:00<00:00, 89.70batch/s, mse_test=nan, mse_train=0.0197]\n",
            "Epoch 50: 100%|██████████| 27/27 [00:00<00:00, 97.58batch/s, mse_test=nan, mse_train=0.0193]\n",
            "Epoch 51: 100%|██████████| 27/27 [00:00<00:00, 84.55batch/s, mse_test=nan, mse_train=0.019]\n",
            "Epoch 52: 100%|██████████| 27/27 [00:00<00:00, 72.66batch/s, mse_test=nan, mse_train=0.0188]\n",
            "Epoch 53: 100%|██████████| 27/27 [00:00<00:00, 77.05batch/s, mse_test=nan, mse_train=0.0185]\n",
            "Epoch 54: 100%|██████████| 27/27 [00:00<00:00, 75.63batch/s, mse_test=nan, mse_train=0.0183]\n",
            "Epoch 55: 100%|██████████| 27/27 [00:00<00:00, 70.42batch/s, mse_test=nan, mse_train=0.0181]\n",
            "Epoch 56: 100%|██████████| 27/27 [00:00<00:00, 95.42batch/s, mse_test=nan, mse_train=0.0179]\n",
            "Epoch 57: 100%|██████████| 27/27 [00:00<00:00, 78.46batch/s, mse_test=nan, mse_train=0.0178]\n",
            "Epoch 58: 100%|██████████| 27/27 [00:00<00:00, 78.25batch/s, mse_test=nan, mse_train=0.0176]\n",
            "Epoch 59: 100%|██████████| 27/27 [00:00<00:00, 96.75batch/s, mse_test=nan, mse_train=0.0175]\n",
            "Epoch 60: 100%|██████████| 27/27 [00:00<00:00, 77.88batch/s, mse_test=nan, mse_train=0.0173]\n",
            "Epoch 61: 100%|██████████| 27/27 [00:00<00:00, 69.90batch/s, mse_test=nan, mse_train=0.0171]\n",
            "Epoch 62: 100%|██████████| 27/27 [00:00<00:00, 77.24batch/s, mse_test=nan, mse_train=0.017]\n",
            "Epoch 63: 100%|██████████| 27/27 [00:00<00:00, 83.34batch/s, mse_test=nan, mse_train=0.0168]\n",
            "Epoch 64: 100%|██████████| 27/27 [00:00<00:00, 86.94batch/s, mse_test=nan, mse_train=0.0167]\n",
            "Epoch 65: 100%|██████████| 27/27 [00:00<00:00, 95.41batch/s, mse_test=nan, mse_train=0.0165]\n",
            "Epoch 66: 100%|██████████| 27/27 [00:00<00:00, 81.81batch/s, mse_test=nan, mse_train=0.0163]\n",
            "Epoch 67: 100%|██████████| 27/27 [00:00<00:00, 79.27batch/s, mse_test=nan, mse_train=0.0162]\n",
            "Epoch 68: 100%|██████████| 27/27 [00:00<00:00, 70.74batch/s, mse_test=nan, mse_train=0.016]\n",
            "Epoch 69: 100%|██████████| 27/27 [00:00<00:00, 73.06batch/s, mse_test=nan, mse_train=0.0158]\n",
            "Epoch 70: 100%|██████████| 27/27 [00:00<00:00, 89.71batch/s, mse_test=nan, mse_train=0.0156]\n",
            "Epoch 71: 100%|██████████| 27/27 [00:00<00:00, 87.72batch/s, mse_test=nan, mse_train=0.0154]\n",
            "Epoch 72: 100%|██████████| 27/27 [00:00<00:00, 72.16batch/s, mse_test=nan, mse_train=0.0153]\n",
            "Epoch 73: 100%|██████████| 27/27 [00:00<00:00, 72.00batch/s, mse_test=nan, mse_train=0.0151]\n",
            "Epoch 74: 100%|██████████| 27/27 [00:00<00:00, 73.34batch/s, mse_test=nan, mse_train=0.0149]\n",
            "Epoch 75: 100%|██████████| 27/27 [00:00<00:00, 79.74batch/s, mse_test=nan, mse_train=0.0147]\n",
            "Epoch 76: 100%|██████████| 27/27 [00:00<00:00, 89.30batch/s, mse_test=nan, mse_train=0.0145]\n",
            "Epoch 77: 100%|██████████| 27/27 [00:00<00:00, 79.65batch/s, mse_test=nan, mse_train=0.0144]\n",
            "Epoch 78: 100%|██████████| 27/27 [00:00<00:00, 55.65batch/s, mse_test=nan, mse_train=0.0142]\n",
            "Epoch 79: 100%|██████████| 27/27 [00:00<00:00, 56.75batch/s, mse_test=nan, mse_train=0.014]\n",
            "Epoch 80: 100%|██████████| 27/27 [00:00<00:00, 82.76batch/s, mse_test=nan, mse_train=0.0139]\n",
            "Epoch 81: 100%|██████████| 27/27 [00:00<00:00, 88.91batch/s, mse_test=nan, mse_train=0.0137]\n",
            "Epoch 82: 100%|██████████| 27/27 [00:00<00:00, 93.79batch/s, mse_test=nan, mse_train=0.0136]\n",
            "Epoch 83: 100%|██████████| 27/27 [00:00<00:00, 63.98batch/s, mse_test=nan, mse_train=0.0134]\n",
            "Epoch 84: 100%|██████████| 27/27 [00:00<00:00, 63.31batch/s, mse_test=nan, mse_train=0.0133]\n",
            "Epoch 85: 100%|██████████| 27/27 [00:00<00:00, 60.44batch/s, mse_test=nan, mse_train=0.0131]\n",
            "Epoch 86: 100%|██████████| 27/27 [00:00<00:00, 58.10batch/s, mse_test=nan, mse_train=0.013]\n",
            "Epoch 87: 100%|██████████| 27/27 [00:00<00:00, 62.30batch/s, mse_test=nan, mse_train=0.0129]\n",
            "Epoch 88: 100%|██████████| 27/27 [00:00<00:00, 71.43batch/s, mse_test=nan, mse_train=0.0127]\n",
            "Epoch 89: 100%|██████████| 27/27 [00:00<00:00, 76.40batch/s, mse_test=nan, mse_train=0.0126]\n",
            "Epoch 90: 100%|██████████| 27/27 [00:00<00:00, 71.47batch/s, mse_test=nan, mse_train=0.0125]\n",
            "Epoch 91: 100%|██████████| 27/27 [00:00<00:00, 70.76batch/s, mse_test=nan, mse_train=0.0123]\n",
            "Epoch 92: 100%|██████████| 27/27 [00:00<00:00, 67.24batch/s, mse_test=nan, mse_train=0.0122]\n",
            "Epoch 93: 100%|██████████| 27/27 [00:00<00:00, 83.01batch/s, mse_test=nan, mse_train=0.0121]\n",
            "Epoch 94: 100%|██████████| 27/27 [00:00<00:00, 90.23batch/s, mse_test=nan, mse_train=0.012]\n",
            "Epoch 95: 100%|██████████| 27/27 [00:00<00:00, 92.83batch/s, mse_test=nan, mse_train=0.0119]\n",
            "Epoch 96: 100%|██████████| 27/27 [00:00<00:00, 68.55batch/s, mse_test=nan, mse_train=0.0118]\n",
            "Epoch 97: 100%|██████████| 27/27 [00:00<00:00, 61.68batch/s, mse_test=nan, mse_train=0.0117]\n",
            "Epoch 98: 100%|██████████| 27/27 [00:00<00:00, 77.84batch/s, mse_test=nan, mse_train=0.0116]\n",
            "Epoch 99: 100%|██████████| 27/27 [00:00<00:00, 78.63batch/s, mse_test=nan, mse_train=0.0115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy 1 for christian_nationalism: christian_nationalism with score: 0.10095037445472552\n",
            "Proxy 2 for christian_nationalism: immigrant_deport with score: 0.08747972244070154\n",
            "Proxy 3 for christian_nationalism: immigrant_citizenship with score: 0.08663656722326765\n",
            "Proxy 4 for christian_nationalism: auth_grid_3 with score: 0.06254779846685989\n",
            "Proxy 5 for christian_nationalism: ideo7 with score: 0.05851350719253787\n",
            "Proxy 6 for christian_nationalism: pff_dt with score: 0.057801202628490894\n",
            "Proxy 7 for christian_nationalism: auth_grid_1 with score: 0.03181674005661009\n",
            "Proxy 8 for christian_nationalism: presvote20post with score: 0.025884471213003912\n"
          ]
        }
      ]
    }
  ]
}