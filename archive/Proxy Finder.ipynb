{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92922b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "224f669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
    "\n",
    "    # validate proxies and st item\n",
    "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
    "\n",
    "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
    "    \n",
    "    for c in predictors:\n",
    "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
    "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset  \n",
    "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'   \n",
    "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'    \n",
    "    \n",
    "    for c in candidates:\n",
    "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
    "        \n",
    "    if (orthogonal_vars != None):\n",
    "        for c in orthogonal_vars:\n",
    "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a41179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a new df that is a copy of df, with: rescale all columns to be\n",
    "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
    "# rows that are missing at least one predictor.\n",
    "def data_rescale(df, predictors, target, drop=True):\n",
    "    df = df.copy() # preserve immutability\n",
    "\n",
    "    # Select only the numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    if drop:\n",
    "      # drop any rows that are missing at least one predictor\n",
    "      df = df.dropna(subset=predictors)\n",
    "     \n",
    "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
    "    # Initialize the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
    "\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "222f7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET PREDICTIONS\n",
    "\n",
    "# Neural network definition\n",
    "def build_nn_model(input_dim, learning_rate=0.001, l2_lambda=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),  \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),  \n",
    "        Dense(1, kernel_regularizer=l2(l2_lambda))\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# return a trained neural network to predict df[item] using df[predictors_df1]\n",
    "# report error and crash if predictors don't predict item\n",
    "def train_nn_model(X_train, y_train, input_dim, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    model = build_nn_model(input_dim, learning_rate, l2_lambda)\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
    "    return model\n",
    "\n",
    "# get predictions from the neural network. Takes in\n",
    "def get_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    \n",
    "    # split data for training and testing. \n",
    "    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
    "    X_test = df_test[predictors].to_numpy()\n",
    "\n",
    "    # train network and get predictions\n",
    "    model = train_nn_model(X_train_train, y_train_train, len(predictors), epochs, learning_rate, l2_lambda)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # exit if correlation between predictions and item is bad\n",
    "    mse = mean_squared_error(model.predict(X_train_test), y_train_test)\n",
    "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
    "    if (mse > 0.03):\n",
    "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
    "        print('Aborting program')\n",
    "        sys.exit(-1)\n",
    "\n",
    "   # print(f\"Predictions before flattening: {predictions[:10]}\") #DEBUG\n",
    "   # print('predictions after flattening: ', predictions.flatten()[:10])#DEBUG\n",
    "\n",
    "    return predictions.flatten()\n",
    "\n",
    "# get predictions from the Torch neural network\n",
    "def get_predictionsTorch(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # split data for training and testing.\n",
    "    training_features, validation_features, training_target, validation_target = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "    training_features = torch.FloatTensor(training_features)\n",
    "    training_target = torch.FloatTensor(training_target)\n",
    "    validation_features = torch.FloatTensor(validation_features)\n",
    "    validation_target = torch.FloatTensor(validation_target)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(len(predictors), 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "    # MSE loss\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    # train the model\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        prediction = model(training_features)\n",
    "        loss = loss_func(prediction, training_target.unsqueeze(1))\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # get predictions\n",
    "    model.eval()\n",
    "    test_data = torch.FloatTensor(df_test[predictors].to_numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_data)\n",
    "        predictions = predictions.numpy().flatten()\n",
    "\n",
    "        val_predictions = model(validation_features)\n",
    "        val_predictions = val_predictions.numpy().flatten()\n",
    "\n",
    "\n",
    "    # exit if correlation between predictions and item is bad\n",
    "    mse = mean_squared_error(val_predictions, validation_target)\n",
    "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
    "    if (mse > 0.03):\n",
    "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
    "        print('Aborting program')\n",
    "        sys.exit(-1)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def nn_imputer(df_train, col):\n",
    "    # NOT DONE ---------------------------------------------------------\n",
    "    df = df_train.copy()\n",
    "    \n",
    "    vals = df[df[col].notnull()]\n",
    "    missing = df[df[col].isnull()]\n",
    "    \n",
    "    X = vals.drop(columns=[col])\n",
    "    y = vals[col]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=8)\n",
    "\n",
    "    if not missing.empty:\n",
    "        X_missing = missing.drop(columns=[col])\n",
    "        predicted_values = model.predict(X_missing).flatten()\n",
    "        df_train.loc[df_train[col].isnull(), col] = predicted_values\n",
    "\n",
    "    # NOT DONE ---------------------------------------------------------\n",
    "\n",
    "\n",
    "def get_predictionsTiered(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    # if a column is missing more than 30% of data, drop it\n",
    "    missing_percentages = df_train.isnull().mean()\n",
    "    df_train = df_train.loc[:, missing_percentages < 0.3]\n",
    "\n",
    "    # if less than 1% of data is missing, use median imputation\n",
    "    for col in df_train.columns:\n",
    "        if col not in predictors or missing_percentages[col] == 0:\n",
    "            continue\n",
    "        if missing_percentages[col] < 0.01 and missing_percentages[col] > 0:\n",
    "            df_train[col].fillna(df_train[col].median(), inplace=True)\n",
    "        else:  # use neural net to impute values\n",
    "            nn_imputer(df_train, col)\n",
    "\n",
    "\n",
    "    return get_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fbf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orthogonalization method\n",
    "# all data is preprocessed and df test has been appended target preds\n",
    "def orthogonalize(candidates, df_test, orthogonal_vars):\n",
    "        orth_scores = {}\n",
    "        for c in candidates:\n",
    "            candset = df_test[[c, 'predicted_target']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
    "            candcol = candset[c]\n",
    "\n",
    "            X = sm.add_constant(candcol)\n",
    "            temp_orth_scores = []\n",
    "            for orth_var in orthogonal_vars:\n",
    "                orthset = df_test[[orth_var]].copy().dropna()\n",
    "                common_indices = candset.index.intersection(orthset.index)\n",
    "                if common_indices.empty:\n",
    "                    continue\n",
    "                orth_col = orthset.loc[common_indices, orth_var]\n",
    "                if np.var(orth_col) == 0:\n",
    "                    print(\"ortho:\", orth_var, \"candidate\", c)\n",
    "                    continue # zero variance leads to divide by zero error\n",
    "                candcol_common = candset.loc[common_indices, c]\n",
    "\n",
    "                X_common = sm.add_constant(candcol_common)\n",
    "                model = sm.OLS(orth_col, X_common).fit()\n",
    "                temp_orth_scores.append(model.rsquared)\n",
    "\n",
    "            if temp_orth_scores:\n",
    "                orth_scores[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
    "            else:\n",
    "                orth_scores[c] = 0\n",
    "        return orth_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bbc7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, neural_net=\"original\", drop=True):\n",
    "    if candidates is None:\n",
    "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
    "\n",
    "\n",
    "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
    "\n",
    "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
    "    #print(f\"Candidates: {candidates}\")\n",
    "\n",
    "    df_train = data_rescale(df_train, predictors, target, drop)\n",
    "    df_test = data_rescale(df_test, predictors, target, drop)\n",
    "    # drop any rows that are missing data from target\n",
    "    df_train = df_train.dropna(subset=target)\n",
    "\n",
    "    if neural_net == \"torch\":\n",
    "      predicted_scores = get_predictionsTorch(df_train, df_test, predictors, target)\n",
    "    elif neural_net == \"tiered\":\n",
    "      predicted_scores = get_predictionsTiered(df_train, df_test, predictors, target)\n",
    "    else:\n",
    "      predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
    "\n",
    "    df_test['predicted_target'] = predicted_scores\n",
    "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for c in candidates:\n",
    "        candset = df_test[[c, 'predicted_target']].copy().dropna()\n",
    "        if candset.empty:\n",
    "            continue\n",
    "\n",
    "        pred_scores = candset['predicted_target']\n",
    "        candcol = candset[c]\n",
    "\n",
    "        X_pred = sm.add_constant(candcol)\n",
    "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
    "        results[c] = {\n",
    "            'R_squared': model_pred.rsquared,\n",
    "            'p_value': model_pred.pvalues[1],\n",
    "            'coef': model_pred.params[1]\n",
    "        }\n",
    "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------\n",
    "\n",
    "    best_proxies = []\n",
    "\n",
    "    if orthogonal_vars:\n",
    "        orth_scores = orthogonalize(candidates, df_test, orthogonal_vars)\n",
    "        proxy_scores = {}\n",
    "        for c in candidates:\n",
    "            try:\n",
    "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_scores[c])\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "\n",
    "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for i in range(min(num_proxies, len(sorted_results))):\n",
    "            proxy, score = sorted_results[i]\n",
    "            best_proxies.append(proxy)\n",
    "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
    "    else:\n",
    "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
    "\n",
    "        for i in range(min(num_proxies, len(sorted_results))):\n",
    "            proxy, metrics = sorted_results[i]\n",
    "            best_proxies.append(proxy)\n",
    "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
    "\n",
    "    return best_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b487cea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\")\n",
    "\n",
    "\n",
    "# Suppress numpy invalid operation warnings\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "#datafile_train =  /path/to/dftrain\n",
    "#datafile_test =  /path/to/dftest\n",
    "#df_train = pd.read_stata(datafile_train)\n",
    "#df_test = pd.read_stata(datafile_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6372b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'christian_nationalism'  # The target variable in the training set\n",
    "predictors = [ # predictors in both training and test set\n",
    "                   'presvote20post',\n",
    "                   'housevote20post',\n",
    "                   'senvote20post',\n",
    "                   'pff_jb',\n",
    "                   'pff_dt',\n",
    "                   'pid7',\n",
    "                   'election_fairnness',\n",
    "                   'educ',\n",
    "                   'white',\n",
    "                   'hispanic',\n",
    "                   'partisan_violence',\n",
    "                   'immigrant_citizenship',\n",
    "                   'immigrant_deport',\n",
    "                   'auth_grid_1',\n",
    "                   'auth_grid_3',\n",
    "                   'auth_grid_2',\n",
    "                   'faminc_new'\n",
    "                   ]\n",
    "\n",
    "orthogonal_vars = [\n",
    "                   'presvote20post',\n",
    "                   'housevote20post',\n",
    "                   'senvote20post',\n",
    "                   'pff_jb',\n",
    "                   'pff_dt',\n",
    "                   'pid7',\n",
    "                   'election_fairnness',\n",
    "                   'educ',\n",
    "                   'hispanic',\n",
    "                   'partisan_violence',\n",
    "                   'immigrant_citizenship',\n",
    "                   'immigrant_deport',\n",
    "                   'auth_grid_1',\n",
    "                   'auth_grid_3',\n",
    "                   'auth_grid_2',\n",
    "                   'faminc_new']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_proxies = proxy_finder(df_train, df_test, target, predictors, orth_weight=0.65, orthogonal_vars=orthogonal_vars, num_proxies=5, neural_net=\"torch\", drop=False)\n",
    "#print(best_proxies)\n",
    "### orth weight 0.9 --> version, how many interviews, etx\n",
    "### 0.85 same thing\n",
    "### 0.8\n",
    "### 0.75 bad\n",
    "### 0.7 bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
