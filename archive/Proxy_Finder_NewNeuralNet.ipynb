{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "92922b8c",
      "metadata": {
        "id": "92922b8c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "224f669f",
      "metadata": {
        "id": "224f669f"
      },
      "outputs": [],
      "source": [
        "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
        "\n",
        "    # validate proxies and st item\n",
        "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
        "\n",
        "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
        "\n",
        "    for c in predictors:\n",
        "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
        "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset\n",
        "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'\n",
        "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'\n",
        "\n",
        "    for c in candidates:\n",
        "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
        "\n",
        "    if (orthogonal_vars != None):\n",
        "        for c in orthogonal_vars:\n",
        "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a41179e",
      "metadata": {
        "id": "5a41179e"
      },
      "outputs": [],
      "source": [
        "# return a new df that is a copy of df, with: rescale all columns to be\n",
        "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any\n",
        "# rows that are missing at least one predictor.\n",
        "def data_rescale(df, predictors):\n",
        "    df = df.copy() # preserve immutability\n",
        "\n",
        "    # Select only the numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # drop any rows that are missing at least one predictor\n",
        "    df = df.dropna(subset=predictors)\n",
        "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
        "    # Initialize the scaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
        "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
        "\n",
        "    return scaled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "rr2IRRrqmp1W",
      "metadata": {
        "id": "rr2IRRrqmp1W"
      },
      "outputs": [],
      "source": [
        "# following class implemented with assistance of Claude AI and reviewed for accuracy\n",
        "# class MissingDataDensity:\n",
        "#   def __init__(self, n_components, input_dim):\n",
        "#     #n_components is how many distributions (typically 5) are used in the GMM\n",
        "#     self.n_components = n_components\n",
        "#     self.input_dim = input_dim\n",
        "\n",
        "#     # mixing weights, means, and standard devs for the GMM model\n",
        "#     self.pi = nn.Parameter(torch.ones(n_components) / n_components)\n",
        "#     self.mu = nn.Parameter(torch.randn(n_components, input_dim))\n",
        "#     self.log_sigma = nn.Parameter(torch.zeros(n_components, input_dim))\n",
        "\n",
        "#   def get_conditional_params(self, x, missing_matrix):\n",
        "#     # x is a tensor with shape (batch_size, input_dim)\n",
        "#     # missing_matrix is 1 where values are missing, 0 otherwise\n",
        "#     batch_size = x.shape[0]\n",
        "#     pi = self.pi.expand(batch_size, -1)\n",
        "#     mu = self.mu.expand(batch_size, -1, -1)\n",
        "#     sigma = self.log_sigma.exp().expand(batch_size, -1, -1)\n",
        "\n",
        "#     # Update GMM parameters based on observed values\n",
        "#     for i in range(batch_size):\n",
        "#         # ~ is bitwise NOT, inverts the masking array\n",
        "#         observed = ~missing_matrix[i]\n",
        "#         if observed.any():\n",
        "#             # Condition Gaussian parameters on observed values\n",
        "#             diff = x[i, observed] - mu[i, :, observed]\n",
        "#             likelihood = torch.exp(-0.5 * (diff**2 / sigma[i, :, observed]).sum(dim=1))\n",
        "#             pi[i] *= likelihood\n",
        "#             pi[i] /= pi[i].sum()\n",
        "\n",
        "#     return pi, mu, sigma\n",
        "\n",
        "# this function will calculate the ReLU activation (an expected value) over the GMM distribution\n",
        "# def generalized_relu(w, b, pi, mu, sigma):\n",
        "#     # w: weight vector\n",
        "#     # b: bias\n",
        "#     # pi: GMM mixture coefficients\n",
        "#     # mu: GMM means\n",
        "#     # sigma: GMM standard devs\n",
        "\n",
        "#     # Compute parameters of resulting 1D Gaussian after linear transformation\n",
        "#     wmu = (w.unsqueeze(1) * mu).sum(dim=2) + b\n",
        "#     wsigma = torch.sqrt((w.unsqueeze(1)**2 * sigma**2).sum(dim=2))\n",
        "\n",
        "#     # Implementation of NR function from equation (2) in the paper\n",
        "#     def nr_function(z):\n",
        "#         sqrt2 = math.sqrt(2)\n",
        "#         return (1/sqrt2/math.sqrt(math.pi) * torch.exp(-z**2/2) +\n",
        "#                 z/2 * (1 + torch.erf(z/sqrt2)))\n",
        "\n",
        "#     # Compute expected ReLU\n",
        "#     z = wmu / (wsigma + 1e-8)\n",
        "#     result = wsigma * nr_function(z)\n",
        "#     return (pi * result).sum(dim=1)\n",
        "\n",
        "# class GeneralizedLinear(nn.Module):\n",
        "#     def __init__(self, in_features, out_features):\n",
        "#         super().__init__()\n",
        "#         self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "#     def forward(self, x, missing_mask, density):\n",
        "#         # For complete data points, use standard linear layer\n",
        "#         complete_mask = ~missing_mask.any(dim=1)\n",
        "#         output = torch.zeros(x.shape[0], self.linear.out_features, device=x.device)\n",
        "\n",
        "#         if complete_mask.any():\n",
        "#             output[complete_mask] = self.linear(x[complete_mask])\n",
        "\n",
        "#         # For incomplete data points, compute expected activation\n",
        "#         if (~complete_mask).any():\n",
        "#             pi, mu, sigma = density.get_conditional_params(x[~complete_mask],\n",
        "#                                                          missing_mask[~complete_mask])\n",
        "#             for j in range(self.linear.out_features):\n",
        "#                 output[~complete_mask, j] = generalized_relu(\n",
        "#                     self.linear.weight[j],\n",
        "#                     self.linear.bias[j],\n",
        "#                     pi, mu, sigma\n",
        "#                 )\n",
        "\n",
        "#         return output\n",
        "\n",
        "# class MissingDataNN(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dims, output_dim, n_components=5):\n",
        "#         super().__init__()\n",
        "#         self.density = MissingDataDensity(n_components, input_dim)\n",
        "\n",
        "#         # First layer handles missing data\n",
        "#         self.first_layer = GeneralizedLinear(input_dim, hidden_dims[0])\n",
        "\n",
        "#         # Regular layers for the rest of the network\n",
        "#         layers = []\n",
        "#         for i in range(len(hidden_dims)-1):\n",
        "#             layers.extend([\n",
        "#                 nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
        "#                 nn.ReLU()\n",
        "#             ])\n",
        "#         layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
        "\n",
        "#         self.layers = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x, missing_mask):\n",
        "#         # First layer handles missing data\n",
        "#         x = self.first_layer(x, missing_mask, self.density)\n",
        "#         x = F.relu(x)\n",
        "\n",
        "#         # Rest of the network processes normally\n",
        "#         return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "222f7fdf",
      "metadata": {
        "id": "222f7fdf"
      },
      "outputs": [],
      "source": [
        "# get predictions from the neural network. Takes in\n",
        "def get_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
        "\n",
        "    # split data for training and testing.\n",
        "    training_features, validation_features, training_target, validation_target = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
        "\n",
        "    training_features = torch.FloatTensor(training_features)\n",
        "    training_target = torch.FloatTensor(training_target)\n",
        "    validation_features = torch.FloatTensor(validation_features)\n",
        "    validation_target = torch.FloatTensor(validation_target)\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(len(predictors), 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),  \n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "    # MSE loss\n",
        "    loss_func = nn.MSELoss()\n",
        "\n",
        "    # train the model\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model(training_features)\n",
        "        loss = loss_func(prediction, training_target.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # get predictions\n",
        "    model.eval()\n",
        "    test_data = torch.FloatTensor(df_test[predictors].to_numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_data)\n",
        "        predictions = predictions.numpy().flatten()\n",
        "\n",
        "        val_predictions = model(validation_features)\n",
        "        val_predictions = val_predictions.numpy().flatten()\n",
        "\n",
        "\n",
        "    # exit if correlation between predictions and item is bad\n",
        "    mse = mean_squared_error(val_predictions, validation_target)\n",
        "    print(f\"Debug statement: Neural Net test MSE = {mse}\") ####DEBUG\n",
        "    if (mse > 0.03):\n",
        "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
        "        print('Aborting program')\n",
        "        sys.exit(-1)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vTElx8nwmv1L",
      "metadata": {
        "id": "vTElx8nwmv1L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b9fbf828",
      "metadata": {
        "id": "b9fbf828"
      },
      "outputs": [],
      "source": [
        " # orthogonalization method\n",
        "# all data is preprocessed and df test has been appended target preds\n",
        "def orthogonalize(candidates, df_test, orthogonal_vars):\n",
        "        orth_scores = {}\n",
        "        for c in candidates:\n",
        "            candset = df_test[[c, 'predicted_target']].copy().dropna() # assumes candidate has mostly non-NaN entries\n",
        "            candcol = candset[c]\n",
        "\n",
        "            X = sm.add_constant(candcol)\n",
        "            temp_orth_scores = []\n",
        "            for orth_var in orthogonal_vars:\n",
        "                orthset = df_test[[orth_var]].copy().dropna()\n",
        "                common_indices = candset.index.intersection(orthset.index)\n",
        "                if common_indices.empty:\n",
        "                    continue\n",
        "                orth_col = orthset.loc[common_indices, orth_var]\n",
        "                if np.var(orth_col) == 0:\n",
        "                    print(\"ortho:\", orth_var, \"candidate\", c)\n",
        "                    continue # zero variance leads to divide by zero error\n",
        "                candcol_common = candset.loc[common_indices, c]\n",
        "\n",
        "                X_common = sm.add_constant(candcol_common)\n",
        "                model = sm.OLS(orth_col, X_common).fit()\n",
        "                temp_orth_scores.append(model.rsquared)\n",
        "\n",
        "            if temp_orth_scores:\n",
        "                orth_scores[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
        "            else:\n",
        "                orth_scores[c] = 0\n",
        "        return orth_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1bbc7703",
      "metadata": {
        "id": "1bbc7703"
      },
      "outputs": [],
      "source": [
        "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None):\n",
        "    if candidates is None:\n",
        "        candidates = list(df_test.select_dtypes(include='number').columns) #only numerical data (don't encode categories, make user do that)\n",
        "\n",
        "\n",
        "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
        "\n",
        "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
        "    #print(f\"Candidates: {candidates}\")\n",
        "\n",
        "    # Predict status threat scores in df_test\n",
        "    df_train = data_rescale(df_train, predictors)\n",
        "    df_test = data_rescale(df_test, predictors)\n",
        "    df_train = df_train.dropna(subset=target)\n",
        "\n",
        "    # Check for NaN entries in the specified columns DEBUG\n",
        "    #for index, row in df_train.iterrows():\n",
        "     #   if row[target].isnull().any():\n",
        "      #      print(f\"Entry is NaN in row {index}\")\n",
        "\n",
        "   # print(df_train.head) ## debug\n",
        "  #  print(df_test.head)\n",
        "    predicted_scores = get_predictions(df_train, df_test, predictors, target)\n",
        "\n",
        "    df_test['predicted_target'] = predicted_scores\n",
        "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for c in candidates:\n",
        "        candset = df_test[[c, 'predicted_target']].copy().dropna()\n",
        "        if candset.empty:\n",
        "            continue\n",
        "\n",
        "        pred_scores = candset['predicted_target']\n",
        "        candcol = candset[c]\n",
        "\n",
        "        X_pred = sm.add_constant(candcol)\n",
        "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
        "        results[c] = {\n",
        "            'R_squared': model_pred.rsquared,\n",
        "            'p_value': model_pred.pvalues[1],\n",
        "            'coef': model_pred.params[1]\n",
        "        }\n",
        "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------\n",
        "\n",
        "    best_proxies = []\n",
        "\n",
        "    if orthogonal_vars:\n",
        "        orth_scores = orthogonalize(candidates, df_test, orthogonal_vars)\n",
        "        proxy_scores = {}\n",
        "        for c in candidates:\n",
        "            try:\n",
        "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_scores[c])\n",
        "            except KeyError as e:\n",
        "                continue\n",
        "\n",
        "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, score = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
        "    else:\n",
        "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
        "\n",
        "        for i in range(min(num_proxies, len(sorted_results))):\n",
        "            proxy, metrics = sorted_results[i]\n",
        "            best_proxies.append(proxy)\n",
        "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
        "\n",
        "    return best_proxies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b487cea5",
      "metadata": {
        "id": "b487cea5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\")\n",
        "\n",
        "\n",
        "# Suppress numpy invalid operation warnings\n",
        "np.seterr(invalid='ignore')\n",
        "\n",
        "datafile_train =  \"/content/temp_yougov.dta\"\n",
        "datafile_test =  \"/content/temp_yougov.dta\"\n",
        "df_train = pd.read_stata(datafile_train)\n",
        "df_test = pd.read_stata(datafile_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f6372b2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6372b2d",
        "outputId": "ff304756-720f-435e-d2de-efacd06a044e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug statement: Neural Net test MSE = 0.017662407927413856\n",
            "Proxy 1 for christian_nationalism: christian_nationalism with score: 0.10373024819112694\n",
            "Proxy 2 for christian_nationalism: auth_grid_1 with score: 0.06742502824884508\n",
            "Proxy 3 for christian_nationalism: auth_grid_3 with score: 0.06482144417052497\n",
            "Proxy 4 for christian_nationalism: ideo7 with score: 0.06094886255412213\n",
            "Proxy 5 for christian_nationalism: immigrant_citizenship with score: 0.05407189056011796\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "target = 'christian_nationalism'  # The target variable in the training set\n",
        "predictors = [ # predictors in both training and test set\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'white',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new'\n",
        "                   ]\n",
        "\n",
        "orthogonal_vars = [\n",
        "                   'presvote20post',\n",
        "                   'housevote20post',\n",
        "                   'senvote20post',\n",
        "                   'pff_jb',\n",
        "                   'pff_dt',\n",
        "                   'pid7',\n",
        "                   'election_fairnness',\n",
        "                   'educ',\n",
        "                   'hispanic',\n",
        "                   'partisan_violence',\n",
        "                   'immigrant_citizenship',\n",
        "                   'immigrant_deport',\n",
        "                   'auth_grid_1',\n",
        "                   'auth_grid_3',\n",
        "                   'auth_grid_2',\n",
        "                   'faminc_new']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_proxies = proxy_finder(df_train, df_test, target, predictors, orth_weight=0.60, orthogonal_vars=orthogonal_vars, num_proxies=5)\n",
        "#print(best_proxies)\n",
        "### orth weight 0.9 --> version, how many interviews, etx\n",
        "### 0.85 same thing\n",
        "### 0.8\n",
        "### 0.75 bad\n",
        "### 0.7 bad"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
