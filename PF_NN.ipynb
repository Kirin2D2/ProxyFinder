{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09485737",
   "metadata": {},
   "source": [
    "# version of proxyFinder algorithm using neural network to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d1e34881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91825a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH: None\n",
      "PATH: c:\\Users\\kirin\\anaconda3\\envs\\torchenv;C:\\Users\\kirin\\anaconda3\\envs\\torchenv;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Library\\mingw-w64\\bin;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Library\\usr\\bin;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Library\\bin;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Scripts;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\bin;C:\\Users\\kirin\\anaconda3\\condabin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\bin;C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.15.10-hotspot\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\Git\\cmd;C:\\Users\\kirin\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\kirin\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64;C:\\Users\\kirin\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(\"PYTHONPATH:\", os.environ.get('PYTHONPATH'))\n",
    "print(\"PATH:\", os.environ.get('PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b535aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
    "\n",
    "    # validate proxies and st item\n",
    "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
    "\n",
    "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
    "    \n",
    "    for c in predictors:\n",
    "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
    "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset  \n",
    "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'   \n",
    "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'    \n",
    "    \n",
    "    for c in candidates:\n",
    "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
    "        \n",
    "    if (orthogonal_vars != None):\n",
    "        for c in orthogonal_vars:\n",
    "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "28ebb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a new df that is a copy of df, with: rescale all columns to be\n",
    "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any \n",
    "# rows that are missing at least one predictor. \n",
    "def data_rescale(df, predictors):\n",
    "    df = df.copy() # preserve immutability\n",
    "\n",
    "    # Select only the numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # drop any rows that are missing at least one predictor\n",
    "    df = df.dropna(subset=predictors)\n",
    "    # print('the dataframe we\\'re rescaling is size: ') # debug\n",
    "    # Initialize the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d017725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network definition\n",
    "def build_nn_model(input_dim, learning_rate=0.001, l2_lambda=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),  \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),  \n",
    "        Dense(1, kernel_regularizer=l2(l2_lambda))\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff982100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a trained neural network to predict df[item] using df[predictors_df1]\n",
    "# report error and crash if predictors don't predict item\n",
    "def train_nn_model(X_train, y_train, input_dim, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    model = build_nn_model(input_dim, learning_rate, l2_lambda)\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a775aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from the neural network. Takes in\n",
    "def get_nn_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    \n",
    "    # split data for training and testing. \n",
    "    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
    "    X_test = df_test[predictors].to_numpy()\n",
    "\n",
    "    # train network and get predictions\n",
    "    model = train_nn_model(X_train_train, y_train_train, len(predictors), epochs, learning_rate, l2_lambda)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # exit if correlation between predictions and item is bad\n",
    "    mse = mean_squared_error(model.predict(X_train_test), y_train_test)\n",
    "    print(f\"Debug statement: MSE = {mse}\") ####DEBUG\n",
    "    print(f'Confidence level: {(int)((1 - (mse / 0.036)) * 100)}% (results under 25% suppressed)')\n",
    "    if (mse > 0.03):\n",
    "        print('Input Error: Predictors cannot predict {target} in df1', file=sys.stderr)\n",
    "        print('Aborting program')\n",
    "        sys.exit(-1)\n",
    "\n",
    "   # print(f\"Predictions before flattening: {predictions[:10]}\") #DEBUG\n",
    "   # print('predictions after flattening: ', predictions.flatten()[:10])#DEBUG\n",
    "\n",
    "    return predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "acfe4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final 3 parameters for debugging/fine tuning\n",
    "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    if candidates is None:\n",
    "        candidates = list(df_test.select_dtypes(include='number').columns)\n",
    "    \n",
    "\n",
    "    proxy_finder_validate(target, candidates, df_train, df_test, predictors, orthogonal_vars)\n",
    "\n",
    "    #print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
    "    #print(f\"Candidates: {candidates}\")\n",
    "\n",
    "    # Predict status threat scores in df_test\n",
    "    df_train = data_rescale(df_train, predictors)\n",
    "    df_test = data_rescale(df_test, predictors)\n",
    "    df_train = df_train.dropna(subset=target)\n",
    "\n",
    "    # Check for NaN entries in the specified columns DEBUG\n",
    "    #for index, row in df_train.iterrows():\n",
    "     #   if row[target].isnull().any():\n",
    "      #      print(f\"Entry is NaN in row {index}\")\n",
    "        \n",
    "   # print(df_train.head) ## debug\n",
    "  #  print(df_test.head)\n",
    "    predicted_scores = get_nn_predictions(df_train, df_test, predictors, target, epochs, learning_rate, l2_lambda)\n",
    "    \n",
    "    df_test['predicted_status_threat'] = predicted_scores\n",
    "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------ \n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for c in candidates:\n",
    "        candset = df_test[[c, 'predicted_status_threat']].copy().dropna()\n",
    "        if candset.empty:\n",
    "            continue\n",
    "        \n",
    "        pred_scores = candset['predicted_status_threat']\n",
    "        candcol = candset[c]\n",
    "\n",
    "        X_pred = sm.add_constant(candcol)\n",
    "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
    "        results[c] = {\n",
    "            'R_squared': model_pred.rsquared,\n",
    "            'p_value': model_pred.pvalues[1],\n",
    "            'coef': model_pred.params[1]\n",
    "        }\n",
    "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------ \n",
    "  \n",
    "    best_proxies = []\n",
    "\n",
    "    if orthogonal_vars:\n",
    "        orth_score = {}\n",
    "        for c in candidates:\n",
    "            candset = df_test[[c, 'predicted_status_threat']].copy().dropna()\n",
    "            pred_scores = candset['predicted_status_threat']\n",
    "            candcol = candset[c]\n",
    "        \n",
    "            X = sm.add_constant(candcol)\n",
    "            temp_orth_scores = []\n",
    "            for orth_var in orthogonal_vars:\n",
    "                orthset = df_test[[orth_var]].copy().dropna()\n",
    "                common_indices = candset.index.intersection(orthset.index)\n",
    "                if common_indices.empty:\n",
    "                    continue\n",
    "                orth_col = orthset.loc[common_indices, orth_var]\n",
    "                candcol_common = candset.loc[common_indices, c]\n",
    "\n",
    "                X_common = sm.add_constant(candcol_common)\n",
    "                model = sm.OLS(orth_col, X_common).fit()\n",
    "                temp_orth_scores.append(model.rsquared)\n",
    "            \n",
    "            if temp_orth_scores:\n",
    "                orth_score[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
    "            else:\n",
    "                orth_score[c] = 0\n",
    "        \n",
    "        proxy_scores = {}\n",
    "        for c in candidates:\n",
    "            try:\n",
    "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_score[c])\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        \n",
    "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i in range(min(num_proxies, len(sorted_results))):\n",
    "            proxy, score = sorted_results[i]\n",
    "            best_proxies.append(proxy)\n",
    "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
    "    else: \n",
    "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
    "    \n",
    "        for i in range(min(num_proxies, len(sorted_results))):\n",
    "            proxy, metrics = sorted_results[i]\n",
    "            best_proxies.append(proxy)\n",
    "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
    "    \n",
    "    return best_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f09aff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\") # I should probably actually fix this one so it doesn't break with future updates\n",
    "\n",
    "\n",
    "# Suppress numpy invalid operation warnings\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "datafile_train =  r\"C:\\Users\\kirin\\OURSIP_summer24\\temp_yougov.dta\"\n",
    "datafile_test =  r\"C:\\Users\\kirin\\OURSIP_summer24\\temp_anes.dta\"\n",
    "df_train = pd.read_stata(datafile_train)\n",
    "df_test = pd.read_stata(datafile_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b52d4ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.844992\n",
      "1       1.000000\n",
      "2       1.000000\n",
      "3       0.707724\n",
      "4       1.000000\n",
      "          ...   \n",
      "3395    0.702534\n",
      "3396    0.702534\n",
      "3397    0.702534\n",
      "3398    0.702534\n",
      "3399    0.702534\n",
      "Name: white, Length: 3400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['white'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82bbc440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Debug statement: MSE = 0.012686556952692416\n",
      "Confidence level: 64% (results under 25% suppressed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kirin\\anaconda3\\envs\\torchenv\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1783: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "c:\\Users\\kirin\\anaconda3\\envs\\torchenv\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1783: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "c:\\Users\\kirin\\anaconda3\\envs\\torchenv\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1783: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "c:\\Users\\kirin\\anaconda3\\envs\\torchenv\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1783: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy 1 for christian_nationalism: V200014a with score: inf\n",
      "Proxy 2 for christian_nationalism: V200014b with score: inf\n",
      "Proxy 3 for christian_nationalism: V200014c with score: inf\n",
      "Proxy 4 for christian_nationalism: V200014d with score: inf\n",
      "Proxy 5 for christian_nationalism: V201339 with score: 0.0002426914782735195\n",
      "Proxy 6 for christian_nationalism: V200004 with score: 8.512196764447606e-10\n",
      "Proxy 7 for christian_nationalism: V201001 with score: 8.512196764447606e-10\n",
      "Proxy 8 for christian_nationalism: V201007b with score: 8.512196764447606e-10\n",
      "Proxy 9 for christian_nationalism: V201007c with score: 8.512196764447606e-10\n",
      "Proxy 10 for christian_nationalism: V201009 with score: 8.512196764447606e-10\n",
      "Proxy 11 for christian_nationalism: V201013b with score: 8.512196764447606e-10\n",
      "Proxy 12 for christian_nationalism: V201014a with score: 8.512196764447606e-10\n",
      "Proxy 13 for christian_nationalism: V201014c with score: 8.512196764447606e-10\n",
      "Proxy 14 for christian_nationalism: V201015z with score: 8.512196764447606e-10\n",
      "Proxy 15 for christian_nationalism: V201017 with score: 8.512196764447606e-10\n",
      "Proxy 16 for christian_nationalism: V201018z with score: 8.512196764447606e-10\n",
      "Proxy 17 for christian_nationalism: V201019 with score: 8.512196764447606e-10\n",
      "Proxy 18 for christian_nationalism: V201022 with score: 8.512196764447606e-10\n",
      "Proxy 19 for christian_nationalism: V201023 with score: 8.512196764447606e-10\n",
      "Proxy 20 for christian_nationalism: V201025x with score: 8.512196764447606e-10\n"
     ]
    }
   ],
   "source": [
    "target = 'christian_nationalism'  # The target variable in the training set\n",
    "predictors = [\n",
    "                   'presvote20post',\n",
    "                   'housevote20post',\n",
    "                   'senvote20post',\n",
    "                   #'trump_presidential_approval',\n",
    "                   'pff_jb',\n",
    "                   'pff_dt',\n",
    "                   'ideo7',\n",
    "                   'pid7',\n",
    "                  # 'para_social_grid_2',\n",
    "                   'election_fairnness',\n",
    "                   #'cab_b', w1, w2, w3\n",
    "                   'educ',\n",
    "                   'white', #white and hispanic indicators replace race\n",
    "                   'hispanic',\n",
    "                   'partisan_violence',\n",
    "                   'immigrant_citizenship',\n",
    "                   'immigrant_deport',\n",
    "                   'auth_grid_1',\n",
    "                   'auth_grid_3',\n",
    "                   'auth_grid_2',\n",
    "                   'faminc_new'\n",
    "                  # 'wc_together', w1, w2, w3\n",
    "                   #'wc_jobs',\n",
    "                   #'racial_id',\n",
    "                   #'hardworkingvlazy',\n",
    "                   #'pronenot_violence',\n",
    "                   #'group_disc_black',\n",
    "                   #'group_disc_hispanic',\n",
    "                   #'group_disc_white'\n",
    "                   ]  # Predictors in both training and testing sets\n",
    "\n",
    "orthogonal_vars = [\n",
    "                   'presvote20post',\n",
    "                   'housevote20post',\n",
    "                   'senvote20post',\n",
    "                   'pff_jb',\n",
    "                   'pff_dt',\n",
    "                   'ideo7',\n",
    "                   'pid7', \n",
    "                   'election_fairnness',\n",
    "                   'educ',\n",
    "                   'hispanic', \n",
    "                   'partisan_violence',\n",
    "                   'immigrant_citizenship',\n",
    "                   'immigrant_deport',\n",
    "                   'auth_grid_1',\n",
    "                   'auth_grid_3',\n",
    "                   'auth_grid_2',\n",
    "                   'faminc_new']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_proxies = proxy_finder(df_train, df_test, target, predictors, orth_weight=0.8, orthogonal_vars=orthogonal_vars, num_proxies=20)\n",
    "#print(best_proxies)\n",
    "### orth weight 0.9 --> version, how many interviews, etx\n",
    "### 0.85 same thing\n",
    "### 0.8 \n",
    "### 0.75 bad\n",
    "### 0.7 bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf8b62ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "AssertionError: predictor psc1_W1_01 not in df2.columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 36\u001b[0m\n\u001b[0;32m     18\u001b[0m predictors \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpsc1_W1_01\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchristian_nationalism\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage501\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Predictors in both training and testing sets\u001b[39;00m\n\u001b[0;32m     27\u001b[0m orthogonal_vars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpsc1_W1_01\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchristian_nationalism\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthoritarianism\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage501\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     34\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 36\u001b[0m best_proxies \u001b[38;5;241m=\u001b[39m \u001b[43mproxy_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morthogonal_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morthogonal_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#print(best_proxies)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[92], line 7\u001b[0m, in \u001b[0;36mproxy_finder\u001b[1;34m(df_train, df_test, target, predictors, num_proxies, orth_weight, candidates, orthogonal_vars, epochs, learning_rate, l2_lambda)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m candidates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_test\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mproxy_finder_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morthogonal_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#print(f\"Candidates: {candidates}\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Predict status threat scores in df_test\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df_train \u001b[38;5;241m=\u001b[39m data_rescale(df_train, predictors)\n",
      "Cell \u001b[1;32mIn[87], line 10\u001b[0m, in \u001b[0;36mproxy_finder_validate\u001b[1;34m(item, candidates, df1, df2, predictors, orthogonal_vars)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m predictors:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df1\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssertionError: predictor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in df1.columns\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df2\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssertionError: predictor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in df2.columns\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# we need same variable in second dataset  \u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df1\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a numeric column in df1\u001b[39m\u001b[38;5;124m'\u001b[39m   \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df2\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a numeric column in df2\u001b[39m\u001b[38;5;124m'\u001b[39m    \n",
      "\u001b[1;31mAssertionError\u001b[0m: AssertionError: predictor psc1_W1_01 not in df2.columns"
     ]
    }
   ],
   "source": [
    "# abortion and traditional family values\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\") # I should probably actually fix this one so it doesn't break with future updates\n",
    "\n",
    "\n",
    "# Suppress numpy invalid operation warnings\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# Example usage: Clearly, the best proxy for status threat should be status threat & related items. \n",
    "datafile_train =  r\"C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved (1).dta\"\n",
    "datafile_test =  r\"C:\\Users\\kirin\\Downloads\\anes2020\\anes_timeseries_2020_stata_20220210.dta\"\n",
    "df_train = pd.read_stata(datafile_train)\n",
    "df_test = pd.read_stata(datafile_test)\n",
    "\n",
    "target = 'status_threat'  # The target variable in the training set\n",
    "predictors = [\n",
    "                   'psc1_W1_01',\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education']  # Predictors in both training and testing sets\n",
    "orthogonal_vars = ['psc1_W1_01',\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education']\n",
    "\n",
    "best_proxies = proxy_finder(df_train, df_test, target, predictors, orthogonal_vars=orthogonal_vars, num_proxies=20)\n",
    "#print(best_proxies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
