{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09485737",
   "metadata": {},
   "source": [
    "# version of proxyFinder algorithm using neural network to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e34881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91825a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH: None\n",
      "PATH: c:\\Users\\kirin\\anaconda3\\envs\\torchenv;C:\\Users\\kirin\\anaconda3\\envs\\torchenv;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Library\\mingw-w64\\bin;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Library\\usr\\bin;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Library\\bin;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\Scripts;C:\\Users\\kirin\\anaconda3\\envs\\torchenv\\bin;C:\\Users\\kirin\\anaconda3\\condabin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\bin;C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.15.10-hotspot\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\Git\\cmd;C:\\Users\\kirin\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\kirin\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64;C:\\Users\\kirin\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(\"PYTHONPATH:\", os.environ.get('PYTHONPATH'))\n",
    "print(\"PATH:\", os.environ.get('PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b535aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_finder_validate(item, candidates, df1, df2, predictors, orthogonal_vars):\n",
    "\n",
    "    # validate proxies and st item\n",
    "    assert item in df1.columns, f'AssertionError: item {item} not in df1.columns'\n",
    "\n",
    "    assert predictors, f'AssertionError: missing predictors. If you would prefer to not specify predictors, do not pass in a variable.'\n",
    "    \n",
    "    for c in predictors:\n",
    "        assert c in df1.columns, f'AssertionError: predictor {c} not in df1.columns'\n",
    "        assert c in df2.columns, f'AssertionError: predictor {c} not in df2.columns' # we need same variable in second dataset  \n",
    "        assert c in df1.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df1'   \n",
    "        assert c in df2.select_dtypes(include=['number']).columns, f'predictor {c} is not a numeric column in df2'    \n",
    "    \n",
    "    for c in candidates:\n",
    "        assert c in df2.columns, f'AssertionError: candidate {c} not in df2.columns'\n",
    "        \n",
    "    if (orthogonal_vars != None):\n",
    "        for c in orthogonal_vars:\n",
    "            assert c in df2.columns, f'AssertionError: orthogonal variable {c} not in df2.columns'\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ebb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a new df that is a copy of df, with: rescale all columns to be\n",
    "#  between 0 and 1, inclusive. Drop any non-numeric columns. Drop any \n",
    "# rows that are missing at least one predictor. \n",
    "def data_rescale(df, predictors):\n",
    "    df = df.copy() # preserve immutability\n",
    "\n",
    "    # Select only the numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # drop any rows that are missing at least one predictor\n",
    "    df = df.dropna(subset=predictors)\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    scaled_values = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Create a new DataFrame with the scaled values, maintaining the original column names\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=numeric_cols, index=df.index)\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d017725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network definition\n",
    "def build_nn_model(input_dim, learning_rate=0.001, l2_lambda=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),  \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),  \n",
    "        Dense(1, kernel_regularizer=l2(l2_lambda))\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff982100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a trained neural network to predict df[item] using df[predictors_df1]\n",
    "# report error and crash if predictors don't predict item\n",
    "def train_nn_model(X_train, y_train, input_dim, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    model = build_nn_model(input_dim, learning_rate, l2_lambda)\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a775aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from the neural network. Takes in\n",
    "def get_nn_predictions(df_train, df_test, predictors, target, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    \n",
    "    # split data for training and testing. \n",
    "    X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(df_train[predictors].to_numpy(), df_train[target].to_numpy(), test_size=0.2, random_state=42)\n",
    "    X_test = df_test[predictors].to_numpy()\n",
    "\n",
    "    # train network and get predictions\n",
    "    model = train_nn_model(X_train_train, y_train_train, len(predictors), epochs, learning_rate, l2_lambda)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # exit if correlation between predictions and item is bad\n",
    "    mse = mean_squared_error(model.predict(X_train_test), y_train_test)\n",
    "    #print(f\"Debug statement: MSE = {mse}\") ####DEBUG\n",
    "    print(f'Confidence level: {(int)((1 - (mse / 0.036)) * 100)}%')\n",
    "    if (mse > 0.03):\n",
    "        print('Input Error: Predictors cannot predict item in df1', file=sys.stderr)\n",
    "        print('Aborting program')\n",
    "        sys.exit(-1)\n",
    "\n",
    "   # print(f\"Predictions before flattening: {predictions[:10]}\") #DEBUG\n",
    "   # print('predictions after flattening: ', predictions.flatten()[:10])#DEBUG\n",
    "\n",
    "    return predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acfe4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final 3 parameters for debugging/fine tuning\n",
    "def proxy_finder(df_train, df_test, target, predictors, num_proxies=1, orth_weight=0.65, candidates=None, orthogonal_vars=None, epochs=100, learning_rate=0.001, l2_lambda=0.001):\n",
    "    if candidates is None:\n",
    "        candidates = list(df_test.select_dtypes(include='number').columns)\n",
    "    \n",
    "  #  print(f\"Predictors: {predictors}\") #DEBUGDEBUGDEBUG------------------------------------------------------------\n",
    "    #print(f\"Candidates: {candidates}\")\n",
    "\n",
    "    # Predict status threat scores in df_test\n",
    "    df_train = data_rescale(df_train, predictors)\n",
    "    df_test = data_rescale(df_test, predictors)\n",
    "   # print(df_train.head) ## debug\n",
    "  #  print(df_test.head)\n",
    "    predicted_scores = get_nn_predictions(df_train, df_test, predictors, target, epochs, learning_rate, l2_lambda)\n",
    "    \n",
    "    df_test['predicted_status_threat'] = predicted_scores\n",
    "    #print(f\"Predicted scores: {predicted_scores[:10]}\")  #DEBUG DEBUG------------------------------------------------------------ \n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for c in candidates:\n",
    "        candset = df_test[[c, 'predicted_status_threat']].copy().dropna()\n",
    "        if candset.empty:\n",
    "            continue\n",
    "        \n",
    "        pred_scores = candset['predicted_status_threat']\n",
    "        candcol = candset[c]\n",
    "\n",
    "        X_pred = sm.add_constant(candcol)\n",
    "        model_pred = sm.OLS(pred_scores, X_pred).fit()\n",
    "        results[c] = {\n",
    "            'R_squared': model_pred.rsquared,\n",
    "            'p_value': model_pred.pvalues[1],\n",
    "            'coef': model_pred.params[1]\n",
    "        }\n",
    "        #print(f\"candidate {c}: Results: {results}\")  # Debug statement------------------------------------------------------------ \n",
    "  \n",
    "    best_proxies = []\n",
    "\n",
    "    if orthogonal_vars:\n",
    "        orth_score = {}\n",
    "        for c in candidates:\n",
    "            candset = df_test[[c, 'predicted_status_threat']].copy().dropna()\n",
    "            pred_scores = candset['predicted_status_threat']\n",
    "            candcol = candset[c]\n",
    "        \n",
    "            X = sm.add_constant(candcol)\n",
    "            temp_orth_scores = []\n",
    "            for orth_var in orthogonal_vars:\n",
    "                orthset = df_test[[orth_var]].copy().dropna()\n",
    "                common_indices = candset.index.intersection(orthset.index)\n",
    "                if common_indices.empty:\n",
    "                    continue\n",
    "                orth_col = orthset.loc[common_indices, orth_var]\n",
    "                candcol_common = candset.loc[common_indices, c]\n",
    "\n",
    "                X_common = sm.add_constant(candcol_common)\n",
    "                model = sm.OLS(orth_col, X_common).fit()\n",
    "                temp_orth_scores.append(model.rsquared)\n",
    "            \n",
    "            if temp_orth_scores:\n",
    "                orth_score[c] = sum(temp_orth_scores) / len(temp_orth_scores)\n",
    "            else:\n",
    "                orth_score[c] = 0\n",
    "        \n",
    "        proxy_scores = {}\n",
    "        for c in candidates:\n",
    "            try:\n",
    "                proxy_scores[c] = (c, (1 - orth_weight) * results[c]['R_squared'] - orth_weight * orth_score[c])\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        \n",
    "        sorted_results = sorted(proxy_scores.values(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i in range(min(num_proxies, len(sorted_results))):\n",
    "            proxy, score = sorted_results[i]\n",
    "            best_proxies.append(proxy)\n",
    "            print(f\"Proxy {i+1} for {target}: {proxy} with score: {score}\")\n",
    "    else: \n",
    "        sorted_results = sorted(results.items(), key=lambda x: (-x[1]['R_squared'], x[1]['p_value']))\n",
    "    \n",
    "        for i in range(min(num_proxies, len(sorted_results))):\n",
    "            proxy, metrics = sorted_results[i]\n",
    "            best_proxies.append(proxy)\n",
    "            print(f\"Proxy {i+1} for {target}: {proxy} with R_squared: {metrics['R_squared']} and p_value: {metrics['p_value']}\")\n",
    "    \n",
    "    return best_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f09aff0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 959us/step\n",
      "8/8 [==============================] - 0s 0s/step\n",
      "Confidence level: 27%\n",
      "Proxy 1 for status_threat: st_W1_01 with score: 0.059827577676469956\n",
      "Proxy 2 for status_threat: status_threat with score: 0.059827577676469956\n",
      "Proxy 3 for status_threat: st_W1 with score: 0.0598275775976887\n",
      "Proxy 4 for status_threat: rr1_W1 with score: 0.054831451729434766\n",
      "Proxy 5 for status_threat: rr_W1 with score: 0.054831451657055524\n",
      "Proxy 6 for status_threat: rrx_W1 with score: 0.054831451657055524\n",
      "Proxy 7 for status_threat: race_resent with score: 0.054831451657055524\n",
      "Proxy 8 for status_threat: status_W2 with score: 0.05148620176210139\n",
      "Proxy 9 for status_threat: status2_W2 with score: 0.05148620157651515\n",
      "Proxy 10 for status_threat: ideology with score: 0.04395950751103625\n",
      "Proxy 11 for status_threat: imm_dep1 with score: 0.04263953878398996\n",
      "Proxy 12 for status_threat: maga_view with score: 0.03994999649819911\n",
      "Proxy 13 for status_threat: sta_W3_01 with score: 0.03429170417259268\n",
      "Proxy 14 for status_threat: stat_W3 with score: 0.03429170389072758\n",
      "Proxy 15 for status_threat: pros_riot with score: 0.032796967177708636\n",
      "Proxy 16 for status_threat: pres2024 with score: 0.03265028067792697\n",
      "Proxy 17 for status_threat: white_discrim_1 with score: 0.0314502069021106\n",
      "Proxy 18 for status_threat: imm_cit with score: 0.03142324059238097\n",
      "Proxy 19 for status_threat: imm_dep with score: 0.03142324059238097\n",
      "Proxy 20 for status_threat: bidenfav with score: 0.031071701835838433\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Series.__getitem__ treating keys as positions is deprecated\") # I should probably actually fix this one so it doesn't break with future updates\n",
    "\n",
    "\n",
    "# Suppress numpy invalid operation warnings\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "# Example usage: Clearly, the best proxy for status threat should be status threat & related items. \n",
    "datafile_train =  r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "datafile_test =  r'C:\\Users\\kirin\\Downloads\\W1_W2_W3_Merged_saved.dta'\n",
    "df_train = pd.read_stata(datafile_train)\n",
    "df_test = pd.read_stata(datafile_test)\n",
    "\n",
    "target = 'status_threat'  # The target variable in the training set\n",
    "predictors = [\n",
    "                   'psc1_W1_01',\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education']  # Predictors in both training and testing sets\n",
    "orthogonal_vars = ['psc1_W1_01',\n",
    "                   'christian_nationalism',\n",
    "                   'authoritarianism',\n",
    "                   'social_dom11',\n",
    "                   'race_resent',\n",
    "                   'party_ID',\n",
    "                   'ideology',\n",
    "                   'age501',\n",
    "                   'education']\n",
    "\n",
    "best_proxies = proxy_finder(df_train, df_test, target, predictors, orthogonal_vars=orthogonal_vars, num_proxies=20)\n",
    "#print(best_proxies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
